apiVersion: simulation.karpenter.sh/v1alpha1
ec2NodeClasses:
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "2342988719042999840"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T00:31:09Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2588
      SfnKubernetes: d13fffcacba5c8becb23e22beedf1dc2cc30c7ef
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: karpenter-integration-test-mgrosser-consolidation-up
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      nodegroup-provider: karpenter
      nodegroupDeployment: karpenter-integration-test-mgrosser-consolidation-up
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T00:31:09Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T00:31:09Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T00:31:11Z"
    name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
    resourceVersion: "3415046009"
    uid: 4605f88a-c65c-4723-a466-562fae98e6f8
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2588
      SfnKubernetes: d13fffcacba5c8becb23e22beedf1dc2cc30c7ef
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='karpenter-integration-test-mgrosser-consolidation-up'
          HOSTGROUP='k8s-karpenter-integration-test-mgrosser-consolidation-up'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: karpenter-integration-test-mgrosser-consolidation-up
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-karpenter-integration-test-mgrosser-consolidation-up","node-type":"karpenter-integration-test-mgrosser-consolidation-up","node-role.kubernetes.compute.zende.sk/karpenter-integration-test-mgrosser-consolidation-up":"true","node.kubernetes.io/role":"karpenter-integration-test-mgrosser-consolidation-up","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=karpenter-integration-test-mgrosser-consolidation-up:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-karpenter-integration-test-mgrosser-consolidation-up","node-type":"karpenter-integration-test-mgrosser-consolidation-up","node-role.kubernetes.compute.zende.sk/karpenter-integration-test-mgrosser-consolidation-up":"true","node.kubernetes.io/role":"karpenter-integration-test-mgrosser-consolidation-up","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=karpenter-integration-test-mgrosser-consolidation-up:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T00:31:10Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T00:31:10Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T00:31:10Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T00:31:10Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T00:31:11Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T00:31:11Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "3827120463488696009"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:06Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-nqltb
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-nqltb
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:06Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"2d4ae4be-9570-4427-a8cd-2f79ccf81a64"}: {}
            k:{"uid":"7cd7441a-5ad5-4052-9303-3952a5014bd5"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:23:06Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:08Z"
    name: node-arm-spot-usw2a-nqltb
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2a-nqltb-1
      uid: 7cd7441a-5ad5-4052-9303-3952a5014bd5
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2a-nqltb-0
      uid: 2d4ae4be-9570-4427-a8cd-2f79ccf81a64
    resourceVersion: "3416855841"
    uid: 9466ce48-24f8-41e4-b65b-f318181479f2
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-nqltb
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm-spot'
          HOSTGROUP='k8s-node-arm-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "12471879938885182241"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:59Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-rvgr8
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-rvgr8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:59Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"363c2b38-228c-4711-9be6-4f2e83bb7ae6"}: {}
            k:{"uid":"c9675973-1015-43af-be78-c9d5d0439a7d"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:59Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:39:00Z"
    name: node-arm-spot-usw2a-rvgr8
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2a-rvgr8-0
      uid: c9675973-1015-43af-be78-c9d5d0439a7d
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2a-rvgr8-1
      uid: 363c2b38-228c-4711-9be6-4f2e83bb7ae6
    resourceVersion: "3417161031"
    uid: 0a46147f-a068-43f7-9002-edbc2aa5cfb6
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-rvgr8
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm-spot'
          HOSTGROUP='k8s-node-arm-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "1249430426667444028"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:25Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-4v8s5
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-4v8s5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:30Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c3197ad9-8a9b-4855-a518-76f13f30b7bf"}: {}
            k:{"uid":"ca3d0776-14fa-4c98-b3e2-9b5f1c163e63"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:34Z"
    name: node-arm-spot-usw2b-4v8s5
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2b-4v8s5-0
      uid: ca3d0776-14fa-4c98-b3e2-9b5f1c163e63
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2b-4v8s5-1
      uid: c3197ad9-8a9b-4855-a518-76f13f30b7bf
    resourceVersion: "3416859926"
    uid: 66213966-bc83-43de-807b-637cb6af46ab
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-4v8s5
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm-spot'
          HOSTGROUP='k8s-node-arm-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:26Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "9831171138325341636"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:50Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-dm4xh
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-dm4xh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:50Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:52Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"7ecb2e43-6315-4993-b5db-5d53a927848a"}: {}
            k:{"uid":"68333296-016e-4a8b-8381-81cc59c60a06"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:41:08Z"
    name: node-arm-spot-usw2b-dm4xh
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2b-dm4xh-0
      uid: 7ecb2e43-6315-4993-b5db-5d53a927848a
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2b-dm4xh-1
      uid: 68333296-016e-4a8b-8381-81cc59c60a06
    resourceVersion: "3417172895"
    uid: 7cf60770-b432-445e-972e-07f3050aaac7
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-dm4xh
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm-spot'
          HOSTGROUP='k8s-node-arm-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "6871812639476746272"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:48Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-87sn8
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-87sn8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"13d1e41a-2413-4630-843e-c11f0145098a"}: {}
            k:{"uid":"fcb471ea-8da8-40df-8bd7-c878d860a0ff"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:49Z"
    name: node-arm-spot-usw2c-87sn8
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2c-87sn8-0
      uid: fcb471ea-8da8-40df-8bd7-c878d860a0ff
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2c-87sn8-1
      uid: 13d1e41a-2413-4630-843e-c11f0145098a
    resourceVersion: "3417160554"
    uid: af5290be-d42c-4d79-9c66-a0784f3c526c
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-87sn8
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm-spot'
          HOSTGROUP='k8s-node-arm-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "2111841805919858352"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:30Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-mc28v
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-mc28v
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:33Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:39Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"248c0683-b790-41c5-bd95-01be2fd47a27"}: {}
            k:{"uid":"e623c744-7fb2-4cfa-8984-5aa8dbd7b887"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:54Z"
    name: node-arm-spot-usw2c-mc28v
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2c-mc28v-1
      uid: e623c744-7fb2-4cfa-8984-5aa8dbd7b887
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-spot-usw2c-mc28v-0
      uid: 248c0683-b790-41c5-bd95-01be2fd47a27
    resourceVersion: "3416876519"
    uid: 3f4a80bc-160d-4cc1-b278-7ce30b5b4f06
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-mc28v
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm-spot'
          HOSTGROUP='k8s-node-arm-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm-spot","node-type":"node-arm-spot","node-role.kubernetes.compute.zende.sk/node-arm-spot":"true","node.kubernetes.io/role":"node-arm-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "6246678945917299987"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:01Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2a-d42lg
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-d42lg
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:01Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"1ebbab61-f562-4b18-8b47-e4e3678c3bb5"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:23:01Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:03Z"
    name: node-arm-usw2a-d42lg
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-usw2a-d42lg-0
      uid: 1ebbab61-f562-4b18-8b47-e4e3678c3bb5
    resourceVersion: "3416855551"
    uid: f6b3fbe0-680e-4d1b-b0c5-c9914762f0fa
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-d42lg
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm'
          HOSTGROUP='k8s-node-arm'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:02Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:02Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:02Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:02Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "10678226489509741797"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:54Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2a-thmxj
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-thmxj
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:54Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"a8d43df6-1126-4442-b581-958c01814bdf"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:55Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:58Z"
    name: node-arm-usw2a-thmxj
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-usw2a-thmxj-0
      uid: a8d43df6-1126-4442-b581-958c01814bdf
    resourceVersion: "3417160914"
    uid: 05a02920-fe94-47d3-b97e-7d12d3f74b31
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-thmxj
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm'
          HOSTGROUP='k8s-node-arm'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:56Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "5501366593409000120"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:45Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2b-xhg9c
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xhg9c
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:45Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c2a5d1ef-aaf1-45d9-95bb-fdb78eea4277"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:45Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:48Z"
    name: node-arm-usw2b-xhg9c
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-usw2b-xhg9c-0
      uid: c2a5d1ef-aaf1-45d9-95bb-fdb78eea4277
    resourceVersion: "3417160447"
    uid: 5bf5c2b7-7333-451c-b2ae-fadffe3976b7
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xhg9c
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm'
          HOSTGROUP='k8s-node-arm'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:47Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:47Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "865170517765891079"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:09Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2b-xjlb5
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xjlb5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:09Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"16ab0fc7-213d-4957-8894-993fa4eb423d"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:23:09Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:11Z"
    name: node-arm-usw2b-xjlb5
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-usw2b-xjlb5-0
      uid: 16ab0fc7-213d-4957-8894-993fa4eb423d
    resourceVersion: "3416856044"
    uid: 41aadacb-3968-4438-8e3d-f3b9b7d075d5
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xjlb5
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm'
          HOSTGROUP='k8s-node-arm'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "12172996977993524519"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:30Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2c-nt5f6
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-nt5f6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:32Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"00507fa7-3c5e-43e6-9f26-7facfa58b390"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:33Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:38Z"
    name: node-arm-usw2c-nt5f6
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-usw2c-nt5f6-0
      uid: 00507fa7-3c5e-43e6-9f26-7facfa58b390
    resourceVersion: "3416860158"
    uid: d2e3743c-7c6a-4c18-a0ce-de105421f4c8
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-nt5f6
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm'
          HOSTGROUP='k8s-node-arm'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "15584009246137945918"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:38Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2c-rpkp2
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-rpkp2
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:38Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e3c259f9-9607-4bbe-92e4-2096bc4a155f"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:38Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:40Z"
    name: node-arm-usw2c-rpkp2
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-arm-usw2c-rpkp2-0
      uid: e3c259f9-9607-4bbe-92e4-2096bc4a155f
    resourceVersion: "3417160067"
    uid: 978bf068-7d8b-4000-b37e-f48d79c9da04
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0f6f1d1eb63733e49
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-arm
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-rpkp2
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-arm'
          HOSTGROUP='k8s-node-arm'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-arm
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_arm.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-arm","node-type":"node-arm","node-role.kubernetes.compute.zende.sk/node-arm":"true","node.kubernetes.io/role":"node-arm","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-arm:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes ARM Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_arm"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_arm],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0f6f1d1eb63733e49
      name: ubuntu22.04_k8s_base_singlemount_arm_etcd_3_6_k8s_1_34-b7f462ec-20260120T053642
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - arm64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:39Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:39Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:39Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:39Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:40Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:40Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "4297614907259343839"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-28T02:58:15Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:58:15Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"8a680867-bcb3-41d9-b341-5dbe9fd300db"}: {}
            k:{"uid":"349eca7e-d998-4f9c-af5d-a8ee1812fa26"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T02:58:15Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:58:17Z"
    name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
      uid: 349eca7e-d998-4f9c-af5d-a8ee1812fa26
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-1
      uid: 8a680867-bcb3-41d9-b341-5dbe9fd300db
    resourceVersion: "3427185966"
    uid: 49b4a10a-715c-4b2d-b8c2-3d5933d90c7e
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 210Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"dra","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"dra","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "15011237666071722850"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-27T01:07:05Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-27T01:07:05Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"47cfa1fe-af51-4c59-b83d-15a83b7aa76f"}: {}
            k:{"uid":"a5dddb15-cd0c-4f54-80f9-7482bae0938d"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-27T01:07:06Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-27T01:07:07Z"
    name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-rajeesh-usw2a-pgrqb-1
      uid: a5dddb15-cd0c-4f54-80f9-7482bae0938d
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-rajeesh-usw2a-pgrqb-0
      uid: 47cfa1fe-af51-4c59-b83d-15a83b7aa76f
    resourceVersion: "3425041085"
    uid: 7aeed623-2518-45a6-984a-8947c3621293
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"dra","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"dra","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "16086797559861543975"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-22T22:24:29Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-vq84w
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-vq84w
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:32Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:40Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"69318ffb-c2da-4e33-9935-ba63e43d406f"}: {}
            k:{"uid":"a3ccbfb9-7755-4620-91a3-d5d5037afee8"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:54Z"
    name: node-nvidia-gpu-shared-spot-usw2a-vq84w
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2a-vq84w-0
      uid: 69318ffb-c2da-4e33-9935-ba63e43d406f
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2a-vq84w-1
      uid: a3ccbfb9-7755-4620-91a3-d5d5037afee8
    resourceVersion: "3416876560"
    uid: 40a392f6-5595-4ca3-ae28-7af53edef11f
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-vq84w
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-shared-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:40Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:40Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:40Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:40Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "775343747640415114"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-23T01:38:57Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:57Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"561fb2cf-e6d3-4ca4-9bfd-6c9531803a0d"}: {}
            k:{"uid":"ab119a97-35f9-414f-97b5-436a778c1107"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:57Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:59Z"
    name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2a-xbk5g-0
      uid: 561fb2cf-e6d3-4ca4-9bfd-6c9531803a0d
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2a-xbk5g-1
      uid: ab119a97-35f9-414f-97b5-436a778c1107
    resourceVersion: "3417160975"
    uid: 07da12a9-1a2d-4c46-bd1b-2a1587dd8126
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-shared-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "3281793563026305991"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-23T01:38:33Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-gd274
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-gd274
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:33Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"85edaf7c-f10f-4bd1-bf4b-e2a9ff357ac9"}: {}
            k:{"uid":"e4ae9ddd-d627-4056-9a9a-ad4d11a8e0c1"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:33Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:34Z"
    name: node-nvidia-gpu-shared-spot-usw2b-gd274
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2b-gd274-0
      uid: e4ae9ddd-d627-4056-9a9a-ad4d11a8e0c1
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2b-gd274-1
      uid: 85edaf7c-f10f-4bd1-bf4b-e2a9ff357ac9
    resourceVersion: "3417159841"
    uid: fd188418-612c-40e4-ab16-3c6a412a56e6
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-gd274
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-shared-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "14629783645692489436"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-22T22:23:10Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-szh5s
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-szh5s
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:10Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:12Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9e706870-bcc8-4513-9701-a4d79b406897"}: {}
            k:{"uid":"1353ca16-3545-44c1-a569-284391330995"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:59Z"
    name: node-nvidia-gpu-shared-spot-usw2b-szh5s
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2b-szh5s-1
      uid: 1353ca16-3545-44c1-a569-284391330995
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2b-szh5s-0
      uid: 9e706870-bcc8-4513-9701-a4d79b406897
    resourceVersion: "3416876887"
    uid: 17df435c-fbb0-48b7-af12-611d0edc379c
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-szh5s
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-shared-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "13919015606863251688"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-22T22:23:11Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-br9dl
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-br9dl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:11Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c2931617-2294-4bb9-9f89-0c5c0355b2a6"}: {}
            k:{"uid":"fe7e5258-0344-4e39-86ea-a11d088fa1db"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:23:11Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:13Z"
    name: node-nvidia-gpu-shared-spot-usw2c-br9dl
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2c-br9dl-0
      uid: c2931617-2294-4bb9-9f89-0c5c0355b2a6
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2c-br9dl-1
      uid: fe7e5258-0344-4e39-86ea-a11d088fa1db
    resourceVersion: "3416856138"
    uid: 5dce85ca-5c72-4e4b-9f59-a1167becacdb
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-br9dl
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-shared-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "12082908811694560013"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-23T01:38:36Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-nssvz
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-nssvz
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:36Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:37Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5ad6c23b-33a5-4f06-8713-15ecbd2dc09e"}: {}
            k:{"uid":"137d03cc-0a7e-42dd-ba54-7f779f1b1f44"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:42:16Z"
    name: node-nvidia-gpu-shared-spot-usw2c-nssvz
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2c-nssvz-0
      uid: 5ad6c23b-33a5-4f06-8713-15ecbd2dc09e
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-spot-usw2c-nssvz-1
      uid: 137d03cc-0a7e-42dd-ba54-7f779f1b1f44
    resourceVersion: "3417181501"
    uid: 60924834-70e1-4cea-b60f-f8a8a769224f
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-nssvz
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-shared-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared-spot","node-type":"node-nvidia-gpu-shared-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "4767289760877331815"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-22T22:23:05Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-2nn5b
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-2nn5b
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:05Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"a365d416-e162-4cd1-9694-664b5f77d02f"}: {}
            k:{"uid":"ad7ba39a-bf31-4c48-b281-a7252c303dbc"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:23:05Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:07Z"
    name: node-nvidia-gpu-shared-usw2a-2nn5b
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2a-2nn5b-0
      uid: ad7ba39a-bf31-4c48-b281-a7252c303dbc
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2a-2nn5b-1
      uid: a365d416-e162-4cd1-9694-664b5f77d02f
    resourceVersion: "3416855820"
    uid: f04aaf40-f5f0-4ff4-9560-8b321eb802d8
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-2nn5b
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared'
          HOSTGROUP='k8s-node-nvidia-gpu-shared'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "6854045066153888307"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-23T01:38:46Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-m5gqh
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-m5gqh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9523f282-e81d-4eda-a6a2-80d098d89f65"}: {}
            k:{"uid":"e9ba370b-eee3-4cb5-9c2f-dc32d57a1e03"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:40:28Z"
    name: node-nvidia-gpu-shared-usw2a-m5gqh
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2a-m5gqh-0
      uid: 9523f282-e81d-4eda-a6a2-80d098d89f65
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2a-m5gqh-1
      uid: e9ba370b-eee3-4cb5-9c2f-dc32d57a1e03
    resourceVersion: "3417167464"
    uid: f409eb12-0851-4436-851d-0e18960927ae
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-m5gqh
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared'
          HOSTGROUP='k8s-node-nvidia-gpu-shared'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "2388084225324805392"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-23T01:38:53Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-27x7j
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-27x7j
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:53Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c6279708-fd18-458a-ae70-91ddb7fe7ab7"}: {}
            k:{"uid":"d28edb05-d054-43e7-849b-b64006b90c8c"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:53Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:54Z"
    name: node-nvidia-gpu-shared-usw2b-27x7j
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2b-27x7j-0
      uid: d28edb05-d054-43e7-849b-b64006b90c8c
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2b-27x7j-1
      uid: c6279708-fd18-458a-ae70-91ddb7fe7ab7
    resourceVersion: "3417160756"
    uid: ec9277bf-8191-4d3b-9ab4-474da12da297
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-27x7j
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared'
          HOSTGROUP='k8s-node-nvidia-gpu-shared'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "18082610193624006662"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-22T22:24:25Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-8b7nw
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-8b7nw
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:29Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"86e9b05d-6d16-4ef8-b84a-0ff4a4f11d8d"}: {}
            k:{"uid":"690d7d6e-c76f-4004-922e-7a93e8d02f1b"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:40:23Z"
    name: node-nvidia-gpu-shared-usw2b-8b7nw
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2b-8b7nw-0
      uid: 86e9b05d-6d16-4ef8-b84a-0ff4a4f11d8d
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2b-8b7nw-1
      uid: 690d7d6e-c76f-4004-922e-7a93e8d02f1b
    resourceVersion: "3416903543"
    uid: 6e6403fa-3f69-4085-83fa-b87660d30fa9
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-8b7nw
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared'
          HOSTGROUP='k8s-node-nvidia-gpu-shared'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "11825162531338685236"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-22T22:24:14Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-gchkd
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-gchkd
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:14Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:16Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"4d65f47d-2e59-4ad6-9f00-7a047d371870"}: {}
            k:{"uid":"e85eb5eb-e4b7-4a4e-9118-ea3294f107fc"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:33Z"
    name: node-nvidia-gpu-shared-usw2c-gchkd
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2c-gchkd-0
      uid: e85eb5eb-e4b7-4a4e-9118-ea3294f107fc
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2c-gchkd-1
      uid: 4d65f47d-2e59-4ad6-9f00-7a047d371870
    resourceVersion: "3416859910"
    uid: 7f6fafd9-883d-403d-8b84-6ca3dee979b3
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-gchkd
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared'
          HOSTGROUP='k8s-node-nvidia-gpu-shared'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:15Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:15Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "16285968324574631676"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/gpu-capacity-multiplier: "3"
    creationTimestamp: "2026-01-23T01:38:34Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-z4v46
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-z4v46
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/gpu-capacity-multiplier: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"28ec4de6-1b31-4bdb-abaf-436dbc74033b"}: {}
            k:{"uid":"c66b2ef2-4814-4c6a-9ef0-1fb107d72f8e"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/gpu-shared: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:35Z"
    name: node-nvidia-gpu-shared-usw2c-z4v46
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2c-z4v46-1
      uid: 28ec4de6-1b31-4bdb-abaf-436dbc74033b
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-shared-usw2c-z4v46-0
      uid: c66b2ef2-4814-4c6a-9ef0-1fb107d72f8e
    resourceVersion: "3417159883"
    uid: 889158ec-5b62-4f0d-a0dc-46bcf12a78fb
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-shared
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      compute.zende.sk/gpu-shared: "true"
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-z4v46
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-shared'
          HOSTGROUP='k8s-node-nvidia-gpu-shared'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-shared
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_shared.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-shared","node-type":"node-nvidia-gpu-shared","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared":"true","node.kubernetes.io/role":"node-nvidia-gpu-shared","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/gpu-shared":"true","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-shared:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_shared",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_shared],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "1643559910592278624"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:31Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-ppn5t
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-ppn5t
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:31Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"89cba676-0ec6-48b3-a045-cb160d8eb4a2"}: {}
            k:{"uid":"934ae2f0-02a0-4cbe-97b3-948a55679c32"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:31Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:33Z"
    name: node-nvidia-gpu-spot-usw2a-ppn5t
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2a-ppn5t-0
      uid: 89cba676-0ec6-48b3-a045-cb160d8eb4a2
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2a-ppn5t-1
      uid: 934ae2f0-02a0-4cbe-97b3-948a55679c32
    resourceVersion: "3417159780"
    uid: 601144ee-5ef2-4909-b35c-45e06cc2d2a1
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-ppn5t
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "9872978453880711317"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:27Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-pvgnw
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-pvgnw
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:28Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:36Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"99f25999-d517-4c20-b71f-6ad45888fa26"}: {}
            k:{"uid":"d80cd3de-114e-4e02-bd91-89f0586acfc4"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:40:14Z"
    name: node-nvidia-gpu-spot-usw2a-pvgnw
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2a-pvgnw-0
      uid: 99f25999-d517-4c20-b71f-6ad45888fa26
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2a-pvgnw-1
      uid: d80cd3de-114e-4e02-bd91-89f0586acfc4
    resourceVersion: "3416903074"
    uid: 31a5be84-7497-4bbb-aeb2-9166fc33aaff
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-pvgnw
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:34Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:36Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:36Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:36Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:36Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "6742194982889868841"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:01Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-gwkj6
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-gwkj6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:02Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"7766f5a7-be90-41db-b22e-3dd0c8e3410c"}: {}
            k:{"uid":"77e74073-8c22-4021-b63c-24cf1d99188a"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:23:02Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:03Z"
    name: node-nvidia-gpu-spot-usw2b-gwkj6
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2b-gwkj6-1
      uid: 7766f5a7-be90-41db-b22e-3dd0c8e3410c
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2b-gwkj6-0
      uid: 77e74073-8c22-4021-b63c-24cf1d99188a
    resourceVersion: "3416855554"
    uid: 2dee03b5-67d7-40ac-b154-faa83b7f3107
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-gwkj6
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "4739062732212556878"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:44Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-jvnqj
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-jvnqj
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:44Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"5131a412-0a7f-44e4-8e6a-3d484807ba27"}: {}
            k:{"uid":"e4b3b50b-8147-4fe5-b1b8-776c1f9a41fb"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:39:37Z"
    name: node-nvidia-gpu-spot-usw2b-jvnqj
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2b-jvnqj-0
      uid: 5131a412-0a7f-44e4-8e6a-3d484807ba27
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2b-jvnqj-1
      uid: e4b3b50b-8147-4fe5-b1b8-776c1f9a41fb
    resourceVersion: "3417161907"
    uid: 651d6c72-b405-470e-8273-f02702d962dd
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-jvnqj
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "16009014554858852594"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:47Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-5wgf8
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-5wgf8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:47Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"42f37909-1908-44a0-b861-46f92ced3e00"}: {}
            k:{"uid":"e38b7c00-f1e2-4b31-b53a-da8c5296acaa"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:49Z"
    name: node-nvidia-gpu-spot-usw2c-5wgf8
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2c-5wgf8-0
      uid: e38b7c00-f1e2-4b31-b53a-da8c5296acaa
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2c-5wgf8-1
      uid: 42f37909-1908-44a0-b861-46f92ced3e00
    resourceVersion: "3417160547"
    uid: f48e2104-0790-4c40-8a38-f8cf5bd03df3
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-5wgf8
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "13203689524784394405"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:07Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-67wwh
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-67wwh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:07Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:09Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"1d8abbd7-b7d6-4ba8-b331-e426070b5507"}: {}
            k:{"uid":"b7d255ca-1157-4121-b3a7-b0ec860a805c"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:25:44Z"
    name: node-nvidia-gpu-spot-usw2c-67wwh
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2c-67wwh-0
      uid: 1d8abbd7-b7d6-4ba8-b331-e426070b5507
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-spot-usw2c-67wwh-1
      uid: b7d255ca-1157-4121-b3a7-b0ec860a805c
    resourceVersion: "3416864877"
    uid: 6fade44e-7bf8-4d8e-992f-20f953f4dcdb
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-67wwh
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu-spot'
          HOSTGROUP='k8s-node-nvidia-gpu-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu-spot","node-type":"node-nvidia-gpu-spot","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot":"true","node.kubernetes.io/role":"node-nvidia-gpu-spot","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu_spot",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:09Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:09Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "4256880303105874967"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:32Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-8nf24
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-8nf24
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:32Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"b15d8aa9-ea9e-4042-af0a-f6ab0b7647b8"}: {}
            k:{"uid":"f83444c1-e003-4938-8dff-f61643e2519d"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:43:03Z"
    name: node-nvidia-gpu-usw2a-8nf24
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2a-8nf24-1
      uid: f83444c1-e003-4938-8dff-f61643e2519d
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2a-8nf24-0
      uid: b15d8aa9-ea9e-4042-af0a-f6ab0b7647b8
    resourceVersion: "3417184453"
    uid: cd454ba6-1443-4b40-8810-c593326fb516
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-8nf24
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "13953559880713592013"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:27Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-j5qg5
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-j5qg5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:28Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:37Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"37b92c5d-2b2c-4eb1-9d38-d8cf1fadb944"}: {}
            k:{"uid":"cfdcf337-eeb6-4e11-a686-9f1b01ba25bb"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:28:00Z"
    name: node-nvidia-gpu-usw2a-j5qg5
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2a-j5qg5-0
      uid: cfdcf337-eeb6-4e11-a686-9f1b01ba25bb
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2a-j5qg5-1
      uid: 37b92c5d-2b2c-4eb1-9d38-d8cf1fadb944
    resourceVersion: "3416876895"
    uid: 2f126582-5758-40ec-a1bd-86c3f0a274e2
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-j5qg5
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "12726635357425418813"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:04Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-7whnf
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-7whnf
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:04Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:06Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"04e2530f-0906-445b-9fba-ace478bafd66"}: {}
            k:{"uid":"f70d77bf-44b3-4645-b1d5-a1d043ca241d"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:49Z"
    name: node-nvidia-gpu-usw2b-7whnf
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2b-7whnf-0
      uid: 04e2530f-0906-445b-9fba-ace478bafd66
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2b-7whnf-1
      uid: f70d77bf-44b3-4645-b1d5-a1d043ca241d
    resourceVersion: "3416876230"
    uid: 91ec761d-4eba-4c62-ab70-7e26969e4e82
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-7whnf
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "715492345082204192"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:30Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-ff8kp
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-ff8kp
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:30Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:32Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"14a0036c-f50f-474d-bb7d-14c0ec11729b"}: {}
            k:{"uid":"baf4be6e-63b5-4c61-9dd9-e46ce055af4d"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:39:00Z"
    name: node-nvidia-gpu-usw2b-ff8kp
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2b-ff8kp-0
      uid: 14a0036c-f50f-474d-bb7d-14c0ec11729b
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2b-ff8kp-1
      uid: baf4be6e-63b5-4c61-9dd9-e46ce055af4d
    resourceVersion: "3417161028"
    uid: 31b0dc10-867c-46ff-aa8b-d5949a64292e
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-ff8kp
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "17094773863102027585"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:43Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-9rbjq
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-9rbjq
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:43Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"166b48f7-9918-4b6d-8323-587f8a024053"}: {}
            k:{"uid":"f36686b6-927c-46c3-b196-be710539502c"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T02:32:23Z"
    name: node-nvidia-gpu-usw2c-9rbjq
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2c-9rbjq-0
      uid: f36686b6-927c-46c3-b196-be710539502c
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2c-9rbjq-1
      uid: 166b48f7-9918-4b6d-8323-587f8a024053
    resourceVersion: "3417262982"
    uid: c3dc1683-a9ff-46a0-95af-5b99c538814a
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-9rbjq
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:45Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:45Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "4511294330392881447"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:26Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-tjbsn
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-tjbsn
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:26Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"29abc5f6-0f61-4f43-b87d-b525e7b19443"}: {}
            k:{"uid":"e490e6b8-a37c-4983-abbd-25e775cfb9f3"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:34Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:35Z"
    name: node-nvidia-gpu-usw2c-tjbsn
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2c-tjbsn-0
      uid: e490e6b8-a37c-4983-abbd-25e775cfb9f3
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-nvidia-gpu-usw2c-tjbsn-1
      uid: 29abc5f6-0f61-4f43-b87d-b525e7b19443
    resourceVersion: "3416860029"
    uid: a49a31ef-6b58-4426-98d9-2a5deeae0f1e
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-nvidia-gpu
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      karpenter.sh/do-not-sync-taints: "true"
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-tjbsn
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-nvidia-gpu'
          HOSTGROUP='k8s-node-nvidia-gpu'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          NVME_KUBELET_PODS_VOLUME_PERCENTAGE='20'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-nvidia-gpu
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_nvidia_gpu.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"karpenter.sh/do-not-sync-taints":"true","StackName":"k8s-node-nvidia-gpu","node-type":"node-nvidia-gpu","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node.kubernetes.io/role":"node-nvidia-gpu","compute.zende.sk/instance-gpu-manufacturer":"nvidia","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-nvidia-gpu:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "enable_nvidia_container_runtime": true
            }
          }
        },
        "description": "Zendesk Kubernetes Nvidia GPU (Worker) Node",
        "json_class": "Chef::Role",
        "name": "k8s_node_nvidia_gpu",
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_nvidia_gpu],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:34Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:34Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "6707043413459906954"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:37Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:37Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:38Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"0ff78475-1b22-4fb5-802f-9f7136f6a360"}: {}
            k:{"uid":"3503e4b4-587c-4f68-b1c5-842d2ab52a2f"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:42:21Z"
    name: node-spot-usw2a-cschs
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2a-cschs-1
      uid: 3503e4b4-587c-4f68-b1c5-842d2ab52a2f
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2a-cschs-0
      uid: 0ff78475-1b22-4fb5-802f-9f7136f6a360
    resourceVersion: "3417181810"
    uid: 51a3ac19-f02d-41e7-b58d-dbe1593608a0
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-spot'
          HOSTGROUP='k8s-node-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "1712337603220599986"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:14Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-fqjjv
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-fqjjv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:24Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:28Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"259fdb37-2bde-49cb-9d02-c7f7efb3f162"}: {}
            k:{"uid":"b35f2d9c-eefb-415b-a230-0bc5dddd9837"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:59Z"
    name: node-spot-usw2a-fqjjv
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2a-fqjjv-0
      uid: b35f2d9c-eefb-415b-a230-0bc5dddd9837
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2a-fqjjv-1
      uid: 259fdb37-2bde-49cb-9d02-c7f7efb3f162
    resourceVersion: "3416876879"
    uid: e2f6df72-45b8-4905-9887-0d9ae158cb86
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-fqjjv
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-spot'
          HOSTGROUP='k8s-node-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:24Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:24Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "7054403893178013645"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:23:12Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-kw9ms
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-kw9ms
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:12Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:23:14Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9346e5d8-4ec1-4318-8e3c-5656fcd49861"}: {}
            k:{"uid":"d803d367-7bee-4192-98b8-d89d7c24f052"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:58Z"
    name: node-spot-usw2b-kw9ms
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2b-kw9ms-0
      uid: 9346e5d8-4ec1-4318-8e3c-5656fcd49861
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2b-kw9ms-1
      uid: d803d367-7bee-4192-98b8-d89d7c24f052
    resourceVersion: "3416876741"
    uid: 8b896297-2f5a-4a9e-9590-61d53a9c5c44
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-kw9ms
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-spot'
          HOSTGROUP='k8s-node-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:23:14Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:14Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "2637377256604986330"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:49Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"3f9f925e-432a-4cc0-b655-689ed68fe507"}: {}
            k:{"uid":"c294e224-35bf-4b9c-b059-2959d954afe2"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:52Z"
    name: node-spot-usw2b-xv8zl
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2b-xv8zl-0
      uid: 3f9f925e-432a-4cc0-b655-689ed68fe507
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2b-xv8zl-1
      uid: c294e224-35bf-4b9c-b059-2959d954afe2
    resourceVersion: "3417160658"
    uid: 7d635c0b-eed0-4b54-9a98-e0fe7dca7f36
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-spot'
          HOSTGROUP='k8s-node-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:51Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:51Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:51Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:51Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "3672137476963783463"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:41Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:41Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:44Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9fcf568b-04f5-430e-8f41-03ae7e9e1dd6"}: {}
            k:{"uid":"652720a8-bc26-4f8e-9caf-1f8961e82501"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:39:37Z"
    name: node-spot-usw2c-ltnzv
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2c-ltnzv-0
      uid: 9fcf568b-04f5-430e-8f41-03ae7e9e1dd6
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2c-ltnzv-1
      uid: 652720a8-bc26-4f8e-9caf-1f8961e82501
    resourceVersion: "3417161921"
    uid: 2ec74734-133c-41b6-bbdf-251da86dc55a
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-spot'
          HOSTGROUP='k8s-node-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:42Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:42Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:43Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:43Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "17414782135767829584"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:53Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-tnfz6
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-tnfz6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:53Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:55Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"83438ef1-8b32-4baf-8287-ede8a87493c9"}: {}
            k:{"uid":"87439c4e-d3b9-4aa4-9599-cdd70ea99dd6"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:27:50Z"
    name: node-spot-usw2c-tnfz6
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2c-tnfz6-1
      uid: 83438ef1-8b32-4baf-8287-ede8a87493c9
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-spot-usw2c-tnfz6-0
      uid: 87439c4e-d3b9-4aa4-9599-cdd70ea99dd6
    resourceVersion: "3416876260"
    uid: d8cc615d-2500-4278-aec2-1ccfa9792afa
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node-spot
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-tnfz6
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node-spot'
          HOSTGROUP='k8s-node-spot'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='1'
          VOLUME_SIZE='200'
          SPOT_DRAIN_ON_REBALANCE='true'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: compute.zende.sk/nodegroup
        effect: NoSchedule
        value: node-spot
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": true,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node_spot.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "node_labels": {"StackName":"k8s-node-spot","node-type":"node-spot","node-role.kubernetes.compute.zende.sk/node-spot":"true","node.kubernetes.io/role":"node-spot","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "compute.zende.sk/nodegroup=node-spot:NoSchedule,karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            }
          }
        },
        "description": "Zendesk Kubernetes Spot Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node_spot"
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[k8s_node_spot],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "6874344816486692784"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:14Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-fm9ph
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-fm9ph
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:16Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:24Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"67ac0e4c-c8f7-4414-b4ed-08a17e6d3133"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:30Z"
    name: node-usw2a-fm9ph
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2a-fm9ph-0
      uid: 67ac0e4c-c8f7-4414-b4ed-08a17e6d3133
    resourceVersion: "3416859564"
    uid: 796b52ea-3cbc-434e-8dcc-41947aa8f016
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-fm9ph
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:15Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:15Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "5868342851000388129"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:56Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-z647p
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:56Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"69ad8568-6057-4b0f-8ce7-a13867e4c859"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:56Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:57Z"
    name: node-usw2a-z647p
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2a-z647p-0
      uid: 69ad8568-6057-4b0f-8ce7-a13867e4c859
    resourceVersion: "3417160897"
    uid: 8e572995-9b0b-417e-8d04-ee877ae158ae
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2a
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0ceb36cbc6aa71a33
    tags:
      AvailabilityZone: us-west-2a
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0ceb36cbc6aa71a33
      zone: us-west-2a
      zoneID: usw2-az1
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "7173101074718267904"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:35Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:35Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:35Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:36Z"
    name: node-usw2b-n7st7
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2b-n7st7-0
      uid: b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364
    resourceVersion: "3417159932"
    uid: 50ac9194-31c5-4592-9a70-6588372cbd53
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "12653968775431993852"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:25Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-vthv2
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-vthv2
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:30Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"fe0fe65f-4b71-4418-acc1-cc35cb8bd408"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:30Z"
    name: node-usw2b-vthv2
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2b-vthv2-0
      uid: fe0fe65f-4b71-4418-acc1-cc35cb8bd408
    resourceVersion: "3416859601"
    uid: a58f3ba6-6906-4d70-a342-c6aecf93eec4
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2b
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-04fdebd9aefa66024
    tags:
      AvailabilityZone: us-west-2b
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-vthv2
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-04fdebd9aefa66024
      zone: us-west-2b
      zoneID: usw2-az2
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "3195692703963473455"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-22T22:24:56Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2c-8ngx9
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-8ngx9
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:56Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d0507415-a8cb-422d-8b7a-595237ababd0"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T22:24:56Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T22:24:58Z"
    name: node-usw2c-8ngx9
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2c-8ngx9-0
      uid: d0507415-a8cb-422d-8b7a-595237ababd0
    resourceVersion: "3416861527"
    uid: 114228f4-c4b0-4ff2-9c6f-2b43c6335198
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-8ngx9
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "5282697677238647102"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2025-12-22T23:50:30Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-hqtql
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hqtql
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2025-12-22T23:50:30Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"86d8bfff-338b-4304-8af2-67213f7c3d82"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-12-22T23:50:30Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-13T14:14:43Z"
    name: node-usw2c-hqtql
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2c-hqtql-0
      uid: 86d8bfff-338b-4304-8af2-67213f7c3d82
    resourceVersion: "3397602334"
    uid: 69b5012b-84be-4ee6-b9d7-1812815c9b71
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0bdd0b7aa95cc1fbb
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v737
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hqtql
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"04661458","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    conditions:
    - lastTransitionTime: "2025-12-22T23:50:32Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2025-12-22T23:50:32Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2025-12-22T23:50:32Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-13T14:14:43Z"
      message: AMISelector did not match any AMIs
      observedGeneration: 1
      reason: AMINotFound
      status: "False"
      type: AMIsReady
    - lastTransitionTime: "2026-01-13T14:14:43Z"
      message: Awaiting AMI, Instance Profile, Security Group, and Subnet resolution
      observedGeneration: 1
      reason: DependenciesNotReady
      status: "False"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-13T14:14:43Z"
      message: ValidationSucceeded=False, AMIsReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "3001288671314827405"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2026-01-23T01:38:42Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2c-hsk2x
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hsk2x
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:42Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"408e6907-5fe0-4082-855a-2d346a13a581"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:42Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:44Z"
    name: node-usw2c-hsk2x
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2c-hsk2x-0
      uid: 408e6907-5fe0-4082-855a-2d346a13a581
    resourceVersion: "3417160242"
    uid: 14155b1b-1903-4d4d-b64a-f6fd6f174176
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-09e9717e54a1307d5
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v740
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hsk2x
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"b7f462ec","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-09e9717e54a1307d5
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-b7f462ec-20260120T052957
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
- apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    annotations:
      karpenter.k8s.aws/ec2nodeclass-hash: "2244640841441788935"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
    creationTimestamp: "2025-12-22T18:41:28Z"
    finalizers:
    - karpenter.k8s.aws/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
          f:finalizers:
            .: {}
            v:"karpenter.k8s.aws/termination": {}
      manager: karpenter
      operation: Update
      time: "2025-12-22T18:41:28Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d1f2a780-9351-4b41-aa62-c3a049e3d0a6"}: {}
        f:spec:
          .: {}
          f:amiFamily: {}
          f:amiSelectorTerms: {}
          f:blockDeviceMappings: {}
          f:capacityReservationSelectorTerms: {}
          f:instanceProfile: {}
          f:kubelet:
            .: {}
            f:evictionHard:
              .: {}
              f:memory.available: {}
            f:systemReserved:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:metadataOptions:
            .: {}
            f:httpEndpoint: {}
            f:httpProtocolIPv6: {}
            f:httpPutResponseHopLimit: {}
            f:httpTokens: {}
          f:securityGroupSelectorTerms: {}
          f:subnetSelectorTerms: {}
          f:tags:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:Hostgroup: {}
            f:KubernetesCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:kube_cluster: {}
            f:kube_cluster_id: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/controller-version: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:service: {}
            f:ssh-teams: {}
            f:ssh-teams-sudo: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
          f:userData: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-12-22T18:41:28Z"
    - apiVersion: karpenter.k8s.aws/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:amis: {}
          f:conditions: {}
          f:instanceProfile: {}
          f:securityGroups: {}
          f:subnets: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-12-22T18:41:30Z"
    name: node-usw2c-twg9f
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      kind: NodePool
      name: node-usw2c-twg9f-0
      uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
    resourceVersion: "3351656992"
    uid: 67fb7275-d1ad-4bec-87a0-ae4a7333caed
  spec:
    amiFamily: Custom
    amiSelectorTerms:
    - id: ami-0eb0affd0e3dd8cfb
    blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        encrypted: true
        iops: 16000
        throughput: 500
        volumeSize: 200Gi
        volumeType: gp3
    capacityReservationSelectorTerms:
    - tags:
        StackName: k8s-node
        team: compute
        topology.kubernetes.io/zone: us-west-2c
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    kubelet:
      evictionHard:
        memory.available: 600Mi
      systemReserved:
        cpu: 250m
        memory: 1024Mi
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 1
      httpTokens: optional
    securityGroupSelectorTerms:
    - id: sg-3987c842
    - id: sg-5e85ca25
    - id: sg-0d8a61c8099ded975
    subnetSelectorTerms:
    - id: subnet-0fd39e6989ec2974a
    tags:
      AvailabilityZone: us-west-2c
      Environment: staging
      Hostgroup: k8s-node
      KubernetesCluster: sandbox
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      kube_cluster: sandbox
      kube_cluster_id: sandbox
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/controller-version: v737
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      service: kubernetes
      ssh-teams: ':compute:sre-dart:secure:network:engineering:'
      ssh-teams-sudo: ':compute:sre-dart:secure:network:engineering:'
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    userData: |
      Content-Type: multipart/mixed; boundary="===============BOUNDARY=="
      MIME-Version: 1.0

      --===============BOUNDARY==
      Content-Type: text/cloud-config; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="cloud-config.txt"

      #cloud-config
      bootcmd:
        - mkdir -p /run/zendesk
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/environment
          STACK_NAME='node'
          HOSTGROUP='k8s-node'
          KUBE_CLUSTER='sandbox'
          EBS_INITIAL_BURST='true'
          EBS_TYPE='gp3'
          BASE_IOPS='3000'
          BASE_THROUGHPUT='125'
          SIZE_PER_CORE_MULTIPLIER='0'
          VOLUME_SIZE='200'
          ENVIRONMENT
        - |
          cat <<'ENVIRONMENT' > /run/zendesk/consul
          CONSUL_DC='usw2-staging-pod998'
          CONSUL_ACL_DATACENTER='usw2-staging-pod998'
          CONSUL_ACL_TOKEN='2b4d09aa-ce9a-1c60-97f0-448119765bb5'
          CONSUL_JOIN='["consul.usw2.zdsystest.com"]'
          CONSUL_USE_DNS_CACHE='true'
          ENVIRONMENT

      --===============BOUNDARY==
      Content-Type: text/x-shellscript; charset="us-ascii"
      MIME-Version: 1.0
      Content-Transfer-Encoding: 7bit
      Content-Disposition: attachment; filename="userdata.sh"

      #!/usr/bin/env bash
      set -exo pipefail
      exec >> '/var/log/first-boot.log' 2>&1
      SECONDS=0 # set special SECONDS variable in bash we can use to time stuff
      AWS_REGION=$(cloud-init query v1.region)

      cat <<'KUBELET' > /etc/kubernetes/kubelet.conf.d/300-user-data-provided.conf
      ---
      apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      registerWithTaints:
      - key: kube-node-monitor
        value: notready
        effect: NoSchedule
      - key: karpenter.sh/unregistered
        effect: NoExecute
      podPidsLimit: 8192
      allowedUnsafeSysctls:
      - net.core.somaxconn
      - net.ipv4.tcp_tw_reuse
      - net.ipv4.tcp_tw_recycle
      - net.ipv4.tcp_timestamps
      - net.ipv4.tcp_max_syn_backlog
      - net.ipv4.tcp_synack_retries
      - net.ipv4.tcp_fin_timeout
      - net.ipv4.tcp_congestion_control

      KUBELET


      # Handle coredns dynamic configuration
      cat <<'RRL' > /etc/coredns/corefile.conf.d/001-rrl
      (rrl) {
        rrl . {
            # calculate every x seconds (sliding window)
            window 1

            # the prefix LENGTH in bits to use for identifying a ipv4 client. Default 24.
            # we want stats and rate limiting per ip, so we set the full ip length (32 bit)
            ipv4-prefix-length 32

            # forwarded requests allowed per pod and second (since we set prefix-length to 32 and 1s window)
            # aws blocks at 1024/s per ENI (10-30 ips (= pods) depending on node type)
            # end goal is to lower coredns.forward_healthcheck_failure_count (dns request failures)
            requests-per-second 100

            # remove to actually block and not just log
            # report-only
        }
      }

      RRL


      echo 'k8s-node' > /srv/hostgroup

      # pull in helper functions and other variables
      source /opt/zendesk/bootstrap/user_data_static.sh
      /opt/zendesk/bootstrap/user_data_static.sh

      trap err_handler ERR
      trap exit_handler EXIT

      # Remove sudocop for non-production instances
      {
        rm -rf /usr/local/bin/sudo
      }

      # Store launch time
      {
        start=$(imds latest/dynamic/instance-identity/document | jq '.pendingTime | fromdate' -r)
        printf -v now '%(%s)T\n'
        export DURATION_LAUNCH=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # configure chef, and run the recipes
      {

        cat > "/var/chef/roles/k8s_aws_bootstrap.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "datadog": {
            "non_local_traffic": true
          },
          "td_agent": {
            "user": "root",
            "group": "root"
          },
          "zendesk_base": {
            "ldap_enabled": "true"
          },
          "zendesk_kubernetes": {
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_rbenv": {
            "default_ruby": "2.6.6"
          },
          "zendesk_vault": {
            "bootstrap": {
              "include_legacy": "true"
            },
            "enable_hvault_for_secrets": true,
            "hvault_role": "paas",
            "default_instance": "master",
            "aws_ec2_auth_enabled": true
          }
        },
        "description": "Role to bootstrap a kubernetes instance in AWS",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_aws_bootstrap",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::remove_install_users]",
          "recipe[zendesk_kubernetes::sudo]"
        ]
      }

      EOF

        cat > "/var/chef/roles/k8s_node.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_consul": {
            "client_addr": "0.0.0.0",
            "limits": {
              "rpc_rate": 1000
            }
          },
          "zendesk_kubernetes": {
            "spot_shutdown": false,
            "kubelet": {
              "node_labels": {"StackName":"k8s-node","node-type":"node","node-role.kubernetes.compute.zende.sk/node":"true","node.kubernetes.io/role":"node","compute.zende.sk/kube-proxy-mode":"daemonset","ownedby":"compute","compute.zendesk.com/ec2_image_sha":"04661458","compute.zende.sk/ebs-initial-burst":"enabled"},
              "extra-taints": "karpenter.sh/unregistered:NoExecute",
              "immutable_eni_hostgroup": true
            },
            "role": "node",
            "vault_role": "node"
          }
        },
        "description": "Zendesk Kubernetes (Worker) Node",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "k8s_node",
        "override_attributes": {},
        "run_list": [
          "recipe[zendesk_kubernetes::node]",
          "recipe[zendesk_kubernetes::ec2]",
          "recipe[zendesk_kubernetes::wait_for_convergeable]"
        ]
      }

      EOF

        cat > "/var/chef/roles/env_staging.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "kubelet": {
              "pod_pids_limit": 8192,
              "network_sysctls": true
            },
            "etcd": {
              "s3_backup_bucket": "zendesk-k8s-etcd-backups-non-prod-e03d7316"
            }
          },
          "zendesk_vault": {
            "vault_endpoint": "https://secret.zdsystest.com:8200"
          }
        },
        "description": "Staging role",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "env_staging",
        "override_attributes": {},
        "run_list": []
      }

      EOF

        cat > "/var/chef/roles/cluster_sandbox.json" << EOF
      {
        "chef_type": "role",
        "default_attributes": {
          "zendesk_kubernetes": {
            "feature_gates": "InPlacePodVerticalScaling=true,MultiCIDRServiceAllocator=true,DisableAllocatorDualWrite=true",
            "pki_suffix": "",
            "api_server": {
              "api_url": "https://k8s-api.sandbox.usw2.zdsystest.com",
              "service_cidr": "172.29.4.0/22",
              "cpu_request": 2,
              "extra_flags": {
                "enable-aggregator-routing": "true",
                "goaway-chance": "0.001",
                "runtime-config": "networking.k8s.io/v1beta1=true"
              }
            },
            "kubelet": {
              "enable_containerd_image_registry": true
            },
            "kube_proxy": {
              "method": "daemonset"
            },
            "cluster": "sandbox",
            "etcd": {
              "dns_zone": "k8s-etcd.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "etcd-events": {
              "dns_zone": "k8s-etcd-events.sandbox.usw2.zdsystest.com",
              "state": "existing"
            },
            "coredns": {
              "forward_rate_limit": 100,
              "throttle_above_forward_rate_limit": true
            }
          }
        },
        "description": "Zendesk Kubernetes sandbox Cluster",
        "env_run_lists": {},
        "json_class": "Chef::Role",
        "name": "cluster_sandbox",
        "override_attributes": {},
        "run_list": []
      }

      EOF


        }

      # time how long it took to get to here
      {
        printf -v now '%(%s)T\n'
        export DURATION_SCRIPTS=$((now-start))
        printf -v start '%(%s)T\n'
      }

      # run chef-client
      {
        chef_runlist="role[k8s_aws_bootstrap],role[k8s_node],role[env_staging],role[cluster_sandbox]"
        printinfo "Running chef with runlist: $chef_runlist"
        /usr/bin/chef-client \
          --local-mode \
          --config=/etc/cinc/client.rb \
          --runlist="$chef_runlist" \
          > /var/log/chef-zero.log
      }

      # time how long chef took
      {
        chef_end=$(date --date $(cat "/var/log/chef-client.log" | grep 'Run complete in' | head -1 | sed -r 's/.(.{19}).*/\1/') +%s)
        export DURATION_CHEF=$((chef_end-start))
        printf -v now '%(%s)T\n'
        export DURATION_CONVERGE=$((now-chef_end))
      }

      # clean up chef environment if not testing
      {
        if [[ -z $TESTING ]]; then
          rm -rf /opt/cookbooks.tgz /etc/chef /var/chef /etc/cinc
          apt-get remove cinc
        fi
      }

      printinfo "Bootstrap script completed successfully"

      --===============BOUNDARY==--
  status:
    amis:
    - id: ami-0eb0affd0e3dd8cfb
      name: ubuntu22.04_k8s_base_singlemount_etcd_3_6_k8s_1_34-04661458-20251222T171006
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values:
        - amd64
    conditions:
    - lastTransitionTime: "2025-12-22T18:41:29Z"
      message: ""
      observedGeneration: 1
      reason: AMIsReady
      status: "True"
      type: AMIsReady
    - lastTransitionTime: "2025-12-22T18:41:29Z"
      message: ""
      observedGeneration: 1
      reason: SubnetsReady
      status: "True"
      type: SubnetsReady
    - lastTransitionTime: "2025-12-22T18:41:29Z"
      message: ""
      observedGeneration: 1
      reason: SecurityGroupsReady
      status: "True"
      type: SecurityGroupsReady
    - lastTransitionTime: "2025-12-22T18:41:29Z"
      message: ""
      observedGeneration: 1
      reason: InstanceProfileReady
      status: "True"
      type: InstanceProfileReady
    - lastTransitionTime: "2025-12-22T18:41:30Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-12-22T18:41:30Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    instanceProfile: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
    securityGroups:
    - id: sg-0d8a61c8099ded975
      name: kubernetes-sandbox-cluster-node-NodeSecurityGroup-144XY5YWM78JB
    - id: sg-3987c842
      name: security-groups-AllowConsulEc2SecurityGroup-8RK04HGLMKGQ
    - id: sg-5e85ca25
      name: security-groups-AppEc2SecurityGroup-1CGB1X5NSS9FS
    subnets:
    - id: subnet-0fd39e6989ec2974a
      zone: us-west-2c
      zoneID: usw2-az3
instanceTypes: []
kind: ClusterSnapshot
metadata:
  capturedAt: "2026-01-28T16:02:46.176037+11:00"
  clusterName: sandbox-admin
  context: sandbox-admin
  region: ""
nodeClaims:
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "10678226489509741797"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "8573347850522616499"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-23T01:39:35Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-arm-usw2a-thmxj-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2a-thmxj
      karpenter.k8s.aws/ec2nodeclass: node-arm-usw2a-thmxj
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: m
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: aws
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: m6g
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "32768"
      karpenter.k8s.aws/instance-network-bandwidth: "2500"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-arm-usw2a-thmxj-0
      kubernetes.io/arch: arm64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-thmxj
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"a8d43df6-1126-4442-b581-958c01814bdf"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:40:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:03:41Z"
    name: node-arm-usw2a-thmxj-0-mfgc9
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-arm-usw2a-thmxj-0
      uid: a8d43df6-1126-4442-b581-958c01814bdf
    resourceVersion: "3427194354"
    uid: 265d2718-57e8-4818-94bb-2e7893b52fab
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-arm-usw2a-thmxj
    requirements:
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-arm-usw2a-thmxj
    - key: Release
      operator: In
      values:
      - v2590
    - key: StackName
      operator: In
      values:
      - k8s-node-arm
    - key: Environment
      operator: In
      values:
      - staging
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c6g.4xlarge
      - c6gd.4xlarge
      - c6gn.4xlarge
      - c7g.4xlarge
      - c7gd.4xlarge
      - c7gn.4xlarge
      - c8g.4xlarge
      - m6g.2xlarge
      - m6g.4xlarge
      - m6gd.2xlarge
      - m6gd.4xlarge
      - m7g.2xlarge
      - m7g.4xlarge
      - m7gd.2xlarge
      - m7gd.4xlarge
      - m8g.2xlarge
      - m8g.4xlarge
      - r6g.2xlarge
      - r6g.4xlarge
      - r6gd.2xlarge
      - r6gd.4xlarge
      - r7g.2xlarge
      - r7g.4xlarge
      - r7gd.2xlarge
      - r7gd.4xlarge
      - r8g.2xlarge
      - r8g.4xlarge
      - r8gd.2xlarge
      - r8gd.4xlarge
      - x2gd.2xlarge
      - x2gd.4xlarge
    - key: service
      operator: In
      values:
      - kubernetes
    - key: product
      operator: In
      values:
      - foundation
    - key: nodegroupDeployment
      operator: In
      values:
      - node-arm-usw2a
    - key: node-type
      operator: In
      values:
      - node-arm
    - key: team
      operator: In
      values:
      - compute
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-arm-usw2a-thmxj
    - key: provider
      operator: In
      values:
      - sfn
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-arm-usw2a-thmxj-0
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-arm
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: node-role.kubernetes.compute.zende.sk/node-arm
      operator: In
      values:
      - "true"
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-arm-usw2a-thmxj
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: ownedby
      operator: In
      values:
      - compute
    resources:
      requests:
        cpu: 821m
        memory: "3823825152"
        pods: "12"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "27523987375"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 32120156Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-23T01:42:22Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-23T01:41:30Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-23T01:39:42Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-23T01:41:30Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    - lastTransitionTime: "2026-01-23T01:40:36Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    imageID: ami-0f6f1d1eb63733e49
    lastPodEventTime: "2026-01-28T03:03:41Z"
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-0be8cb19d8c2b2a0f
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "5501366593409000120"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "12003469939111056955"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-23T01:39:35Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-arm-usw2b-xhg9c-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2b-xhg9c
      karpenter.k8s.aws/ec2nodeclass: node-arm-usw2b-xhg9c
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: m
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: aws
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: m6g
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "32768"
      karpenter.k8s.aws/instance-network-bandwidth: "2500"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-arm-usw2b-xhg9c-0
      kubernetes.io/arch: arm64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xhg9c
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az2
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"c2a5d1ef-aaf1-45d9-95bb-fdb78eea4277"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:40:51Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:04:45Z"
    name: node-arm-usw2b-xhg9c-0-sgg7r
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-arm-usw2b-xhg9c-0
      uid: c2a5d1ef-aaf1-45d9-95bb-fdb78eea4277
    resourceVersion: "3427195983"
    uid: 8b610fe2-81ee-4196-8d88-087992e03e45
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-arm-usw2b-xhg9c
    requirements:
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-arm
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c6g.4xlarge
      - c6gd.4xlarge
      - c6gn.4xlarge
      - c7g.4xlarge
      - c7gd.4xlarge
      - c7gn.4xlarge
      - c8g.4xlarge
      - m6g.2xlarge
      - m6g.4xlarge
      - m6gd.2xlarge
      - m6gd.4xlarge
      - m7g.2xlarge
      - m7g.4xlarge
      - m7gd.2xlarge
      - m7gd.4xlarge
      - m8g.2xlarge
      - m8g.4xlarge
      - r6g.2xlarge
      - r6g.4xlarge
      - r6gd.2xlarge
      - r6gd.4xlarge
      - r7g.2xlarge
      - r7g.4xlarge
      - r7gd.2xlarge
      - r7gd.4xlarge
      - r8g.2xlarge
      - r8g.4xlarge
      - r8gd.2xlarge
      - r8gd.4xlarge
      - x2gd.2xlarge
      - x2gd.4xlarge
    - key: service
      operator: In
      values:
      - kubernetes
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2b
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-arm-usw2b-xhg9c-0
    - key: Release
      operator: In
      values:
      - v2590
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-arm-usw2b-xhg9c
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: node-type
      operator: In
      values:
      - node-arm
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: ownedby
      operator: In
      values:
      - compute
    - key: team
      operator: In
      values:
      - compute
    - key: product
      operator: In
      values:
      - foundation
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: provider
      operator: In
      values:
      - sfn
    - key: nodegroupDeployment
      operator: In
      values:
      - node-arm-usw2b
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2b
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-arm-usw2b-xhg9c
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-arm-usw2b-xhg9c
    - key: StackName
      operator: In
      values:
      - k8s-node-arm
    - key: Environment
      operator: In
      values:
      - staging
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: node-role.kubernetes.compute.zende.sk/node-arm
      operator: In
      values:
      - "true"
    resources:
      requests:
        cpu: 821m
        memory: "3823825152"
        pods: "12"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "27523987375"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 32120156Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-23T01:39:40Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-23T01:40:36Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-23T01:41:29Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-23T01:42:22Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-23T01:41:29Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-0f6f1d1eb63733e49
    lastPodEventTime: "2026-01-28T03:04:45Z"
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    providerID: aws:///us-west-2b/i-064c2e3e5ae6fa9ad
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "15584009246137945918"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "15838600678817066645"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-23T01:39:35Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-arm-usw2c-rpkp2-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2c-rpkp2
      karpenter.k8s.aws/ec2nodeclass: node-arm-usw2c-rpkp2
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: m
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: aws
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: m6g
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "32768"
      karpenter.k8s.aws/instance-network-bandwidth: "2500"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-arm-usw2c-rpkp2-0
      kubernetes.io/arch: arm64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-rpkp2
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"e3c259f9-9607-4bbe-92e4-2096bc4a155f"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:40:39Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:02:58Z"
    name: node-arm-usw2c-rpkp2-0-kkhhn
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-arm-usw2c-rpkp2-0
      uid: e3c259f9-9607-4bbe-92e4-2096bc4a155f
    resourceVersion: "3427192942"
    uid: cf89dfcc-2d7b-43df-88a3-d18a714da117
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-arm-usw2c-rpkp2
    requirements:
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: node-type
      operator: In
      values:
      - node-arm
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: Environment
      operator: In
      values:
      - staging
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: product
      operator: In
      values:
      - foundation
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: provider
      operator: In
      values:
      - sfn
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-arm-usw2c-rpkp2
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-arm
    - key: nodegroupDeployment
      operator: In
      values:
      - node-arm-usw2c
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-arm-usw2c-rpkp2
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c6g.4xlarge
      - c6gd.4xlarge
      - c6gn.4xlarge
      - c7g.4xlarge
      - c7gd.4xlarge
      - c7gn.4xlarge
      - c8g.4xlarge
      - m6g.2xlarge
      - m6g.4xlarge
      - m6gd.2xlarge
      - m6gd.4xlarge
      - m7g.2xlarge
      - m7g.4xlarge
      - m7gd.2xlarge
      - m7gd.4xlarge
      - m8g.2xlarge
      - m8g.4xlarge
      - r6g.2xlarge
      - r6g.4xlarge
      - r6gd.2xlarge
      - r6gd.4xlarge
      - r7g.2xlarge
      - r7g.4xlarge
      - r7gd.2xlarge
      - r7gd.4xlarge
      - r8g.2xlarge
      - r8g.4xlarge
      - r8gd.2xlarge
      - r8gd.4xlarge
      - x2gd.2xlarge
      - x2gd.4xlarge
    - key: ownedby
      operator: In
      values:
      - compute
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-arm-usw2c-rpkp2-0
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: service
      operator: In
      values:
      - kubernetes
    - key: team
      operator: In
      values:
      - compute
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: Release
      operator: In
      values:
      - v2590
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: StackName
      operator: In
      values:
      - k8s-node-arm
    - key: node-role.kubernetes.compute.zende.sk/node-arm
      operator: In
      values:
      - "true"
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-arm-usw2c-rpkp2
    resources:
      requests:
        cpu: 821m
        memory: "3823825152"
        pods: "12"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "27523987375"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 32120156Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-23T01:42:22Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-23T01:41:31Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-23T01:39:45Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-23T01:41:31Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    - lastTransitionTime: "2026-01-23T01:40:34Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    imageID: ami-0f6f1d1eb63733e49
    lastPodEventTime: "2026-01-28T03:02:58Z"
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-0a58c862ae9a8aadd
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "4297614907259343839"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "14444235493744154215"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"karpenter.sh/v1","kind":"NodeClaim","metadata":{"annotations":{"compatibility.karpenter.k8s.aws/cluster-name-tagged":"true","k8s.io/cluster-autoscaler-enabled":"false","karpenter.k8s.aws/ec2nodeclass-hash":"4297614907259343839","karpenter.k8s.aws/ec2nodeclass-hash-version":"v4","karpenter.k8s.aws/instance-profile-name":"kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd","karpenter.k8s.aws/tagged":"true","karpenter.sh/nodeclaim-min-values-relaxed":"false","karpenter.sh/nodepool-hash":"14444235493744154215","karpenter.sh/nodepool-hash-version":"v3","karpenter.sh/price-overlay-applied":"true"},"labels":{"AvailabilityZone":"us-west-2a","Environment":"staging","PoddedCluster":"false","Release":"v2589","SfnKubernetes":"7619591bc433d7dc286332e1d8aea7c9df2420b7","StackName":"k8s-node-nvidia-gpu","compute.zende.sk/ebs-initial-burst":"enabled","compute.zende.sk/instance-gpu-manufacturer":"dra","compute.zende.sk/kube-proxy-mode":"daemonset","compute.zendesk.com/ec2_image_sha":"b7f462ec","k8s.amazonaws.com/eniConfig":"node-nvidia-gpu-rajeesh-usw2a-8ck9g","karpenter.k8s.aws/ec2nodeclass":"node-nvidia-gpu-rajeesh-usw2a-8ck9g","karpenter.k8s.aws/instance-capability-flex":"false","karpenter.k8s.aws/instance-category":"g","karpenter.k8s.aws/instance-cpu":"4","karpenter.k8s.aws/instance-cpu-manufacturer":"intel","karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz":"2500","karpenter.k8s.aws/instance-ebs-bandwidth":"3500","karpenter.k8s.aws/instance-encryption-in-transit-supported":"true","karpenter.k8s.aws/instance-family":"g4dn","karpenter.k8s.aws/instance-generation":"4","karpenter.k8s.aws/instance-gpu-count":"1","karpenter.k8s.aws/instance-gpu-manufacturer":"nvidia","karpenter.k8s.aws/instance-gpu-memory":"16384","karpenter.k8s.aws/instance-gpu-name":"t4","karpenter.k8s.aws/instance-hypervisor":"nitro","karpenter.k8s.aws/instance-local-nvme":"125","karpenter.k8s.aws/instance-memory":"16384","karpenter.k8s.aws/instance-network-bandwidth":"5000","karpenter.k8s.aws/instance-size":"xlarge","karpenter.k8s.aws/instance-tenancy":"default","karpenter.sh/capacity-type":"on-demand","karpenter.sh/nodepool":"node-nvidia-gpu-rajeesh-usw2a-8ck9g-0","kubernetes.io/arch":"amd64","kubernetes.io/os":"linux","managed-by":"karpenter","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node-type":"node-nvidia-gpu","node.kubernetes.io/instance-type":"g4dn.xlarge","node.kubernetes.io/role":"node-nvidia-gpu","nodegroup-operator.compute.zende.sk/cluster-name":"sandbox","nodegroup-operator.compute.zende.sk/nodegroup-name":"node-nvidia-gpu-rajeesh-usw2a-8ck9g","nodegroup-provider":"karpenter","nodegroupDeployment":"node-nvidia-gpu-rajeesh-usw2a","ownedby":"compute","product":"foundation","project":"sfn_kubernetes","provider":"sfn","service":"kubernetes","team":"compute","topology.k8s.aws/zone-id":"usw2-az1","topology.kubernetes.io/region":"us-west-2","topology.kubernetes.io/zone":"us-west-2a"},"name":"node-nvidia-gpu-rajeesh-usw2a-8ck9g-0-nc1"},"spec":{"expireAfter":"Never","nodeClassRef":{"group":"karpenter.k8s.aws","kind":"EC2NodeClass","name":"node-nvidia-gpu-rajeesh-usw2a-8ck9g"},"requirements":[{"key":"node.kubernetes.io/role","operator":"In","values":["node-nvidia-gpu"]},{"key":"project","operator":"In","values":["sfn_kubernetes"]},{"key":"nodegroup-operator.compute.zende.sk/cluster-name","operator":"In","values":["sandbox"]},{"key":"compute.zende.sk/ebs-initial-burst","operator":"In","values":["enabled"]},{"key":"ownedby","operator":"In","values":["compute"]},{"key":"Release","operator":"In","values":["v2589"]},{"key":"karpenter.sh/capacity-type","operator":"In","values":["on-demand"]},{"key":"nodegroup-operator.compute.zende.sk/nodegroup-name","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g"]},{"key":"node-type","operator":"In","values":["node-nvidia-gpu"]},{"key":"PoddedCluster","operator":"In","values":["false"]},{"key":"team","operator":"In","values":["compute"]},{"key":"k8s.amazonaws.com/eniConfig","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g"]},{"key":"node-role.kubernetes.compute.zende.sk/node-nvidia-gpu","operator":"In","values":["true"]},{"key":"nodegroupDeployment","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a"]},{"key":"node.kubernetes.io/instance-type","operator":"In","values":["g4dn.2xlarge","g4dn.4xlarge","g4dn.xlarge","g5.2xlarge","g5.4xlarge","g5.xlarge","g6.2xlarge","g6.4xlarge","g6.xlarge","g6e.2xlarge","g6e.4xlarge","g6e.xlarge"]},{"key":"compute.zende.sk/instance-gpu-manufacturer","operator":"In","values":["dra"]},{"key":"SfnKubernetes","operator":"In","values":["7619591bc433d7dc286332e1d8aea7c9df2420b7"]},{"key":"karpenter.sh/nodepool","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g-0"]},{"key":"AvailabilityZone","operator":"In","values":["us-west-2a"]},{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-west-2a"]},{"key":"nodegroup-provider","operator":"In","values":["karpenter"]},{"key":"StackName","operator":"In","values":["k8s-node-nvidia-gpu"]},{"key":"compute.zende.sk/kube-proxy-mode","operator":"In","values":["daemonset"]},{"key":"karpenter.k8s.aws/ec2nodeclass","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g"]},{"key":"provider","operator":"In","values":["sfn"]},{"key":"managed-by","operator":"In","values":["karpenter"]},{"key":"compute.zendesk.com/ec2_image_sha","operator":"In","values":["b7f462ec"]},{"key":"service","operator":"In","values":["kubernetes"]},{"key":"Environment","operator":"In","values":["staging"]},{"key":"product","operator":"In","values":["foundation"]}],"resources":{"requests":{"cpu":"786m","memory":"3823972608","pods":"12"}},"startupTaints":[{"effect":"NoSchedule","key":"kube-node-monitor","value":"notready"}],"taints":[{"effect":"NoSchedule","key":"compute.zende.sk/nodegroup","value":"node-nvidia-gpu"}],"terminationGracePeriod":"96h0m0s"}}
    creationTimestamp: "2026-01-28T03:02:58Z"
    finalizers:
    - karpenter.sh/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.k8s.aws/ec2nodeclass: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: g
      karpenter.k8s.aws/instance-cpu: "4"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "3500"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: g4dn
      karpenter.k8s.aws/instance-generation: "4"
      karpenter.k8s.aws/instance-gpu-count: "1"
      karpenter.k8s.aws/instance-gpu-manufacturer: nvidia
      karpenter.k8s.aws/instance-gpu-memory: "16384"
      karpenter.k8s.aws/instance-gpu-name: t4
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-local-nvme: "125"
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "5000"
      karpenter.k8s.aws/instance-size: xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T03:02:58Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-gpu-count: {}
            f:karpenter.k8s.aws/instance-gpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-gpu-memory: {}
            f:karpenter.k8s.aws/instance-gpu-name: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-local-nvme: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2026-01-28T03:02:58Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:04:15Z"
    name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0-nc1
    resourceVersion: "3427195239"
    uid: f8e157a5-d968-4c7e-b5c8-d8ce8109d65d
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    requirements:
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-nvidia-gpu
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: ownedby
      operator: In
      values:
      - compute
    - key: Release
      operator: In
      values:
      - v2589
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: node-type
      operator: In
      values:
      - node-nvidia-gpu
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: team
      operator: In
      values:
      - compute
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: node-role.kubernetes.compute.zende.sk/node-nvidia-gpu
      operator: In
      values:
      - "true"
    - key: nodegroupDeployment
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - g4dn.2xlarge
      - g4dn.4xlarge
      - g4dn.xlarge
      - g5.2xlarge
      - g5.4xlarge
      - g5.xlarge
      - g6.2xlarge
      - g6.4xlarge
      - g6.xlarge
      - g6e.2xlarge
      - g6e.4xlarge
      - g6e.xlarge
    - key: compute.zende.sk/instance-gpu-manufacturer
      operator: In
      values:
      - dra
    - key: SfnKubernetes
      operator: In
      values:
      - 7619591bc433d7dc286332e1d8aea7c9df2420b7
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: StackName
      operator: In
      values:
      - k8s-node-nvidia-gpu
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: provider
      operator: In
      values:
      - sfn
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: service
      operator: In
      values:
      - kubernetes
    - key: Environment
      operator: In
      values:
      - staging
    - key: product
      operator: In
      values:
      - foundation
    resources:
      requests:
        cpu: 786m
        memory: "3823972608"
        pods: "12"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-nvidia-gpu
    terminationGracePeriod: 96h0m0s
  status:
    allocatable:
      cpu: 3670m
      ephemeral-storage: 188Gi
      memory: "12189265921"
      nvidia.com/gpu: "1"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "39"
    capacity:
      cpu: "4"
      ephemeral-storage: 210Gi
      memory: 16167004Ki
      nvidia.com/gpu: "1"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "39"
    conditions:
    - lastTransitionTime: "2026-01-28T03:03:00Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T03:03:55Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T03:04:15Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T03:04:15Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: null
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-07582b05c1d7c23a6
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "4297614907259343839"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "14444235493744154215"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"karpenter.sh/v1","kind":"NodeClaim","metadata":{"annotations":{"compatibility.karpenter.k8s.aws/cluster-name-tagged":"true","k8s.io/cluster-autoscaler-enabled":"false","karpenter.k8s.aws/ec2nodeclass-hash":"4297614907259343839","karpenter.k8s.aws/ec2nodeclass-hash-version":"v4","karpenter.k8s.aws/instance-profile-name":"kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd","karpenter.k8s.aws/tagged":"true","karpenter.sh/nodeclaim-min-values-relaxed":"false","karpenter.sh/nodepool-hash":"14444235493744154215","karpenter.sh/nodepool-hash-version":"v3","karpenter.sh/price-overlay-applied":"true"},"labels":{"AvailabilityZone":"us-west-2a","Environment":"staging","PoddedCluster":"false","Release":"v2589","SfnKubernetes":"7619591bc433d7dc286332e1d8aea7c9df2420b7","StackName":"k8s-node-nvidia-gpu","compute.zende.sk/ebs-initial-burst":"enabled","compute.zende.sk/instance-gpu-manufacturer":"dra","compute.zende.sk/kube-proxy-mode":"daemonset","compute.zendesk.com/ec2_image_sha":"b7f462ec","k8s.amazonaws.com/eniConfig":"node-nvidia-gpu-rajeesh-usw2a-8ck9g","karpenter.k8s.aws/ec2nodeclass":"node-nvidia-gpu-rajeesh-usw2a-8ck9g","karpenter.k8s.aws/instance-capability-flex":"false","karpenter.k8s.aws/instance-category":"g","karpenter.k8s.aws/instance-cpu":"4","karpenter.k8s.aws/instance-cpu-manufacturer":"intel","karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz":"2500","karpenter.k8s.aws/instance-ebs-bandwidth":"3500","karpenter.k8s.aws/instance-encryption-in-transit-supported":"true","karpenter.k8s.aws/instance-family":"g4dn","karpenter.k8s.aws/instance-generation":"4","karpenter.k8s.aws/instance-gpu-count":"1","karpenter.k8s.aws/instance-gpu-manufacturer":"nvidia","karpenter.k8s.aws/instance-gpu-memory":"16384","karpenter.k8s.aws/instance-gpu-name":"t4","karpenter.k8s.aws/instance-hypervisor":"nitro","karpenter.k8s.aws/instance-local-nvme":"125","karpenter.k8s.aws/instance-memory":"16384","karpenter.k8s.aws/instance-network-bandwidth":"5000","karpenter.k8s.aws/instance-size":"xlarge","karpenter.k8s.aws/instance-tenancy":"default","karpenter.sh/capacity-type":"on-demand","karpenter.sh/nodepool":"node-nvidia-gpu-rajeesh-usw2a-8ck9g-0","kubernetes.io/arch":"amd64","kubernetes.io/os":"linux","managed-by":"karpenter","node-role.kubernetes.compute.zende.sk/node-nvidia-gpu":"true","node-type":"node-nvidia-gpu","node.kubernetes.io/instance-type":"g4dn.xlarge","node.kubernetes.io/role":"node-nvidia-gpu","nodegroup-operator.compute.zende.sk/cluster-name":"sandbox","nodegroup-operator.compute.zende.sk/nodegroup-name":"node-nvidia-gpu-rajeesh-usw2a-8ck9g","nodegroup-provider":"karpenter","nodegroupDeployment":"node-nvidia-gpu-rajeesh-usw2a","ownedby":"compute","product":"foundation","project":"sfn_kubernetes","provider":"sfn","service":"kubernetes","team":"compute","topology.k8s.aws/zone-id":"usw2-az1","topology.kubernetes.io/region":"us-west-2","topology.kubernetes.io/zone":"us-west-2a"},"name":"node-nvidia-gpu-rajeesh-usw2a-8ck9g-0-nc2"},"spec":{"expireAfter":"Never","nodeClassRef":{"group":"karpenter.k8s.aws","kind":"EC2NodeClass","name":"node-nvidia-gpu-rajeesh-usw2a-8ck9g"},"requirements":[{"key":"node.kubernetes.io/role","operator":"In","values":["node-nvidia-gpu"]},{"key":"project","operator":"In","values":["sfn_kubernetes"]},{"key":"nodegroup-operator.compute.zende.sk/cluster-name","operator":"In","values":["sandbox"]},{"key":"compute.zende.sk/ebs-initial-burst","operator":"In","values":["enabled"]},{"key":"ownedby","operator":"In","values":["compute"]},{"key":"Release","operator":"In","values":["v2589"]},{"key":"karpenter.sh/capacity-type","operator":"In","values":["on-demand"]},{"key":"nodegroup-operator.compute.zende.sk/nodegroup-name","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g"]},{"key":"node-type","operator":"In","values":["node-nvidia-gpu"]},{"key":"PoddedCluster","operator":"In","values":["false"]},{"key":"team","operator":"In","values":["compute"]},{"key":"k8s.amazonaws.com/eniConfig","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g"]},{"key":"node-role.kubernetes.compute.zende.sk/node-nvidia-gpu","operator":"In","values":["true"]},{"key":"nodegroupDeployment","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a"]},{"key":"node.kubernetes.io/instance-type","operator":"In","values":["g4dn.2xlarge","g4dn.4xlarge","g4dn.xlarge","g5.2xlarge","g5.4xlarge","g5.xlarge","g6.2xlarge","g6.4xlarge","g6.xlarge","g6e.2xlarge","g6e.4xlarge","g6e.xlarge"]},{"key":"compute.zende.sk/instance-gpu-manufacturer","operator":"In","values":["dra"]},{"key":"SfnKubernetes","operator":"In","values":["7619591bc433d7dc286332e1d8aea7c9df2420b7"]},{"key":"karpenter.sh/nodepool","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g-0"]},{"key":"AvailabilityZone","operator":"In","values":["us-west-2a"]},{"key":"topology.kubernetes.io/zone","operator":"In","values":["us-west-2a"]},{"key":"nodegroup-provider","operator":"In","values":["karpenter"]},{"key":"StackName","operator":"In","values":["k8s-node-nvidia-gpu"]},{"key":"compute.zende.sk/kube-proxy-mode","operator":"In","values":["daemonset"]},{"key":"karpenter.k8s.aws/ec2nodeclass","operator":"In","values":["node-nvidia-gpu-rajeesh-usw2a-8ck9g"]},{"key":"provider","operator":"In","values":["sfn"]},{"key":"managed-by","operator":"In","values":["karpenter"]},{"key":"compute.zendesk.com/ec2_image_sha","operator":"In","values":["b7f462ec"]},{"key":"service","operator":"In","values":["kubernetes"]},{"key":"Environment","operator":"In","values":["staging"]},{"key":"product","operator":"In","values":["foundation"]}],"resources":{"requests":{"cpu":"786m","memory":"3823972608","pods":"12"}},"startupTaints":[{"effect":"NoSchedule","key":"kube-node-monitor","value":"notready"}],"taints":[{"effect":"NoSchedule","key":"compute.zende.sk/nodegroup","value":"node-nvidia-gpu"}],"terminationGracePeriod":"96h0m0s"}}
    creationTimestamp: "2026-01-28T03:03:10Z"
    finalizers:
    - karpenter.sh/termination
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.k8s.aws/ec2nodeclass: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: g
      karpenter.k8s.aws/instance-cpu: "4"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "3500"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: g4dn
      karpenter.k8s.aws/instance-generation: "4"
      karpenter.k8s.aws/instance-gpu-count: "1"
      karpenter.k8s.aws/instance-gpu-manufacturer: nvidia
      karpenter.k8s.aws/instance-gpu-memory: "16384"
      karpenter.k8s.aws/instance-gpu-name: t4
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-local-nvme: "125"
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "5000"
      karpenter.k8s.aws/instance-size: xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T03:03:10Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-gpu-count: {}
            f:karpenter.k8s.aws/instance-gpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-gpu-memory: {}
            f:karpenter.k8s.aws/instance-gpu-name: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-local-nvme: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2026-01-28T03:03:10Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:04:38Z"
    name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0-nc2
    resourceVersion: "3427195791"
    uid: bd02431a-e02a-422e-9cd4-5cfacd41ba77
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    requirements:
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-nvidia-gpu
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: ownedby
      operator: In
      values:
      - compute
    - key: Release
      operator: In
      values:
      - v2589
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: node-type
      operator: In
      values:
      - node-nvidia-gpu
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: team
      operator: In
      values:
      - compute
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: node-role.kubernetes.compute.zende.sk/node-nvidia-gpu
      operator: In
      values:
      - "true"
    - key: nodegroupDeployment
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - g4dn.2xlarge
      - g4dn.4xlarge
      - g4dn.xlarge
      - g5.2xlarge
      - g5.4xlarge
      - g5.xlarge
      - g6.2xlarge
      - g6.4xlarge
      - g6.xlarge
      - g6e.2xlarge
      - g6e.4xlarge
      - g6e.xlarge
    - key: compute.zende.sk/instance-gpu-manufacturer
      operator: In
      values:
      - dra
    - key: SfnKubernetes
      operator: In
      values:
      - 7619591bc433d7dc286332e1d8aea7c9df2420b7
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: StackName
      operator: In
      values:
      - k8s-node-nvidia-gpu
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: provider
      operator: In
      values:
      - sfn
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: service
      operator: In
      values:
      - kubernetes
    - key: Environment
      operator: In
      values:
      - staging
    - key: product
      operator: In
      values:
      - foundation
    resources:
      requests:
        cpu: 786m
        memory: "3823972608"
        pods: "12"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-nvidia-gpu
    terminationGracePeriod: 96h0m0s
  status:
    allocatable:
      cpu: 3670m
      ephemeral-storage: 188Gi
      memory: "12189265921"
      nvidia.com/gpu: "1"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "39"
    capacity:
      cpu: "4"
      ephemeral-storage: 210Gi
      memory: 16167004Ki
      nvidia.com/gpu: "1"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "39"
    conditions:
    - lastTransitionTime: "2026-01-28T03:03:12Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T03:04:14Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T03:04:38Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T03:04:38Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: null
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-02ef8ae4559f58e80
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "4297614907259343839"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "14444235493744154215"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T02:58:45Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.k8s.aws/ec2nodeclass: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: g
      karpenter.k8s.aws/instance-cpu: "4"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "3500"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: g4dn
      karpenter.k8s.aws/instance-generation: "4"
      karpenter.k8s.aws/instance-gpu-count: "1"
      karpenter.k8s.aws/instance-gpu-manufacturer: nvidia
      karpenter.k8s.aws/instance-gpu-memory: "16384"
      karpenter.k8s.aws/instance-gpu-name: t4
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-local-nvme: "125"
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "5000"
      karpenter.k8s.aws/instance-size: xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-gpu-count: {}
            f:karpenter.k8s.aws/instance-gpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-gpu-memory: {}
            f:karpenter.k8s.aws/instance-gpu-name: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-local-nvme: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"349eca7e-d998-4f9c-af5d-a8ee1812fa26"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:59:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:04Z"
    name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0-njrnb
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
      uid: 349eca7e-d998-4f9c-af5d-a8ee1812fa26
    resourceVersion: "3427188762"
    uid: 04898adf-4eda-4081-a3c8-a4bc58c1888a
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    requirements:
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-nvidia-gpu
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: ownedby
      operator: In
      values:
      - compute
    - key: Release
      operator: In
      values:
      - v2589
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: node-type
      operator: In
      values:
      - node-nvidia-gpu
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: team
      operator: In
      values:
      - compute
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: node-role.kubernetes.compute.zende.sk/node-nvidia-gpu
      operator: In
      values:
      - "true"
    - key: nodegroupDeployment
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - g4dn.2xlarge
      - g4dn.4xlarge
      - g4dn.xlarge
      - g5.2xlarge
      - g5.4xlarge
      - g5.xlarge
      - g6.2xlarge
      - g6.4xlarge
      - g6.xlarge
      - g6e.2xlarge
      - g6e.4xlarge
      - g6e.xlarge
    - key: compute.zende.sk/instance-gpu-manufacturer
      operator: In
      values:
      - dra
    - key: SfnKubernetes
      operator: In
      values:
      - 7619591bc433d7dc286332e1d8aea7c9df2420b7
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: StackName
      operator: In
      values:
      - k8s-node-nvidia-gpu
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-nvidia-gpu-rajeesh-usw2a-8ck9g
    - key: provider
      operator: In
      values:
      - sfn
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: service
      operator: In
      values:
      - kubernetes
    - key: Environment
      operator: In
      values:
      - staging
    - key: product
      operator: In
      values:
      - foundation
    resources:
      requests:
        cpu: 786m
        memory: "3823972608"
        pods: "12"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-nvidia-gpu
    terminationGracePeriod: 96h0m0s
  status:
    allocatable:
      cpu: 3670m
      ephemeral-storage: 188Gi
      memory: "12189265921"
      nvidia.com/gpu: "1"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "39"
    capacity:
      cpu: "4"
      ephemeral-storage: 210Gi
      memory: 16167004Ki
      nvidia.com/gpu: "1"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "39"
    conditions:
    - lastTransitionTime: "2026-01-28T02:58:47Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T02:59:47Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T03:00:04Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T03:00:04Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T02:59:47Z"
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-0b14e96587341e0d7
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "6707043413459906954"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "11522083887248565427"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T00:25:48Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-spot-usw2a-cschs-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
      karpenter.k8s.aws/ec2nodeclass: node-spot-usw2a-cschs
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: r
      karpenter.k8s.aws/instance-cpu: "32"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: r6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "262144"
      karpenter.k8s.aws/instance-network-bandwidth: "12500"
      karpenter.k8s.aws/instance-size: 8xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: spot
      karpenter.sh/nodepool: node-spot-usw2a-cschs-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"0ff78475-1b22-4fb5-802f-9f7136f6a360"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T00:26:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T00:36:47Z"
    name: node-spot-usw2a-cschs-0-rzw47
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-spot-usw2a-cschs-0
      uid: 0ff78475-1b22-4fb5-802f-9f7136f6a360
    resourceVersion: "3426981222"
    uid: f8c1b2d4-9e60-4ac0-94cc-8cd34bbe2592
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-spot-usw2a-cschs
    requirements:
    - key: StackName
      operator: In
      values:
      - k8s-node-spot
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.12xlarge
      - c5.9xlarge
      - c6i.12xlarge
      - c6i.16xlarge
      - c6i.8xlarge
      - c7i.12xlarge
      - c7i.16xlarge
      - c7i.8xlarge
      - m5.12xlarge
      - m5.16xlarge
      - m5.8xlarge
      - m5d.12xlarge
      - m5d.16xlarge
      - m5d.8xlarge
      - m6i.12xlarge
      - m6i.16xlarge
      - m6i.8xlarge
      - m6id.12xlarge
      - m6id.16xlarge
      - m6id.8xlarge
      - m7i-flex.12xlarge
      - m7i-flex.16xlarge
      - m7i-flex.8xlarge
      - m7i.12xlarge
      - m7i.16xlarge
      - m7i.8xlarge
      - r5.12xlarge
      - r5.16xlarge
      - r5.8xlarge
      - r6i.12xlarge
      - r6i.16xlarge
      - r6i.8xlarge
      - r7i.12xlarge
      - r7i.16xlarge
      - r7i.8xlarge
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: Release
      operator: In
      values:
      - v2590
    - key: team
      operator: In
      values:
      - compute
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-spot
    - key: product
      operator: In
      values:
      - foundation
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-spot-usw2a-cschs-0
    - key: node-type
      operator: In
      values:
      - node-spot
    - key: Environment
      operator: In
      values:
      - staging
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
      - spot
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: service
      operator: In
      values:
      - kubernetes
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: provider
      operator: In
      values:
      - sfn
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-spot-usw2a-cschs
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: node-role.kubernetes.compute.zende.sk/node-spot
      operator: In
      values:
      - "true"
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: ownedby
      operator: In
      values:
      - compute
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-spot-usw2a-cschs
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-spot-usw2a-cschs
    - key: nodegroupDeployment
      operator: In
      values:
      - node-spot-usw2a
    resources:
      requests:
        cpu: 434m
        memory: 2965828Ki
        pods: "11"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-spot
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 31600m
      ephemeral-storage: 179Gi
      memory: "251944003995"
      pods: "250"
      vpc.amazonaws.com/pod-eni: "84"
    capacity:
      cpu: "32"
      ephemeral-storage: 200Gi
      memory: 259756468Ki
      pods: "250"
      vpc.amazonaws.com/pod-eni: "84"
    conditions:
    - lastTransitionTime: "2026-01-28T00:25:51Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T00:26:47Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T00:27:05Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T00:35:51Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T00:36:47Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-28T00:27:05Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T00:26:47Z"
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-09203d47c303c0b85
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "2637377256604986330"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "9229452721016153549"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T01:11:28Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-spot-usw2b-xv8zl-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
      karpenter.k8s.aws/ec2nodeclass: node-spot-usw2b-xv8zl
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "64"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3200"
      karpenter.k8s.aws/instance-ebs-bandwidth: "20000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c7i
      karpenter.k8s.aws/instance-generation: "7"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "131072"
      karpenter.k8s.aws/instance-network-bandwidth: "25000"
      karpenter.k8s.aws/instance-size: 16xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: spot
      karpenter.sh/nodepool: node-spot-usw2b-xv8zl-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az2
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"3f9f925e-432a-4cc0-b655-689ed68fe507"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T01:12:18Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T01:24:51Z"
    name: node-spot-usw2b-xv8zl-0-kqq86
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-spot-usw2b-xv8zl-0
      uid: 3f9f925e-432a-4cc0-b655-689ed68fe507
    resourceVersion: "3427049057"
    uid: 4db7eddc-9a77-4dc6-995b-2f38f29f1640
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-spot-usw2b-xv8zl
    requirements:
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-spot-usw2b-xv8zl
    - key: node-role.kubernetes.compute.zende.sk/node-spot
      operator: In
      values:
      - "true"
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: node-type
      operator: In
      values:
      - node-spot
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: team
      operator: In
      values:
      - compute
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2b
    - key: Release
      operator: In
      values:
      - v2590
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2b
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-spot-usw2b-xv8zl
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.12xlarge
      - c5.9xlarge
      - c6i.12xlarge
      - c6i.16xlarge
      - c6i.8xlarge
      - c7i.12xlarge
      - c7i.16xlarge
      - c7i.8xlarge
      - m5.12xlarge
      - m5.16xlarge
      - m5.8xlarge
      - m5d.12xlarge
      - m5d.16xlarge
      - m5d.8xlarge
      - m6i.12xlarge
      - m6i.16xlarge
      - m6i.8xlarge
      - m6id.12xlarge
      - m6id.16xlarge
      - m6id.8xlarge
      - m7i-flex.12xlarge
      - m7i-flex.16xlarge
      - m7i-flex.8xlarge
      - m7i.12xlarge
      - m7i.16xlarge
      - m7i.8xlarge
      - r5.12xlarge
      - r5.16xlarge
      - r5.8xlarge
      - r6i.12xlarge
      - r6i.16xlarge
      - r6i.8xlarge
      - r7i.12xlarge
      - r7i.16xlarge
      - r7i.8xlarge
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
      - spot
    - key: ownedby
      operator: In
      values:
      - compute
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: product
      operator: In
      values:
      - foundation
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-spot-usw2b-xv8zl-0
    - key: Environment
      operator: In
      values:
      - staging
    - key: provider
      operator: In
      values:
      - sfn
    - key: StackName
      operator: In
      values:
      - k8s-node-spot
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: nodegroupDeployment
      operator: In
      values:
      - node-spot-usw2b
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-spot-usw2b-xv8zl
    - key: service
      operator: In
      values:
      - kubernetes
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-spot
    resources:
      requests:
        cpu: 688m
        memory: "3860140032"
        pods: "11"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-spot
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 63520m
      ephemeral-storage: 179Gi
      memory: "121808400713"
      pods: "250"
      vpc.amazonaws.com/pod-eni: "107"
    capacity:
      cpu: "64"
      ephemeral-storage: 200Gi
      memory: 129785204Ki
      pods: "250"
      vpc.amazonaws.com/pod-eni: "107"
    conditions:
    - lastTransitionTime: "2026-01-28T01:11:31Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T01:12:16Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T01:12:34Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T01:21:31Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T01:24:51Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-28T01:12:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T01:14:51Z"
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    providerID: aws:///us-west-2b/i-022ad1b23bc585a34
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "3672137476963783463"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "12307986074497143793"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T03:00:19Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-spot-usw2c-ltnzv-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
      karpenter.k8s.aws/ec2nodeclass: node-spot-usw2c-ltnzv
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "48"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3600"
      karpenter.k8s.aws/instance-ebs-bandwidth: "9500"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: c5
      karpenter.k8s.aws/instance-generation: "5"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "98304"
      karpenter.k8s.aws/instance-network-bandwidth: "12000"
      karpenter.k8s.aws/instance-size: 12xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: spot
      karpenter.sh/nodepool: node-spot-usw2c-ltnzv-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9fcf568b-04f5-430e-8f41-03ae7e9e1dd6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T03:01:11Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:01:27Z"
    name: node-spot-usw2c-ltnzv-0-qv8wv
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-spot-usw2c-ltnzv-0
      uid: 9fcf568b-04f5-430e-8f41-03ae7e9e1dd6
    resourceVersion: "3427190935"
    uid: 11cc1a81-3341-482c-89b1-b9b5af11bf13
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-spot-usw2c-ltnzv
    requirements:
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: product
      operator: In
      values:
      - foundation
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.12xlarge
      - c5.9xlarge
      - c6i.12xlarge
      - c6i.16xlarge
      - c6i.8xlarge
      - c7i.12xlarge
      - c7i.16xlarge
      - c7i.8xlarge
      - m5.12xlarge
      - m5.16xlarge
      - m5.8xlarge
      - m5d.12xlarge
      - m5d.16xlarge
      - m5d.8xlarge
      - m6i.12xlarge
      - m6i.16xlarge
      - m6i.8xlarge
      - m6id.12xlarge
      - m6id.16xlarge
      - m6id.8xlarge
      - m7i-flex.12xlarge
      - m7i-flex.16xlarge
      - m7i-flex.8xlarge
      - m7i.12xlarge
      - m7i.16xlarge
      - m7i.8xlarge
      - r5.12xlarge
      - r5.16xlarge
      - r5.8xlarge
      - r6i.12xlarge
      - r6i.16xlarge
      - r6i.8xlarge
      - r7i.12xlarge
      - r7i.16xlarge
      - r7i.8xlarge
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
      - spot
    - key: StackName
      operator: In
      values:
      - k8s-node-spot
    - key: provider
      operator: In
      values:
      - sfn
    - key: node-type
      operator: In
      values:
      - node-spot
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-spot-usw2c-ltnzv-0
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: team
      operator: In
      values:
      - compute
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: service
      operator: In
      values:
      - kubernetes
    - key: Release
      operator: In
      values:
      - v2590
    - key: Environment
      operator: In
      values:
      - staging
    - key: nodegroupDeployment
      operator: In
      values:
      - node-spot-usw2c
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-spot
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-spot-usw2c-ltnzv
    - key: node-role.kubernetes.compute.zende.sk/node-spot
      operator: In
      values:
      - "true"
    - key: ownedby
      operator: In
      values:
      - compute
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-spot-usw2c-ltnzv
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-spot-usw2c-ltnzv
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    resources:
      requests:
        cpu: 657m
        memory: 4486468Ki
        pods: "11"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-spot
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 47560m
      ephemeral-storage: 179Gi
      memory: "89898187490"
      pods: "250"
      vpc.amazonaws.com/pod-eni: "54"
    capacity:
      cpu: "48"
      ephemeral-storage: 200Gi
      memory: 96760640Ki
      pods: "250"
      vpc.amazonaws.com/pod-eni: "54"
    conditions:
    - lastTransitionTime: "2026-01-28T03:00:22Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T03:01:10Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T03:01:27Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T03:01:27Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T03:01:10Z"
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-092f11a650f55f796
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "3672137476963783463"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "12307986074497143793"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T01:14:57Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-spot-usw2c-ltnzv-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
      karpenter.k8s.aws/ec2nodeclass: node-spot-usw2c-ltnzv
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "48"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3600"
      karpenter.k8s.aws/instance-ebs-bandwidth: "9500"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: c5
      karpenter.k8s.aws/instance-generation: "5"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "98304"
      karpenter.k8s.aws/instance-network-bandwidth: "12000"
      karpenter.k8s.aws/instance-size: 12xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: spot
      karpenter.sh/nodepool: node-spot-usw2c-ltnzv-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"9fcf568b-04f5-430e-8f41-03ae7e9e1dd6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:taints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T01:15:59Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:39Z"
    name: node-spot-usw2c-ltnzv-0-r9tc4
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-spot-usw2c-ltnzv-0
      uid: 9fcf568b-04f5-430e-8f41-03ae7e9e1dd6
    resourceVersion: "3427182455"
    uid: f5e639bb-ce18-4996-b5a5-2ad791efa1f1
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-spot-usw2c-ltnzv
    requirements:
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: service
      operator: In
      values:
      - kubernetes
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node-spot
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-spot-usw2c-ltnzv
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
      - spot
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-spot-usw2c-ltnzv-0
    - key: Release
      operator: In
      values:
      - v2590
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: product
      operator: In
      values:
      - foundation
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: provider
      operator: In
      values:
      - sfn
    - key: node-role.kubernetes.compute.zende.sk/node-spot
      operator: In
      values:
      - "true"
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: StackName
      operator: In
      values:
      - k8s-node-spot
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: ownedby
      operator: In
      values:
      - compute
    - key: node-type
      operator: In
      values:
      - node-spot
    - key: nodegroupDeployment
      operator: In
      values:
      - node-spot-usw2c
    - key: Environment
      operator: In
      values:
      - staging
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-spot-usw2c-ltnzv
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.12xlarge
      - c5.9xlarge
      - c6i.12xlarge
      - c6i.16xlarge
      - c6i.8xlarge
      - c7i.12xlarge
      - c7i.16xlarge
      - c7i.8xlarge
      - m5.12xlarge
      - m5.16xlarge
      - m5.8xlarge
      - m5d.12xlarge
      - m5d.16xlarge
      - m5d.8xlarge
      - m6i.12xlarge
      - m6i.16xlarge
      - m6i.8xlarge
      - m6id.12xlarge
      - m6id.16xlarge
      - m6id.8xlarge
      - m7i-flex.12xlarge
      - m7i-flex.16xlarge
      - m7i-flex.8xlarge
      - m7i.12xlarge
      - m7i.16xlarge
      - m7i.8xlarge
      - r5.12xlarge
      - r5.16xlarge
      - r5.8xlarge
      - r6i.12xlarge
      - r6i.16xlarge
      - r6i.8xlarge
      - r7i.12xlarge
      - r7i.16xlarge
      - r7i.8xlarge
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-spot-usw2c-ltnzv
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: team
      operator: In
      values:
      - compute
    resources:
      requests:
        cpu: 706m
        memory: 3769668Ki
        pods: "11"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    taints:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-spot
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 47560m
      ephemeral-storage: 179Gi
      memory: "89898187490"
      pods: "250"
      vpc.amazonaws.com/pod-eni: "54"
    capacity:
      cpu: "48"
      ephemeral-storage: 200Gi
      memory: 96760640Ki
      pods: "250"
      vpc.amazonaws.com/pod-eni: "54"
    conditions:
    - lastTransitionTime: "2026-01-28T01:14:59Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T01:15:58Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T01:16:12Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T01:24:59Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T02:55:39Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-28T01:16:12Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T02:45:39Z"
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-06745aa58d1ddcc0c
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "5868342851000388129"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "11850957934695291872"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-25T14:27:12Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2a-z647p-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-z647p
      karpenter.k8s.aws/ec2nodeclass: node-usw2a-z647p
      karpenter.k8s.aws/instance-capability-flex: "true"
      karpenter.k8s.aws/instance-category: m
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3200"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: m7i-flex
      karpenter.k8s.aws/instance-generation: "7"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "32768"
      karpenter.k8s.aws/instance-network-bandwidth: "1562"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2a-z647p-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"69ad8568-6057-4b0f-8ce7-a13867e4c859"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-25T14:28:00Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:13Z"
    name: node-usw2a-z647p-0-2xkwz
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2a-z647p-0
      uid: 69ad8568-6057-4b0f-8ce7-a13867e4c859
    resourceVersion: "3427189045"
    uid: a276736e-1875-4d98-9329-e8fd6e0c332b
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2a-z647p
    requirements:
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: ownedby
      operator: In
      values:
      - compute
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2a-z647p
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: Release
      operator: In
      values:
      - v2590
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2a-z647p-0
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2a-z647p
    - key: service
      operator: In
      values:
      - kubernetes
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2a
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: node-type
      operator: In
      values:
      - node
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: product
      operator: In
      values:
      - foundation
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: Environment
      operator: In
      values:
      - staging
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: team
      operator: In
      values:
      - compute
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: provider
      operator: In
      values:
      - sfn
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2a-z647p
    resources:
      requests:
        cpu: 3006m
        memory: 14431155Ki
        pods: "17"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "27718849496"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "18"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 32314076Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "18"
    conditions:
    - lastTransitionTime: "2026-01-25T14:27:14Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-25T14:27:59Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-25T14:28:16Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-25T14:37:14Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T03:00:13Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-25T14:28:16Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T03:00:12Z"
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-010877a189d01d91a
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "5868342851000388129"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "11850957934695291872"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T01:20:41Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2a-z647p-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-z647p
      karpenter.k8s.aws/ec2nodeclass: node-usw2a-z647p
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3400"
      karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: c5
      karpenter.k8s.aws/instance-generation: "5"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "2500"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2a-z647p-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"69ad8568-6057-4b0f-8ce7-a13867e4c859"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T01:21:42Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:03:01Z"
    name: node-usw2a-z647p-0-4v2kb
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2a-z647p-0
      uid: 69ad8568-6057-4b0f-8ce7-a13867e4c859
    resourceVersion: "3427193063"
    uid: 6bfaa263-5f90-4620-b484-5b3e15a243c5
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2a-z647p
    requirements:
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2a-z647p
    - key: provider
      operator: In
      values:
      - sfn
    - key: product
      operator: In
      values:
      - foundation
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2a-z647p
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2a-z647p-0
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: Environment
      operator: In
      values:
      - staging
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2a-z647p
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: service
      operator: In
      values:
      - kubernetes
    - key: team
      operator: In
      values:
      - compute
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: Release
      operator: In
      values:
      - v2590
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: node-type
      operator: In
      values:
      - node
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: ownedby
      operator: In
      values:
      - compute
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2a
    resources:
      requests:
        cpu: 816m
        memory: 5107012Ki
        pods: "15"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12011671553"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 15993572Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-28T01:20:44Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T01:21:41Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T01:21:57Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T01:30:44Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T03:03:01Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-28T01:21:57Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T03:03:00Z"
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-0c394e6fb29b43132
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "5868342851000388129"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "11850957934695291872"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-27T21:09:53Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2a-z647p-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-z647p
      karpenter.k8s.aws/ec2nodeclass: node-usw2a-z647p
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3400"
      karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
      karpenter.k8s.aws/instance-family: c5
      karpenter.k8s.aws/instance-generation: "5"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "2500"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2a-z647p-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az1
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"69ad8568-6057-4b0f-8ce7-a13867e4c859"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-27T21:10:45Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:10:10Z"
    name: node-usw2a-z647p-0-r2shf
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2a-z647p-0
      uid: 69ad8568-6057-4b0f-8ce7-a13867e4c859
    resourceVersion: "3427115132"
    uid: ca443723-c79f-4ecf-adb3-93ad2d4e7745
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2a-z647p
    requirements:
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2a-z647p
    - key: node-type
      operator: In
      values:
      - node
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2a-z647p
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: ownedby
      operator: In
      values:
      - compute
    - key: Environment
      operator: In
      values:
      - staging
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: Release
      operator: In
      values:
      - v2590
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: team
      operator: In
      values:
      - compute
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2a
    - key: product
      operator: In
      values:
      - foundation
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2a-z647p-0
    - key: provider
      operator: In
      values:
      - sfn
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2a-z647p
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2a
    - key: service
      operator: In
      values:
      - kubernetes
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2a
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    resources:
      requests:
        cpu: 543m
        memory: 2537796Ki
        pods: "13"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12011671553"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 15993572Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-27T21:09:55Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-27T21:10:44Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-27T21:11:01Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-27T21:19:55Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T02:10:10Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-27T21:11:01Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T02:10:09Z"
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    providerID: aws:///us-west-2a/i-0b5fecdbd22bdd3e3
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "7173101074718267904"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "16124741842375123321"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-23T01:43:24Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2b-n7st7-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
      karpenter.k8s.aws/ec2nodeclass: node-usw2b-n7st7
      karpenter.k8s.aws/instance-capability-flex: "true"
      karpenter.k8s.aws/instance-category: m
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3200"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: m7i-flex
      karpenter.k8s.aws/instance-generation: "7"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "32768"
      karpenter.k8s.aws/instance-network-bandwidth: "1562"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2b-n7st7-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az2
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:44:17Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:04:28Z"
    name: node-usw2b-n7st7-0-9nvqt
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2b-n7st7-0
      uid: b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364
    resourceVersion: "3427195533"
    uid: 381b74bc-069d-46da-82d9-7a524b609c79
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2b-n7st7
    requirements:
    - key: ownedby
      operator: In
      values:
      - compute
    - key: node-type
      operator: In
      values:
      - node
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2b-n7st7-0
    - key: product
      operator: In
      values:
      - foundation
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2b
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2b-n7st7
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: provider
      operator: In
      values:
      - sfn
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2b-n7st7
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: Release
      operator: In
      values:
      - v2590
    - key: Environment
      operator: In
      values:
      - staging
    - key: service
      operator: In
      values:
      - kubernetes
    - key: team
      operator: In
      values:
      - compute
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2b
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2b
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2b-n7st7
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    resources:
      requests:
        cpu: 4997m
        memory: 11994436Ki
        pods: "15"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "27718849496"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "18"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 32314076Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "18"
    conditions:
    - lastTransitionTime: "2026-01-23T01:53:25Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-23T01:44:30Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-23T01:43:25Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-23T01:44:30Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    - lastTransitionTime: "2026-01-23T01:44:15Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T03:04:28Z"
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    providerID: aws:///us-west-2b/i-0af7d5e8a274b2b88
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "7173101074718267904"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "16124741842375123321"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T02:23:17Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2b-n7st7-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
      karpenter.k8s.aws/ec2nodeclass: node-usw2b-n7st7
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2b-n7st7-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az2
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:24:16Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:30Z"
    name: node-usw2b-n7st7-0-f9nlf
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2b-n7st7-0
      uid: b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364
    resourceVersion: "3427189747"
    uid: d1cf4b59-7905-4fad-941b-ac033ec39438
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2b-n7st7
    requirements:
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2b-n7st7
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: ownedby
      operator: In
      values:
      - compute
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2b
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2b-n7st7-0
    - key: node-type
      operator: In
      values:
      - node
    - key: product
      operator: In
      values:
      - foundation
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2b
    - key: provider
      operator: In
      values:
      - sfn
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2b
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: Release
      operator: In
      values:
      - v2590
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: team
      operator: In
      values:
      - compute
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2b-n7st7
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: service
      operator: In
      values:
      - kubernetes
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2b-n7st7
    - key: Environment
      operator: In
      values:
      - staging
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    resources:
      requests:
        cpu: 926m
        memory: 7657796Ki
        pods: "17"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101849089"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081636Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-28T02:23:20Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T02:24:15Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T02:24:32Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T02:33:20Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T02:24:32Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T03:00:30Z"
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    providerID: aws:///us-west-2b/i-00dfd5a02430664e8
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "7173101074718267904"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "16124741842375123321"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T02:32:12Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2b-n7st7-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
      karpenter.k8s.aws/ec2nodeclass: node-usw2b-n7st7
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2b-n7st7-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az2
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:33:08Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:03:00Z"
    name: node-usw2b-n7st7-0-zrvpl
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2b-n7st7-0
      uid: b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364
    resourceVersion: "3427193019"
    uid: c732474c-9cc3-44c4-b954-2a8547ba2f1c
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2b-n7st7
    requirements:
    - key: SfnKubernetes
      operator: In
      values:
      - e94a235199dde75b7f68bb0075a0a7a91367606f
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2b-n7st7
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: team
      operator: In
      values:
      - compute
    - key: node-type
      operator: In
      values:
      - node
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2b
    - key: Release
      operator: In
      values:
      - v2590
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2b
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - b7f462ec
    - key: service
      operator: In
      values:
      - kubernetes
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2b
    - key: ownedby
      operator: In
      values:
      - compute
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2b-n7st7
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2b-n7st7
    - key: provider
      operator: In
      values:
      - sfn
    - key: Environment
      operator: In
      values:
      - staging
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: product
      operator: In
      values:
      - foundation
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2b-n7st7-0
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: StackName
      operator: In
      values:
      - k8s-node
    resources:
      requests:
        cpu: 822m
        memory: "3958706176"
        pods: "14"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101849089"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081636Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-28T02:32:15Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T02:33:07Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T02:33:24Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T02:42:15Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T02:33:24Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-09e9717e54a1307d5
    lastPodEventTime: "2026-01-28T03:03:00Z"
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    providerID: aws:///us-west-2b/i-09517c0dfc55bfd1e
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "2244640841441788935"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "3158429970100788055"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-27T21:26:11Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2c-twg9f-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2c-twg9f-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d1f2a780-9351-4b41-aa62-c3a049e3d0a6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-27T21:27:29Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:07Z"
    name: node-usw2c-twg9f-0-d4mrz
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2c-twg9f-0
      uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
    resourceVersion: "3427188831"
    uid: cd5cb8dd-265a-4348-98bd-7502422da4e4
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2c-twg9f
    requirements:
    - key: team
      operator: In
      values:
      - compute
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2c
    - key: SfnKubernetes
      operator: In
      values:
      - fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2c-twg9f
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: Environment
      operator: In
      values:
      - staging
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2c-twg9f-0
    - key: service
      operator: In
      values:
      - kubernetes
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - "04661458"
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: product
      operator: In
      values:
      - foundation
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: provider
      operator: In
      values:
      - sfn
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: node-type
      operator: In
      values:
      - node
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2c-twg9f
    - key: ownedby
      operator: In
      values:
      - compute
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2c-twg9f
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: Release
      operator: In
      values:
      - v2573
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    resources:
      requests:
        cpu: 766m
        memory: 4058436Ki
        pods: "13"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101857281"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081644Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-27T21:36:13Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-27T21:27:45Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-27T21:26:13Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-27T21:27:45Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    - lastTransitionTime: "2026-01-27T21:27:28Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    imageID: ami-0eb0affd0e3dd8cfb
    lastPodEventTime: "2026-01-28T03:00:07Z"
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-00d9ece9adca2e6a7
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "2244640841441788935"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "3158429970100788055"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T01:56:59Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2c-twg9f-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2c-twg9f-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d1f2a780-9351-4b41-aa62-c3a049e3d0a6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T01:58:24Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:10Z"
    name: node-usw2c-twg9f-0-fxv5j
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2c-twg9f-0
      uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
    resourceVersion: "3427188973"
    uid: 6cafc35e-9c9f-4a0b-b544-4c82187cff5e
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2c-twg9f
    requirements:
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: Environment
      operator: In
      values:
      - staging
    - key: Release
      operator: In
      values:
      - v2573
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2c-twg9f-0
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: team
      operator: In
      values:
      - compute
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: node-type
      operator: In
      values:
      - node
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - "04661458"
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2c-twg9f
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: service
      operator: In
      values:
      - kubernetes
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2c-twg9f
    - key: provider
      operator: In
      values:
      - sfn
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2c
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: ownedby
      operator: In
      values:
      - compute
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2c-twg9f
    - key: product
      operator: In
      values:
      - foundation
    - key: SfnKubernetes
      operator: In
      values:
      - fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    resources:
      requests:
        cpu: 791m
        memory: 4582724Ki
        pods: "14"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101857281"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081644Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-28T01:57:01Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T01:58:23Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T01:58:40Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T02:07:01Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T01:58:40Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-0eb0affd0e3dd8cfb
    lastPodEventTime: "2026-01-28T03:00:10Z"
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-06bfb6b4b3967a8e2
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "2244640841441788935"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "3158429970100788055"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T02:34:39Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2c-twg9f-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2c-twg9f-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d1f2a780-9351-4b41-aa62-c3a049e3d0a6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:36:00Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:00Z"
    name: node-usw2c-twg9f-0-qdz42
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2c-twg9f-0
      uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
    resourceVersion: "3427188598"
    uid: f95b75f0-8e6a-4b31-9744-d1cf30da0a81
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2c-twg9f
    requirements:
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: ownedby
      operator: In
      values:
      - compute
    - key: provider
      operator: In
      values:
      - sfn
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2c-twg9f
    - key: Release
      operator: In
      values:
      - v2573
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: node-type
      operator: In
      values:
      - node
    - key: Environment
      operator: In
      values:
      - staging
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2c
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2c-twg9f-0
    - key: service
      operator: In
      values:
      - kubernetes
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: product
      operator: In
      values:
      - foundation
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - "04661458"
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: team
      operator: In
      values:
      - compute
    - key: SfnKubernetes
      operator: In
      values:
      - fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2c-twg9f
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2c-twg9f
    - key: managed-by
      operator: In
      values:
      - karpenter
    resources:
      requests:
        cpu: 941m
        memory: 4534148Ki
        pods: "13"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101857281"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081644Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-28T02:44:41Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T02:36:16Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T02:34:41Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T02:36:16Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    - lastTransitionTime: "2026-01-28T02:35:59Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    imageID: ami-0eb0affd0e3dd8cfb
    lastPodEventTime: "2026-01-28T03:00:00Z"
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-04e3afe8284718809
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "2244640841441788935"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "3158429970100788055"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-28T02:30:11Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2c-twg9f-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2c-twg9f-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d1f2a780-9351-4b41-aa62-c3a049e3d0a6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:31:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:04:10Z"
    name: node-usw2c-twg9f-0-sxq9x
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2c-twg9f-0
      uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
    resourceVersion: "3427195039"
    uid: 5783e4b5-9602-4c1e-a3f4-18cac8f7a4e9
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2c-twg9f
    requirements:
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2c
    - key: SfnKubernetes
      operator: In
      values:
      - fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: Environment
      operator: In
      values:
      - staging
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2c-twg9f-0
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - "04661458"
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: ownedby
      operator: In
      values:
      - compute
    - key: product
      operator: In
      values:
      - foundation
    - key: provider
      operator: In
      values:
      - sfn
    - key: team
      operator: In
      values:
      - compute
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: service
      operator: In
      values:
      - kubernetes
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2c-twg9f
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: Release
      operator: In
      values:
      - v2573
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2c-twg9f
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2c-twg9f
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: node-type
      operator: In
      values:
      - node
    resources:
      requests:
        cpu: 1741m
        memory: 4058436Ki
        pods: "13"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101857281"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081644Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-28T02:30:13Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-28T02:31:25Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    - lastTransitionTime: "2026-01-28T02:31:46Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-28T02:40:13Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-28T03:04:10Z"
      message: ""
      observedGeneration: 1
      reason: Consolidatable
      status: "True"
      type: Consolidatable
    - lastTransitionTime: "2026-01-28T02:31:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    imageID: ami-0eb0affd0e3dd8cfb
    lastPodEventTime: "2026-01-28T02:54:10Z"
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-0f8e0c5e9e6e06171
- apiVersion: karpenter.sh/v1
  kind: NodeClaim
  metadata:
    annotations:
      compatibility.karpenter.k8s.aws/cluster-name-tagged: "true"
      k8s.io/cluster-autoscaler-enabled: "false"
      karpenter.k8s.aws/ec2nodeclass-hash: "2244640841441788935"
      karpenter.k8s.aws/ec2nodeclass-hash-version: v4
      karpenter.k8s.aws/instance-profile-name: kubernetes-sandbox-cluster-node-kubernetes-sandbox-c009eafgd7q90cmd
      karpenter.k8s.aws/tagged: "true"
      karpenter.sh/nodeclaim-min-values-relaxed: "false"
      karpenter.sh/nodepool-hash: "3158429970100788055"
      karpenter.sh/nodepool-hash-version: v3
      karpenter.sh/price-overlay-applied: "true"
    creationTimestamp: "2026-01-26T19:30:11Z"
    finalizers:
    - karpenter.sh/termination
    generateName: node-usw2c-twg9f-0-
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
      karpenter.k8s.aws/instance-capability-flex: "false"
      karpenter.k8s.aws/instance-category: c
      karpenter.k8s.aws/instance-cpu: "8"
      karpenter.k8s.aws/instance-cpu-manufacturer: intel
      karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
      karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
      karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
      karpenter.k8s.aws/instance-family: c6i
      karpenter.k8s.aws/instance-generation: "6"
      karpenter.k8s.aws/instance-hypervisor: nitro
      karpenter.k8s.aws/instance-memory: "16384"
      karpenter.k8s.aws/instance-network-bandwidth: "3125"
      karpenter.k8s.aws/instance-size: 2xlarge
      karpenter.k8s.aws/instance-tenancy: default
      karpenter.sh/capacity-type: on-demand
      karpenter.sh/nodepool: node-usw2c-twg9f-0
      kubernetes.io/arch: amd64
      kubernetes.io/os: linux
      managed-by: karpenter
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.k8s.aws/zone-id: usw2-az3
      topology.kubernetes.io/region: us-west-2
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:compatibility.karpenter.k8s.aws/cluster-name-tagged: {}
            f:k8s.io/cluster-autoscaler-enabled: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash: {}
            f:karpenter.k8s.aws/ec2nodeclass-hash-version: {}
            f:karpenter.k8s.aws/instance-profile-name: {}
            f:karpenter.k8s.aws/tagged: {}
            f:karpenter.sh/nodeclaim-min-values-relaxed: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
            f:karpenter.sh/price-overlay-applied: {}
          f:finalizers:
            .: {}
            v:"karpenter.sh/termination": {}
          f:generateName: {}
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.k8s.aws/ec2nodeclass: {}
            f:karpenter.k8s.aws/instance-capability-flex: {}
            f:karpenter.k8s.aws/instance-category: {}
            f:karpenter.k8s.aws/instance-cpu: {}
            f:karpenter.k8s.aws/instance-cpu-manufacturer: {}
            f:karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: {}
            f:karpenter.k8s.aws/instance-ebs-bandwidth: {}
            f:karpenter.k8s.aws/instance-encryption-in-transit-supported: {}
            f:karpenter.k8s.aws/instance-family: {}
            f:karpenter.k8s.aws/instance-generation: {}
            f:karpenter.k8s.aws/instance-hypervisor: {}
            f:karpenter.k8s.aws/instance-memory: {}
            f:karpenter.k8s.aws/instance-network-bandwidth: {}
            f:karpenter.k8s.aws/instance-size: {}
            f:karpenter.k8s.aws/instance-tenancy: {}
            f:karpenter.sh/capacity-type: {}
            f:karpenter.sh/nodepool: {}
            f:kubernetes.io/arch: {}
            f:kubernetes.io/os: {}
            f:managed-by: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/instance-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.k8s.aws/zone-id: {}
            f:topology.kubernetes.io/region: {}
            f:topology.kubernetes.io/zone: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"d1f2a780-9351-4b41-aa62-c3a049e3d0a6"}: {}
        f:spec:
          .: {}
          f:expireAfter: {}
          f:nodeClassRef:
            .: {}
            f:group: {}
            f:kind: {}
            f:name: {}
          f:requirements: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
              f:pods: {}
          f:startupTaints: {}
          f:terminationGracePeriod: {}
      manager: karpenter
      operation: Update
      time: "2026-01-26T19:31:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:allocatable:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:capacity:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
          f:conditions: {}
          f:imageID: {}
          f:lastPodEventTime: {}
          f:nodeName: {}
          f:providerID: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:00:04Z"
    name: node-usw2c-twg9f-0-wx5xb
    ownerReferences:
    - apiVersion: karpenter.sh/v1
      blockOwnerDeletion: true
      kind: NodePool
      name: node-usw2c-twg9f-0
      uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
    resourceVersion: "3427188745"
    uid: 38994374-22de-41ae-949c-356b3b4bfb93
  spec:
    expireAfter: Never
    nodeClassRef:
      group: karpenter.k8s.aws
      kind: EC2NodeClass
      name: node-usw2c-twg9f
    requirements:
    - key: k8s.amazonaws.com/eniConfig
      operator: In
      values:
      - node-usw2c-twg9f
    - key: service
      operator: In
      values:
      - kubernetes
    - key: node-type
      operator: In
      values:
      - node
    - key: team
      operator: In
      values:
      - compute
    - key: managed-by
      operator: In
      values:
      - karpenter
    - key: PoddedCluster
      operator: In
      values:
      - "false"
    - key: karpenter.sh/nodepool
      operator: In
      values:
      - node-usw2c-twg9f-0
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
      - c5.2xlarge
      - c6i.2xlarge
      - c7i.2xlarge
      - m5.2xlarge
      - m5d.2xlarge
      - m6i.2xlarge
      - m6id.2xlarge
      - m7i-flex.2xlarge
      - m7i.2xlarge
      - r5.2xlarge
      - r6i.2xlarge
      - r7i.2xlarge
    - key: Release
      operator: In
      values:
      - v2573
    - key: compute.zende.sk/kube-proxy-mode
      operator: In
      values:
      - daemonset
    - key: node.kubernetes.io/role
      operator: In
      values:
      - node
    - key: provider
      operator: In
      values:
      - sfn
    - key: node-role.kubernetes.compute.zende.sk/node
      operator: In
      values:
      - "true"
    - key: SfnKubernetes
      operator: In
      values:
      - fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    - key: ownedby
      operator: In
      values:
      - compute
    - key: compute.zende.sk/ebs-initial-burst
      operator: In
      values:
      - enabled
    - key: karpenter.k8s.aws/ec2nodeclass
      operator: In
      values:
      - node-usw2c-twg9f
    - key: nodegroup-operator.compute.zende.sk/cluster-name
      operator: In
      values:
      - sandbox
    - key: project
      operator: In
      values:
      - sfn_kubernetes
    - key: Environment
      operator: In
      values:
      - staging
    - key: topology.kubernetes.io/zone
      operator: In
      values:
      - us-west-2c
    - key: nodegroup-operator.compute.zende.sk/nodegroup-name
      operator: In
      values:
      - node-usw2c-twg9f
    - key: nodegroup-provider
      operator: In
      values:
      - karpenter
    - key: karpenter.sh/capacity-type
      operator: In
      values:
      - on-demand
    - key: compute.zendesk.com/ec2_image_sha
      operator: In
      values:
      - "04661458"
    - key: AvailabilityZone
      operator: In
      values:
      - us-west-2c
    - key: nodegroupDeployment
      operator: In
      values:
      - node-usw2c
    - key: StackName
      operator: In
      values:
      - k8s-node
    - key: product
      operator: In
      values:
      - foundation
    resources:
      requests:
        cpu: 822m
        memory: 3214660Ki
        pods: "14"
    startupTaints:
    - effect: NoSchedule
      key: kube-node-monitor
      value: notready
    terminationGracePeriod: 1h0m0s
  status:
    allocatable:
      cpu: 7660m
      ephemeral-storage: 179Gi
      memory: "12101853185"
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    capacity:
      cpu: "8"
      ephemeral-storage: 200Gi
      memory: 16081640Ki
      pods: "110"
      vpc.amazonaws.com/pod-eni: "38"
    conditions:
    - lastTransitionTime: "2026-01-26T19:40:14Z"
      message: ""
      observedGeneration: 1
      reason: ConsistentStateFound
      status: "True"
      type: ConsistentStateFound
    - lastTransitionTime: "2026-01-26T19:31:28Z"
      message: ""
      observedGeneration: 1
      reason: Initialized
      status: "True"
      type: Initialized
    - lastTransitionTime: "2026-01-26T19:30:14Z"
      message: ""
      observedGeneration: 1
      reason: Launched
      status: "True"
      type: Launched
    - lastTransitionTime: "2026-01-26T19:31:28Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    - lastTransitionTime: "2026-01-26T19:31:13Z"
      message: ""
      observedGeneration: 1
      reason: Registered
      status: "True"
      type: Registered
    imageID: ami-0eb0affd0e3dd8cfb
    lastPodEventTime: "2026-01-28T03:00:04Z"
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    providerID: aws:///us-west-2c/i-00c5b7259f4923f4f
nodePools:
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "972488033683353484"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T00:31:09Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2588
      SfnKubernetes: d13fffcacba5c8becb23e22beedf1dc2cc30c7ef
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: karpenter-integration-test-mgrosser-consolidation-up
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      nodegroup-provider: karpenter
      nodegroupDeployment: karpenter-integration-test-mgrosser-consolidation-up
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T00:31:09Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T00:31:09Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T21:50:10Z"
    name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5-0
    resourceVersion: "3416810275"
    uid: b7ed1d38-3a1e-4c55-9e17-7e364f6dbcf4
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
        - Underutilized
      consolidateAfter: 10s
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2588
          SfnKubernetes: d13fffcacba5c8becb23e22beedf1dc2cc30c7ef
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: karpenter-integration-test-mgrosser-consolidation-up
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
          nodegroup-provider: karpenter
          nodegroupDeployment: karpenter-integration-test-mgrosser-consolidation-up
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6i.4xlarge
          - m7i.4xlarge
          - m5.4xlarge
          - m6i.8xlarge
          - m7i.8xlarge
          - m5.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: karpenter-integration-test-mgrosser-consolidation-up
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T00:31:09Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T00:31:11Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-22T00:34:33Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T00:31:11Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "972488033683353484"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T00:31:09Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2588
      SfnKubernetes: d13fffcacba5c8becb23e22beedf1dc2cc30c7ef
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: karpenter-integration-test-mgrosser-consolidation-up
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
      nodegroup-provider: karpenter
      nodegroupDeployment: karpenter-integration-test-mgrosser-consolidation-up
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T00:31:09Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-22T00:31:09Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-22T00:31:11Z"
    name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5-1
    resourceVersion: "3415046012"
    uid: 61f7ab93-a709-4fa1-9222-1e73481a4348
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
        - Underutilized
      consolidateAfter: 10s
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2588
          SfnKubernetes: d13fffcacba5c8becb23e22beedf1dc2cc30c7ef
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: karpenter-integration-test-mgrosser-consolidation-up
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
          nodegroup-provider: karpenter
          nodegroupDeployment: karpenter-integration-test-mgrosser-consolidation-up
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: karpenter-integration-test-mgrosser-consolidation-up-sdbl5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6i.4xlarge
          - m7i.4xlarge
          - m5.4xlarge
          - m6i.8xlarge
          - m7i.8xlarge
          - m5.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: karpenter-integration-test-mgrosser-consolidation-up
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T00:31:09Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T00:31:09Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T00:31:11Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-22T00:31:11Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13029045033658580903"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:06Z"
    generation: 702
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-nqltb
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-nqltb
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:06Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-arm-spot-usw2a-nqltb-0
    resourceVersion: "3427338618"
    uid: 2d4ae4be-9570-4427-a8cd-2f79ccf81a64
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-nqltb
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-nqltb
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2a-nqltb
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.12xlarge
          - c6g.16xlarge
          - c6g.8xlarge
          - c6gd.12xlarge
          - c6gd.16xlarge
          - c6gd.8xlarge
          - c6gn.12xlarge
          - c6gn.16xlarge
          - c6gn.8xlarge
          - c7g.12xlarge
          - c7g.16xlarge
          - c7g.8xlarge
          - c7gd.12xlarge
          - c7gd.16xlarge
          - c7gd.8xlarge
          - c7gn.12xlarge
          - c7gn.16xlarge
          - c7gn.8xlarge
          - c8g.12xlarge
          - c8g.16xlarge
          - c8g.8xlarge
          - m6g.12xlarge
          - m6g.16xlarge
          - m6g.8xlarge
          - m6gd.12xlarge
          - m6gd.16xlarge
          - m6gd.8xlarge
          - m7g.12xlarge
          - m7g.16xlarge
          - m7g.8xlarge
          - m7gd.12xlarge
          - m7gd.16xlarge
          - m7gd.8xlarge
          - m8g.12xlarge
          - m8g.16xlarge
          - m8g.8xlarge
          - r6g.12xlarge
          - r6g.16xlarge
          - r6g.8xlarge
          - r6gd.12xlarge
          - r6gd.16xlarge
          - r6gd.8xlarge
          - r7g.12xlarge
          - r7g.16xlarge
          - r7g.8xlarge
          - r7gd.12xlarge
          - r7gd.16xlarge
          - r7gd.8xlarge
          - r8g.12xlarge
          - r8g.16xlarge
          - r8g.8xlarge
          - r8gd.12xlarge
          - r8gd.16xlarge
          - r8gd.8xlarge
          - x2gd.12xlarge
          - x2gd.16xlarge
          - x2gd.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 693
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 693
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:49Z"
      message: object is awaiting reconciliation
      observedGeneration: 693
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 693
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13029045033658580903"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:06Z"
    generation: 556
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-nqltb
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-nqltb
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:06Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:01Z"
    name: node-arm-spot-usw2a-nqltb-1
    resourceVersion: "3427338619"
    uid: 7cd7441a-5ad5-4052-9303-3952a5014bd5
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-nqltb
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-nqltb
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2a-nqltb
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6g.4xlarge
          - m6gd.4xlarge
          - m7g.4xlarge
          - m7gd.4xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
          - x2gd.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 548
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: object is awaiting reconciliation
      observedGeneration: 548
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:08Z"
      message: ""
      observedGeneration: 548
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 548
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "10963855257783007416"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:59Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-rvgr8
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-rvgr8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:59Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:59Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:28Z"
    name: node-arm-spot-usw2a-rvgr8-0
    resourceVersion: "3417201286"
    uid: c9675973-1015-43af-be78-c9d5d0439a7d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-rvgr8
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-rvgr8
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2a-rvgr8
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.12xlarge
          - c6g.16xlarge
          - c6g.8xlarge
          - c6gd.12xlarge
          - c6gd.16xlarge
          - c6gd.8xlarge
          - c6gn.12xlarge
          - c6gn.16xlarge
          - c6gn.8xlarge
          - c7g.12xlarge
          - c7g.16xlarge
          - c7g.8xlarge
          - c7gd.12xlarge
          - c7gd.16xlarge
          - c7gd.8xlarge
          - c7gn.12xlarge
          - c7gn.16xlarge
          - c7gn.8xlarge
          - c8g.12xlarge
          - c8g.16xlarge
          - c8g.8xlarge
          - m6g.12xlarge
          - m6g.16xlarge
          - m6g.8xlarge
          - m6gd.12xlarge
          - m6gd.16xlarge
          - m6gd.8xlarge
          - m7g.12xlarge
          - m7g.16xlarge
          - m7g.8xlarge
          - m7gd.12xlarge
          - m7gd.16xlarge
          - m7gd.8xlarge
          - m8g.12xlarge
          - m8g.16xlarge
          - m8g.8xlarge
          - r6g.12xlarge
          - r6g.16xlarge
          - r6g.8xlarge
          - r6gd.12xlarge
          - r6gd.16xlarge
          - r6gd.8xlarge
          - r7g.12xlarge
          - r7g.16xlarge
          - r7g.8xlarge
          - r7gd.12xlarge
          - r7gd.16xlarge
          - r7gd.8xlarge
          - r8g.12xlarge
          - r8g.16xlarge
          - r8g.8xlarge
          - r8gd.12xlarge
          - r8gd.16xlarge
          - r8gd.8xlarge
          - x2gd.12xlarge
          - x2gd.16xlarge
          - x2gd.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "10963855257783007416"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:59Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-rvgr8
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-rvgr8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:59Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:59Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:39:00Z"
    name: node-arm-spot-usw2a-rvgr8-1
    resourceVersion: "3417161032"
    uid: 363c2b38-228c-4711-9be6-4f2e83bb7ae6
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2a-rvgr8
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2a-rvgr8
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2a-rvgr8
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6g.4xlarge
          - m6gd.4xlarge
          - m7g.4xlarge
          - m7gd.4xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
          - x2gd.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:00Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "7596931623569149499"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:25Z"
    generation: 678
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-4v8s5
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-4v8s5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:01Z"
    name: node-arm-spot-usw2b-4v8s5-0
    resourceVersion: "3427338621"
    uid: ca3d0776-14fa-4c98-b3e2-9b5f1c163e63
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-4v8s5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-4v8s5
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2b-4v8s5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.12xlarge
          - c6g.16xlarge
          - c6g.8xlarge
          - c6gd.12xlarge
          - c6gd.16xlarge
          - c6gd.8xlarge
          - c6gn.12xlarge
          - c6gn.16xlarge
          - c6gn.8xlarge
          - c7g.12xlarge
          - c7g.16xlarge
          - c7g.8xlarge
          - c7gd.12xlarge
          - c7gd.16xlarge
          - c7gd.8xlarge
          - c7gn.12xlarge
          - c7gn.16xlarge
          - c7gn.8xlarge
          - c8g.12xlarge
          - c8g.16xlarge
          - c8g.8xlarge
          - m6g.12xlarge
          - m6g.16xlarge
          - m6g.8xlarge
          - m6gd.12xlarge
          - m6gd.16xlarge
          - m6gd.8xlarge
          - m7g.12xlarge
          - m7g.16xlarge
          - m7g.8xlarge
          - m7gd.12xlarge
          - m7gd.16xlarge
          - m7gd.8xlarge
          - m8g.12xlarge
          - m8g.16xlarge
          - m8g.8xlarge
          - r6g.12xlarge
          - r6g.16xlarge
          - r6g.8xlarge
          - r6gd.12xlarge
          - r6gd.16xlarge
          - r6gd.8xlarge
          - r7g.12xlarge
          - r7g.16xlarge
          - r7g.8xlarge
          - r7gd.12xlarge
          - r7gd.16xlarge
          - r7gd.8xlarge
          - r8g.12xlarge
          - r8g.16xlarge
          - r8g.8xlarge
          - r8gd.12xlarge
          - r8gd.16xlarge
          - r8gd.8xlarge
          - x2gd.12xlarge
          - x2gd.16xlarge
          - x2gd.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 670
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 670
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:08Z"
      message: object is awaiting reconciliation
      observedGeneration: 670
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 670
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "7596931623569149499"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:25Z"
    generation: 559
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-4v8s5
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-4v8s5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:01Z"
    name: node-arm-spot-usw2b-4v8s5-1
    resourceVersion: "3427338620"
    uid: c3197ad9-8a9b-4855-a518-76f13f30b7bf
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-4v8s5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-4v8s5
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2b-4v8s5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6g.4xlarge
          - m6gd.4xlarge
          - m7g.4xlarge
          - m7gd.4xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
          - x2gd.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: object is awaiting reconciliation
      observedGeneration: 552
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 552
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 552
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 552
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14126352269707455300"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:50Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-dm4xh
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-dm4xh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:57Z"
    name: node-arm-spot-usw2b-dm4xh-0
    resourceVersion: "3417202217"
    uid: 7ecb2e43-6315-4993-b5db-5d53a927848a
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-dm4xh
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-dm4xh
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2b-dm4xh
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.12xlarge
          - c6g.16xlarge
          - c6g.8xlarge
          - c6gd.12xlarge
          - c6gd.16xlarge
          - c6gd.8xlarge
          - c6gn.12xlarge
          - c6gn.16xlarge
          - c6gn.8xlarge
          - c7g.12xlarge
          - c7g.16xlarge
          - c7g.8xlarge
          - c7gd.12xlarge
          - c7gd.16xlarge
          - c7gd.8xlarge
          - c7gn.12xlarge
          - c7gn.16xlarge
          - c7gn.8xlarge
          - c8g.12xlarge
          - c8g.16xlarge
          - c8g.8xlarge
          - m6g.12xlarge
          - m6g.16xlarge
          - m6g.8xlarge
          - m6gd.12xlarge
          - m6gd.16xlarge
          - m6gd.8xlarge
          - m7g.12xlarge
          - m7g.16xlarge
          - m7g.8xlarge
          - m7gd.12xlarge
          - m7gd.16xlarge
          - m7gd.8xlarge
          - m8g.12xlarge
          - m8g.16xlarge
          - m8g.8xlarge
          - r6g.12xlarge
          - r6g.16xlarge
          - r6g.8xlarge
          - r6gd.12xlarge
          - r6gd.16xlarge
          - r6gd.8xlarge
          - r7g.12xlarge
          - r7g.16xlarge
          - r7g.8xlarge
          - r7gd.12xlarge
          - r7gd.16xlarge
          - r7gd.8xlarge
          - r8g.12xlarge
          - r8g.16xlarge
          - r8g.8xlarge
          - r8gd.12xlarge
          - r8gd.16xlarge
          - r8gd.8xlarge
          - x2gd.12xlarge
          - x2gd.16xlarge
          - x2gd.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:46Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14126352269707455300"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:50Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-dm4xh
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-dm4xh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:52Z"
    name: node-arm-spot-usw2b-dm4xh-1
    resourceVersion: "3417160656"
    uid: 68333296-016e-4a8b-8381-81cc59c60a06
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2b-dm4xh
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2b-dm4xh
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2b-dm4xh
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6g.4xlarge
          - m6gd.4xlarge
          - m7g.4xlarge
          - m7gd.4xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
          - x2gd.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9270956144801114357"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:48Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-87sn8
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-87sn8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:37Z"
    name: node-arm-spot-usw2c-87sn8-0
    resourceVersion: "3417201568"
    uid: fcb471ea-8da8-40df-8bd7-c878d860a0ff
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-87sn8
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-87sn8
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2c-87sn8
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.12xlarge
          - c6g.16xlarge
          - c6g.8xlarge
          - c6gd.12xlarge
          - c6gd.16xlarge
          - c6gd.8xlarge
          - c6gn.12xlarge
          - c6gn.16xlarge
          - c6gn.8xlarge
          - c7g.12xlarge
          - c7g.16xlarge
          - c7g.8xlarge
          - c7gd.12xlarge
          - c7gd.16xlarge
          - c7gd.8xlarge
          - c7gn.12xlarge
          - c7gn.16xlarge
          - c7gn.8xlarge
          - c8g.12xlarge
          - c8g.16xlarge
          - c8g.8xlarge
          - m6g.12xlarge
          - m6g.16xlarge
          - m6g.8xlarge
          - m6gd.12xlarge
          - m6gd.16xlarge
          - m6gd.8xlarge
          - m7g.12xlarge
          - m7g.16xlarge
          - m7g.8xlarge
          - m7gd.12xlarge
          - m7gd.16xlarge
          - m7gd.8xlarge
          - m8g.12xlarge
          - m8g.16xlarge
          - m8g.8xlarge
          - r6g.12xlarge
          - r6g.16xlarge
          - r6g.8xlarge
          - r6gd.12xlarge
          - r6gd.16xlarge
          - r6gd.8xlarge
          - r7g.12xlarge
          - r7g.16xlarge
          - r7g.8xlarge
          - r7gd.12xlarge
          - r7gd.16xlarge
          - r7gd.8xlarge
          - r8g.12xlarge
          - r8g.16xlarge
          - r8g.8xlarge
          - r8gd.12xlarge
          - r8gd.16xlarge
          - r8gd.8xlarge
          - x2gd.12xlarge
          - x2gd.16xlarge
          - x2gd.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:27Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9270956144801114357"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:48Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-87sn8
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-87sn8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:50Z"
    name: node-arm-spot-usw2c-87sn8-1
    resourceVersion: "3417160555"
    uid: 13d1e41a-2413-4630-843e-c11f0145098a
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-87sn8
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-87sn8
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2c-87sn8
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6g.4xlarge
          - m6gd.4xlarge
          - m7g.4xlarge
          - m7gd.4xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
          - x2gd.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:50Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5998082754696825662"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:15Z"
    generation: 660
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-mc28v
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-mc28v
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:02Z"
    name: node-arm-spot-usw2c-mc28v-0
    resourceVersion: "3427338653"
    uid: 248c0683-b790-41c5-bd95-01be2fd47a27
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-mc28v
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-mc28v
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2c-mc28v
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.12xlarge
          - c6g.16xlarge
          - c6g.8xlarge
          - c6gd.12xlarge
          - c6gd.16xlarge
          - c6gd.8xlarge
          - c6gn.12xlarge
          - c6gn.16xlarge
          - c6gn.8xlarge
          - c7g.12xlarge
          - c7g.16xlarge
          - c7g.8xlarge
          - c7gd.12xlarge
          - c7gd.16xlarge
          - c7gd.8xlarge
          - c7gn.12xlarge
          - c7gn.16xlarge
          - c7gn.8xlarge
          - c8g.12xlarge
          - c8g.16xlarge
          - c8g.8xlarge
          - m6g.12xlarge
          - m6g.16xlarge
          - m6g.8xlarge
          - m6gd.12xlarge
          - m6gd.16xlarge
          - m6gd.8xlarge
          - m7g.12xlarge
          - m7g.16xlarge
          - m7g.8xlarge
          - m7gd.12xlarge
          - m7gd.16xlarge
          - m7gd.8xlarge
          - m8g.12xlarge
          - m8g.16xlarge
          - m8g.8xlarge
          - r6g.12xlarge
          - r6g.16xlarge
          - r6g.8xlarge
          - r6gd.12xlarge
          - r6gd.16xlarge
          - r6gd.8xlarge
          - r7g.12xlarge
          - r7g.16xlarge
          - r7g.8xlarge
          - r7gd.12xlarge
          - r7gd.16xlarge
          - r7gd.8xlarge
          - r8g.12xlarge
          - r8g.16xlarge
          - r8g.8xlarge
          - r8gd.12xlarge
          - r8gd.16xlarge
          - r8gd.8xlarge
          - x2gd.12xlarge
          - x2gd.16xlarge
          - x2gd.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:15Z"
      message: ""
      observedGeneration: 650
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 650
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:06Z"
      message: object is awaiting reconciliation
      observedGeneration: 650
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:50Z"
      message: ""
      observedGeneration: 650
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5998082754696825662"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:15Z"
    generation: 560
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-mc28v
      node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
      node-type: node-arm-spot
      node.kubernetes.io/role: node-arm-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-mc28v
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:50Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:02Z"
    name: node-arm-spot-usw2c-mc28v-1
    resourceVersion: "3427338655"
    uid: e623c744-7fb2-4cfa-8984-5aa8dbd7b887
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-spot-usw2c-mc28v
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm-spot: "true"
          node-type: node-arm-spot
          node.kubernetes.io/role: node-arm-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-spot-usw2c-mc28v
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-spot-usw2c-mc28v
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m6g.4xlarge
          - m6gd.4xlarge
          - m7g.4xlarge
          - m7gd.4xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
          - x2gd.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:15Z"
      message: ""
      observedGeneration: 553
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: object is awaiting reconciliation
      observedGeneration: 553
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:39Z"
      message: ""
      observedGeneration: 553
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:50Z"
      message: ""
      observedGeneration: 553
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14997190130688923436"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:01Z"
    generation: 942
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2a-d42lg
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-d42lg
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:01Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:55Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:07Z"
    name: node-arm-usw2a-d42lg-0
    resourceVersion: "3427338857"
    uid: 1ebbab61-f562-4b18-8b47-e4e3678c3bb5
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-usw2a-d42lg
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm: "true"
          node-type: node-arm
          node.kubernetes.io/role: node-arm
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-d42lg
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-usw2a-d42lg
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.4xlarge
          - c6gd.4xlarge
          - c6gn.4xlarge
          - c7g.4xlarge
          - c7gd.4xlarge
          - c7gn.4xlarge
          - c8g.4xlarge
          - m6g.2xlarge
          - m6g.4xlarge
          - m6gd.2xlarge
          - m6gd.4xlarge
          - m7g.2xlarge
          - m7g.4xlarge
          - m7gd.2xlarge
          - m7gd.4xlarge
          - m8g.2xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:01Z"
      message: ""
      observedGeneration: 928
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 928
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:11Z"
      message: object is awaiting reconciliation
      observedGeneration: 928
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:55Z"
      message: ""
      observedGeneration: 928
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "8573347850522616499"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:54Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2a-thmxj
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-thmxj
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:54Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:54Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:hugepages-32Mi: {}
            f:hugepages-64Ki: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T02:14:03Z"
    name: node-arm-usw2a-thmxj-0
    resourceVersion: "3417237636"
    uid: a8d43df6-1126-4442-b581-958c01814bdf
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "48"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-usw2a-thmxj
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm: "true"
          node-type: node-arm
          node.kubernetes.io/role: node-arm
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-thmxj
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-usw2a-thmxj
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.4xlarge
          - c6gd.4xlarge
          - c6gn.4xlarge
          - c7g.4xlarge
          - c7gd.4xlarge
          - c7gn.4xlarge
          - c8g.4xlarge
          - m6g.2xlarge
          - m6g.4xlarge
          - m6gd.2xlarge
          - m6gd.4xlarge
          - m7g.2xlarge
          - m7g.4xlarge
          - m7gd.2xlarge
          - m7gd.4xlarge
          - m8g.2xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:55Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:36Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:58Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 1
    resources:
      cpu: "8"
      ephemeral-storage: 203034800Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      hugepages-32Mi: "0"
      hugepages-64Ki: "0"
      memory: 32120156Ki
      nodes: "1"
      pods: "160"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12003469939111056955"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:45Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2b-xhg9c
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xhg9c
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:45Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:45Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:hugepages-32Mi: {}
            f:hugepages-64Ki: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T02:14:02Z"
    name: node-arm-usw2b-xhg9c-0
    resourceVersion: "3417237624"
    uid: c2a5d1ef-aaf1-45d9-95bb-fdb78eea4277
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "48"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-usw2b-xhg9c
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm: "true"
          node-type: node-arm
          node.kubernetes.io/role: node-arm
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xhg9c
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-usw2b-xhg9c
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.4xlarge
          - c6gd.4xlarge
          - c6gn.4xlarge
          - c7g.4xlarge
          - c7gd.4xlarge
          - c7gn.4xlarge
          - c8g.4xlarge
          - m6g.2xlarge
          - m6g.4xlarge
          - m6gd.2xlarge
          - m6gd.4xlarge
          - m7g.2xlarge
          - m7g.4xlarge
          - m7gd.2xlarge
          - m7gd.4xlarge
          - m8g.2xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:45Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:36Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:48Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 1
    resources:
      cpu: "8"
      ephemeral-storage: 203034800Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      hugepages-32Mi: "0"
      hugepages-64Ki: "0"
      memory: 32120164Ki
      nodes: "1"
      pods: "160"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17279533277780304179"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:09Z"
    generation: 944
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2b-xjlb5
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xjlb5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:09Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-arm-usw2b-xjlb5-0
    resourceVersion: "3427338788"
    uid: 16ab0fc7-213d-4957-8894-993fa4eb423d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-usw2b-xjlb5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm: "true"
          node-type: node-arm
          node.kubernetes.io/role: node-arm
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xjlb5
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-usw2b-xjlb5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.4xlarge
          - c6gd.4xlarge
          - c6gn.4xlarge
          - c7g.4xlarge
          - c7gd.4xlarge
          - c7gn.4xlarge
          - c8g.4xlarge
          - m6g.2xlarge
          - m6g.4xlarge
          - m6gd.2xlarge
          - m6gd.4xlarge
          - m7g.2xlarge
          - m7g.4xlarge
          - m7gd.2xlarge
          - m7gd.4xlarge
          - m8g.2xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:09Z"
      message: ""
      observedGeneration: 930
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 930
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:03Z"
      message: object is awaiting reconciliation
      observedGeneration: 930
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 930
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "3756933665322755481"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:30Z"
    generation: 942
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2c-nt5f6
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-nt5f6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:55Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:07Z"
    name: node-arm-usw2c-nt5f6-0
    resourceVersion: "3427338866"
    uid: 00507fa7-3c5e-43e6-9f26-7facfa58b390
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-arm
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-usw2c-nt5f6
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm: "true"
          node-type: node-arm
          node.kubernetes.io/role: node-arm
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-nt5f6
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-usw2c-nt5f6
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.4xlarge
          - c6gd.4xlarge
          - c6gn.4xlarge
          - c7g.4xlarge
          - c7gd.4xlarge
          - c7gn.4xlarge
          - c8g.4xlarge
          - m6g.2xlarge
          - m6g.4xlarge
          - m6gd.2xlarge
          - m6gd.4xlarge
          - m7g.2xlarge
          - m7g.4xlarge
          - m7gd.2xlarge
          - m7gd.4xlarge
          - m8g.2xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 928
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:38Z"
      message: ""
      observedGeneration: 928
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:37Z"
      message: object is awaiting reconciliation
      observedGeneration: 928
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:55Z"
      message: ""
      observedGeneration: 928
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15838600678817066645"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:38Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-arm-usw2c-rpkp2
      node-role.kubernetes.compute.zende.sk/node-arm: "true"
      node-type: node-arm
      node.kubernetes.io/role: node-arm
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-rpkp2
      nodegroup-provider: karpenter
      nodegroupDeployment: node-arm-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:38Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-arm: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-arm: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:38Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:hugepages-32Mi: {}
            f:hugepages-64Ki: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:41:33Z"
    name: node-arm-usw2c-rpkp2-0
    resourceVersion: "3417176440"
    uid: e3c259f9-9607-4bbe-92e4-2096bc4a155f
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "48"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-arm
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-arm-usw2c-rpkp2
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-arm: "true"
          node-type: node-arm
          node.kubernetes.io/role: node-arm
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-rpkp2
          nodegroup-provider: karpenter
          nodegroupDeployment: node-arm-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-arm-usw2c-rpkp2
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c6g.4xlarge
          - c6gd.4xlarge
          - c6gn.4xlarge
          - c7g.4xlarge
          - c7gd.4xlarge
          - c7gn.4xlarge
          - c8g.4xlarge
          - m6g.2xlarge
          - m6g.4xlarge
          - m6gd.2xlarge
          - m6gd.4xlarge
          - m7g.2xlarge
          - m7g.4xlarge
          - m7gd.2xlarge
          - m7gd.4xlarge
          - m8g.2xlarge
          - m8g.4xlarge
          - r6g.2xlarge
          - r6g.4xlarge
          - r6gd.2xlarge
          - r6gd.4xlarge
          - r7g.2xlarge
          - r7g.4xlarge
          - r7gd.2xlarge
          - r7gd.4xlarge
          - r8g.2xlarge
          - r8g.4xlarge
          - r8gd.2xlarge
          - r8gd.4xlarge
          - x2gd.2xlarge
          - x2gd.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-arm
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:40Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:34Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:40Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 1
    resources:
      cpu: "8"
      ephemeral-storage: 203034800Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      hugepages-32Mi: "0"
      hugepages-64Ki: "0"
      memory: 32120156Ki
      nodes: "1"
      pods: "160"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14247483834921945440"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T18:52:36Z"
    generation: 2
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2a-npr4j
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T18:52:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:42:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:46:51Z"
    name: node-nvidia-gpu-large-usw2a-npr4j-0
    resourceVersion: "2981591267"
    uid: b687bed8-f03b-4ff9-b9f8-f114bd1b18a7
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2458
          SfnKubernetes: 8ca5cfa8236c050078d049454ee7792b006e9be1
          StackName: k8s-node-nvidia-gpu-large
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-large-usw2a-npr4j
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: "true"
          node-type: node-nvidia-gpu-large
          node.kubernetes.io/role: node-nvidia-gpu-large
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2a-npr4j
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-large-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-large-usw2a-npr4j
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
          - g4dn.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-large
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T18:52:36Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-08-28T21:42:25Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:46:48Z"
      message: NodeClass not found on cluster
      observedGeneration: 2
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:46:48Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "2723778260290192188"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T21:40:36Z"
    generation: 1
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2a-vgb5v
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T21:40:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:40:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:46:53Z"
    name: node-nvidia-gpu-large-usw2a-vgb5v-0
    resourceVersion: "2981591306"
    uid: 353ca888-d9b1-4c70-b470-c53772566d08
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2459
          SfnKubernetes: 280cb984ebb8c9ea67a19a4c4f1de9240afed2d9
          StackName: k8s-node-nvidia-gpu-large
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-large-usw2a-vgb5v
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: "true"
          node-type: node-nvidia-gpu-large
          node.kubernetes.io/role: node-nvidia-gpu-large
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2a-vgb5v
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-large-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-large-usw2a-vgb5v
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
          - g4dn.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-large
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T21:40:36Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-10-15T05:46:50Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:46:50Z"
      message: NodeClass not found on cluster
      observedGeneration: 1
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:46:50Z"
      message: NodeClassReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1565269793719825649"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T21:40:26Z"
    generation: 1
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2b-gbv5m
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T21:40:26Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:40:26Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:46:54Z"
    name: node-nvidia-gpu-large-usw2b-gbv5m-0
    resourceVersion: "2981591337"
    uid: bc63ff23-3e37-4778-97ac-b7ec7ca40013
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2459
          SfnKubernetes: 280cb984ebb8c9ea67a19a4c4f1de9240afed2d9
          StackName: k8s-node-nvidia-gpu-large
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-large-usw2b-gbv5m
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: "true"
          node-type: node-nvidia-gpu-large
          node.kubernetes.io/role: node-nvidia-gpu-large
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2b-gbv5m
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-large-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-large-usw2b-gbv5m
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
          - g4dn.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-large
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T21:40:26Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-10-15T05:46:51Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:46:51Z"
      message: NodeClass not found on cluster
      observedGeneration: 1
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:46:51Z"
      message: NodeClassReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1320935415636410039"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T18:52:41Z"
    generation: 2
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2b-mmq4q
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T18:52:41Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:42:16Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:08Z"
    name: node-nvidia-gpu-large-usw2b-mmq4q-0
    resourceVersion: "2981591579"
    uid: d25101c2-eea2-4fb4-a5c9-3bd0c72c0719
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2458
          SfnKubernetes: 8ca5cfa8236c050078d049454ee7792b006e9be1
          StackName: k8s-node-nvidia-gpu-large
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-large-usw2b-mmq4q
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: "true"
          node-type: node-nvidia-gpu-large
          node.kubernetes.io/role: node-nvidia-gpu-large
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2b-mmq4q
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-large-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-large-usw2b-mmq4q
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
          - g4dn.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-large
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T18:52:41Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-08-28T21:42:16Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:05Z"
      message: NodeClass not found on cluster
      observedGeneration: 2
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:05Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17789276541535374735"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T21:40:25Z"
    generation: 1
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2c-5pb2j
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T21:40:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:40:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:46:56Z"
    name: node-nvidia-gpu-large-usw2c-5pb2j-0
    resourceVersion: "2981591377"
    uid: c65f19ef-2767-40f8-9192-eda64af055bf
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2459
          SfnKubernetes: 280cb984ebb8c9ea67a19a4c4f1de9240afed2d9
          StackName: k8s-node-nvidia-gpu-large
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-large-usw2c-5pb2j
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: "true"
          node-type: node-nvidia-gpu-large
          node.kubernetes.io/role: node-nvidia-gpu-large
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2c-5pb2j
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-large-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-large-usw2c-5pb2j
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
          - g4dn.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-large
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T21:40:25Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-10-15T05:46:53Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:46:53Z"
      message: NodeClass not found on cluster
      observedGeneration: 1
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:46:53Z"
      message: NodeClassReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13817772792605319166"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T18:52:24Z"
    generation: 2
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2c-c96lq
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T18:52:24Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:42:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:57Z"
    name: node-nvidia-gpu-large-usw2c-c96lq-0
    resourceVersion: "2981592612"
    uid: 7253206c-383f-4004-88e9-891f1596b7d9
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2458
          SfnKubernetes: 8ca5cfa8236c050078d049454ee7792b006e9be1
          StackName: k8s-node-nvidia-gpu-large
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-large-usw2c-c96lq
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-large: "true"
          node-type: node-nvidia-gpu-large
          node.kubernetes.io/role: node-nvidia-gpu-large
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-large-usw2c-c96lq
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-large-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-large-usw2c-c96lq
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
          - g4dn.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-large
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T18:52:24Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-08-28T21:42:15Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:54Z"
      message: NodeClass not found on cluster
      observedGeneration: 2
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:54Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14444235493744154215"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-28T02:58:15Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:58:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T02:58:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:nvidia.com/gpu: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:04:38Z"
    name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    resourceVersion: "3427195799"
    uid: 349eca7e-d998-4f9c-af5d-a8ee1812fa26
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: dra
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-28T02:58:15Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:59:47Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 3
    resources:
      cpu: "12"
      ephemeral-storage: 639657360Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 48501028Ki
      nodes: "3"
      nvidia.com/gpu: "0"
      pods: "480"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14444235493744154215"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-28T02:58:15Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-28T02:58:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T02:58:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:58:17Z"
    name: node-nvidia-gpu-rajeesh-usw2a-8ck9g-1
    resourceVersion: "3427185968"
    uid: 8a680867-bcb3-41d9-b341-5dbe9fd300db
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: dra
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-28T02:58:15Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-28T02:58:15Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:58:17Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12537053137681239993"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-27T01:07:05Z"
    generation: 12
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-27T01:07:05Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:58:47Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:05Z"
    name: node-nvidia-gpu-rajeesh-usw2a-pgrqb-0
    resourceVersion: "3427338815"
    uid: 47cfa1fe-af51-4c59-b83d-15a83b7aa76f
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: dra
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-pgrqb
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-27T01:07:05Z"
      message: ""
      observedGeneration: 5
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 5
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:58:46Z"
      message: object is awaiting reconciliation
      observedGeneration: 5
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:58:47Z"
      message: ""
      observedGeneration: 5
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12537053137681239993"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-27T01:07:05Z"
    generation: 12
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: dra
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-27T01:07:05Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:58:47Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:05Z"
    name: node-nvidia-gpu-rajeesh-usw2a-pgrqb-1
    resourceVersion: "3427338814"
    uid: a5dddb15-cd0c-4f54-80f9-7482bae0938d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: dra
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-pgrqb
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-rajeesh-usw2a-pgrqb
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-27T01:07:05Z"
      message: ""
      observedGeneration: 4
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-27T01:07:05Z"
      message: object is awaiting reconciliation
      observedGeneration: 4
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-27T01:07:07Z"
      message: ""
      observedGeneration: 4
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:58:47Z"
      message: ""
      observedGeneration: 4
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5011227653280716520"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:29Z"
    generation: 714
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-vq84w
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-vq84w
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:29Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-spot-usw2a-vq84w-0
    resourceVersion: "3427338791"
    uid: 69318ffb-c2da-4e33-9935-ba63e43d406f
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-vq84w
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-vq84w
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2a-vq84w
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 702
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:40Z"
      message: ""
      observedGeneration: 702
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:48Z"
      message: object is awaiting reconciliation
      observedGeneration: 702
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 702
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5011227653280716520"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:29Z"
    generation: 568
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-vq84w
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-vq84w
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:29Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-spot-usw2a-vq84w-1
    resourceVersion: "3427338792"
    uid: a3ccbfb9-7755-4620-91a3-d5d5037afee8
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-vq84w
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-vq84w
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2a-vq84w
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: object is awaiting reconciliation
      observedGeneration: 560
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 560
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:40Z"
      message: ""
      observedGeneration: 560
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 560
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "10902029565798891986"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:57Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:57Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:57Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:57Z"
    name: node-nvidia-gpu-shared-spot-usw2a-xbk5g-0
    resourceVersion: "3417202218"
    uid: 561fb2cf-e6d3-4ca4-9bfd-6c9531803a0d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-xbk5g
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:38Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "10902029565798891986"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:57Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:57Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:57Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:59Z"
    name: node-nvidia-gpu-shared-spot-usw2a-xbk5g-1
    resourceVersion: "3417160976"
    uid: ab119a97-35f9-414f-97b5-436a778c1107
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2a-xbk5g
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2a-xbk5g
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:59Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5883339628228486155"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:33Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-gd274
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-gd274
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:33Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:33Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:17Z"
    name: node-nvidia-gpu-shared-spot-usw2b-gd274-0
    resourceVersion: "3417201088"
    uid: e4ae9ddd-d627-4056-9a9a-ad4d11a8e0c1
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-gd274
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-gd274
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2b-gd274
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:54Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5883339628228486155"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:33Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-gd274
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-gd274
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:33Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:33Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:34Z"
    name: node-nvidia-gpu-shared-spot-usw2b-gd274-1
    resourceVersion: "3417159843"
    uid: 85edaf7c-f10f-4bd1-bf4b-e2a9ff357ac9
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-gd274
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-gd274
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2b-gd274
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "11036276478311346869"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:10Z"
    generation: 678
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-szh5s
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-szh5s
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:10Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-nvidia-gpu-shared-spot-usw2b-szh5s-0
    resourceVersion: "3427338611"
    uid: 9e706870-bcc8-4513-9701-a4d79b406897
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-szh5s
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-szh5s
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2b-szh5s
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: ""
      observedGeneration: 668
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 668
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:58Z"
      message: object is awaiting reconciliation
      observedGeneration: 668
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 668
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "11036276478311346869"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:10Z"
    generation: 578
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-szh5s
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-szh5s
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:10Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-nvidia-gpu-shared-spot-usw2b-szh5s-1
    resourceVersion: "3427338612"
    uid: 1353ca16-3545-44c1-a569-284391330995
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2b-szh5s
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2b-szh5s
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2b-szh5s
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: ""
      observedGeneration: 569
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:10Z"
      message: object is awaiting reconciliation
      observedGeneration: 569
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 569
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 569
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17047407912079069066"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:11Z"
    generation: 687
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-br9dl
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-br9dl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:11Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-spot-usw2c-br9dl-0
    resourceVersion: "3427338793"
    uid: c2931617-2294-4bb9-9f89-0c5c0355b2a6
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-br9dl
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-br9dl
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2c-br9dl
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 679
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 679
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:07Z"
      message: object is awaiting reconciliation
      observedGeneration: 679
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 679
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17047407912079069066"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:11Z"
    generation: 566
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-br9dl
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-br9dl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:11Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-spot-usw2c-br9dl-1
    resourceVersion: "3427338794"
    uid: fe7e5258-0344-4e39-86ea-a11d088fa1db
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-br9dl
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-br9dl
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2c-br9dl
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: object is awaiting reconciliation
      observedGeneration: 558
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:11Z"
      message: ""
      observedGeneration: 558
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:13Z"
      message: ""
      observedGeneration: 558
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 558
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12843351317218425197"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:36Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-nssvz
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-nssvz
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:17Z"
    name: node-nvidia-gpu-shared-spot-usw2c-nssvz-0
    resourceVersion: "3417201083"
    uid: 5ad6c23b-33a5-4f06-8713-15ecbd2dc09e
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-nssvz
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-nssvz
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2c-nssvz
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12843351317218425197"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:36Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-nssvz
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
      node-type: node-nvidia-gpu-shared-spot
      node.kubernetes.io/role: node-nvidia-gpu-shared-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-nssvz
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:37Z"
    name: node-nvidia-gpu-shared-spot-usw2c-nssvz-1
    resourceVersion: "3417159979"
    uid: 137d03cc-0a7e-42dd-ba54-7f779f1b1f44
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-spot-usw2c-nssvz
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared-spot: "true"
          node-type: node-nvidia-gpu-shared-spot
          node.kubernetes.io/role: node-nvidia-gpu-shared-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-spot-usw2c-nssvz
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-spot-usw2c-nssvz
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "3060190816281640090"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:05Z"
    generation: 676
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-2nn5b
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-2nn5b
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:05Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:54Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:06Z"
    name: node-nvidia-gpu-shared-usw2a-2nn5b-0
    resourceVersion: "3427338821"
    uid: ad7ba39a-bf31-4c48-b281-a7252c303dbc
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-2nn5b
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-2nn5b
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2a-2nn5b
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: ""
      observedGeneration: 667
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 667
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:05Z"
      message: object is awaiting reconciliation
      observedGeneration: 667
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:54Z"
      message: ""
      observedGeneration: 667
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "3060190816281640090"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:05Z"
    generation: 562
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-2nn5b
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-2nn5b
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:05Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:54Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:06Z"
    name: node-nvidia-gpu-shared-usw2a-2nn5b-1
    resourceVersion: "3427338822"
    uid: a365d416-e162-4cd1-9694-664b5f77d02f
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-2nn5b
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-2nn5b
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2a-2nn5b
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: object is awaiting reconciliation
      observedGeneration: 554
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:05Z"
      message: ""
      observedGeneration: 554
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 554
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:54Z"
      message: ""
      observedGeneration: 554
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "4468238999752342966"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T21:40:18Z"
    generation: 1
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-7h7jd
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T21:40:18Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:40:18Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:22Z"
    name: node-nvidia-gpu-shared-usw2a-7h7jd-0
    resourceVersion: "2981591907"
    uid: d0ca9bc5-d587-4ae5-9a3d-9301800527ef
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2459
          SfnKubernetes: 280cb984ebb8c9ea67a19a4c4f1de9240afed2d9
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-7h7jd
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-7h7jd
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2a-7h7jd
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.xlarge
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g6.xlarge
          - g6.2xlarge
          - g6.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T21:40:18Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-10-15T05:47:19Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:19Z"
      message: NodeClass not found on cluster
      observedGeneration: 1
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:19Z"
      message: NodeClassReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "4617822205921990691"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T18:52:25Z"
    generation: 2
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-95jmr
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T18:52:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:42:39Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:24Z"
    name: node-nvidia-gpu-shared-usw2a-95jmr-0
    resourceVersion: "2981591953"
    uid: 6d50c331-6fd8-42d3-a943-c27287912cb9
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2458
          SfnKubernetes: 8ca5cfa8236c050078d049454ee7792b006e9be1
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-95jmr
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-95jmr
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2a-95jmr
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.xlarge
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g6.xlarge
          - g6.2xlarge
          - g6.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T18:52:25Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-08-28T21:42:39Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:21Z"
      message: NodeClass not found on cluster
      observedGeneration: 2
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:21Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9009776887369560151"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:46Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-m5gqh
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-m5gqh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:17Z"
    name: node-nvidia-gpu-shared-usw2a-m5gqh-0
    resourceVersion: "3417201086"
    uid: 9523f282-e81d-4eda-a6a2-80d098d89f65
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-m5gqh
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-m5gqh
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2a-m5gqh
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9009776887369560151"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:46Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-m5gqh
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-m5gqh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:46Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:49Z"
    name: node-nvidia-gpu-shared-usw2a-m5gqh-1
    resourceVersion: "3417160511"
    uid: e9ba370b-eee3-4cb5-9c2f-dc32d57a1e03
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2a-m5gqh
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2a-m5gqh
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2a-m5gqh
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9130101933425432667"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:53Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-27x7j
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-27x7j
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:17Z"
    name: node-nvidia-gpu-shared-usw2b-27x7j-0
    resourceVersion: "3417201087"
    uid: d28edb05-d054-43e7-849b-b64006b90c8c
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-27x7j
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-27x7j
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2b-27x7j
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:53Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:40Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9130101933425432667"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:53Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-27x7j
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-27x7j
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:54Z"
    name: node-nvidia-gpu-shared-usw2b-27x7j-1
    resourceVersion: "3417160762"
    uid: c6279708-fd18-458a-ae70-91ddb7fe7ab7
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-27x7j
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-27x7j
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2b-27x7j
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:53Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:53Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:54Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14431325412296233730"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:25Z"
    generation: 686
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-8b7nw
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-8b7nw
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-usw2b-8b7nw-0
    resourceVersion: "3427338773"
    uid: 86e9b05d-6d16-4ef8-b84a-0ff4a4f11d8d
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-8b7nw
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-8b7nw
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2b-8b7nw
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 678
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 678
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:10Z"
      message: object is awaiting reconciliation
      observedGeneration: 678
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 678
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14431325412296233730"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:25Z"
    generation: 555
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-8b7nw
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-8b7nw
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-usw2b-8b7nw-1
    resourceVersion: "3427338771"
    uid: 690d7d6e-c76f-4004-922e-7a93e8d02f1b
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-8b7nw
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-8b7nw
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2b-8b7nw
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 547
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: object is awaiting reconciliation
      observedGeneration: 547
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:29Z"
      message: ""
      observedGeneration: 547
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 547
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "7329102824052362953"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T18:52:30Z"
    generation: 2
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-pw9gn
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T18:52:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:42:36Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:30Z"
    name: node-nvidia-gpu-shared-usw2b-pw9gn-0
    resourceVersion: "2981592067"
    uid: 07463ef2-8bb0-4938-8c8b-c9664495c62b
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2458
          SfnKubernetes: 8ca5cfa8236c050078d049454ee7792b006e9be1
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-pw9gn
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-pw9gn
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2b-pw9gn
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.xlarge
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g6.xlarge
          - g6.2xlarge
          - g6.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T18:52:30Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-08-28T21:42:36Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:27Z"
      message: NodeClass not found on cluster
      observedGeneration: 2
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:27Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13492086026628122743"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T21:40:15Z"
    generation: 1
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-qj9rc
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T21:40:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:40:15Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:32Z"
    name: node-nvidia-gpu-shared-usw2b-qj9rc-0
    resourceVersion: "2981592102"
    uid: 963c3b48-d745-448c-a160-f3be34f6a70a
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2459
          SfnKubernetes: 280cb984ebb8c9ea67a19a4c4f1de9240afed2d9
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2b-qj9rc
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2b-qj9rc
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2b-qj9rc
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.xlarge
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g6.xlarge
          - g6.2xlarge
          - g6.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T21:40:15Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-10-15T05:47:29Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:29Z"
      message: NodeClass not found on cluster
      observedGeneration: 1
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:29Z"
      message: NodeClassReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "18224156543751570890"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T21:40:27Z"
    generation: 1
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-d8z54
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T21:40:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:40:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:40Z"
    name: node-nvidia-gpu-shared-usw2c-d8z54-0
    resourceVersion: "2981592259"
    uid: 924cb1fb-bdf1-496e-b422-aacc74064af5
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2459
          SfnKubernetes: 280cb984ebb8c9ea67a19a4c4f1de9240afed2d9
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-d8z54
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-d8z54
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2c-d8z54
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.xlarge
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g6.xlarge
          - g6.2xlarge
          - g6.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T21:40:27Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-10-15T05:47:37Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:37Z"
      message: NodeClass not found on cluster
      observedGeneration: 1
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:37Z"
      message: NodeClassReady=False
      observedGeneration: 1
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "7758426776212187342"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:14Z"
    generation: 677
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-gchkd
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-gchkd
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:14Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-usw2c-gchkd-0
    resourceVersion: "3427338774"
    uid: e85eb5eb-e4b7-4a4e-9118-ea3294f107fc
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-gchkd
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-gchkd
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2c-gchkd
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 666
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 666
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:58Z"
      message: object is awaiting reconciliation
      observedGeneration: 666
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 666
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "7758426776212187342"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:14Z"
    generation: 568
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-gchkd
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-gchkd
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:14Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-shared-usw2c-gchkd-1
    resourceVersion: "3427338775"
    uid: 4d65f47d-2e59-4ad6-9f00-7a047d371870
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmpty
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-gchkd
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-gchkd
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2c-gchkd
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 560
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: object is awaiting reconciliation
      observedGeneration: 560
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 560
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 560
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "14585270678864016213"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-08-28T18:52:22Z"
    generation: 2
    labels:
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-lz9h2
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-08-28T18:52:22Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-08-28T21:43:13Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2025-10-15T05:47:46Z"
    name: node-nvidia-gpu-shared-usw2c-lz9h2-0
    resourceVersion: "2981592388"
    uid: e8bdccad-93dd-49db-a197-2c1aa644502d
  spec:
    disruption:
      budgets:
      - nodes: 10%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2458
          SfnKubernetes: 8ca5cfa8236c050078d049454ee7792b006e9be1
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: d6059c0e
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-lz9h2
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-lz9h2
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2c-lz9h2
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.xlarge
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g6.xlarge
          - g6.2xlarge
          - g6.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2025-08-28T18:52:23Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-08-28T21:43:13Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-10-15T05:47:44Z"
      message: NodeClass not found on cluster
      observedGeneration: 2
      reason: NodeClassNotFound
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2025-10-15T05:47:44Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 2
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12026451535306340939"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:34Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-z4v46
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-z4v46
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:17Z"
    name: node-nvidia-gpu-shared-usw2c-z4v46-0
    resourceVersion: "3417201085"
    uid: c66b2ef2-4814-4c6a-9ef0-1fb107d72f8e
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-z4v46
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-z4v46
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2c-z4v46
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12026451535306340939"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:34Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-shared
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/gpu-shared: "true"
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-z4v46
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
      node-type: node-nvidia-gpu-shared
      node.kubernetes.io/role: node-nvidia-gpu-shared
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-z4v46
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-shared-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/gpu-shared: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/gpu-shared: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:34Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:35Z"
    name: node-nvidia-gpu-shared-usw2c-z4v46-1
    resourceVersion: "3417159888"
    uid: 28ec4de6-1b31-4bdb-abaf-436dbc74033b
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-shared
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/gpu-shared: "true"
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-shared-usw2c-z4v46
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-shared: "true"
          node-type: node-nvidia-gpu-shared
          node.kubernetes.io/role: node-nvidia-gpu-shared
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-shared-usw2c-z4v46
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-shared-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-shared-usw2c-z4v46
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-shared
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1152047374648953371"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:31Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-ppn5t
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-ppn5t
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:31Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:31Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:50:52Z"
    name: node-nvidia-gpu-spot-usw2a-ppn5t-0
    resourceVersion: "3417200338"
    uid: 89cba676-0ec6-48b3-a045-cb160d8eb4a2
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-ppn5t
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-ppn5t
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2a-ppn5t
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:32Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1152047374648953371"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:31Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-ppn5t
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-ppn5t
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:31Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:31Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:33Z"
    name: node-nvidia-gpu-spot-usw2a-ppn5t-1
    resourceVersion: "3417159782"
    uid: 934ae2f0-02a0-4cbe-97b3-948a55679c32
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-ppn5t
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-ppn5t
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2a-ppn5t
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:31Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:33Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15310158553306314844"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:27Z"
    generation: 674
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-pvgnw
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-pvgnw
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:51Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:03Z"
    name: node-nvidia-gpu-spot-usw2a-pvgnw-0
    resourceVersion: "3427338743"
    uid: 99f25999-d517-4c20-b71f-6ad45888fa26
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-pvgnw
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-pvgnw
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2a-pvgnw
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: ""
      observedGeneration: 663
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:36Z"
      message: ""
      observedGeneration: 663
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:57Z"
      message: object is awaiting reconciliation
      observedGeneration: 663
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:51Z"
      message: ""
      observedGeneration: 663
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15310158553306314844"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:27Z"
    generation: 556
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-pvgnw
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-pvgnw
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:51Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:03Z"
    name: node-nvidia-gpu-spot-usw2a-pvgnw-1
    resourceVersion: "3427338742"
    uid: d80cd3de-114e-4e02-bd91-89f0586acfc4
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2a-pvgnw
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2a-pvgnw
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2a-pvgnw
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: object is awaiting reconciliation
      observedGeneration: 546
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: ""
      observedGeneration: 546
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:36Z"
      message: ""
      observedGeneration: 546
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:51Z"
      message: ""
      observedGeneration: 546
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1434522510058362195"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:01Z"
    generation: 677
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-gwkj6
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-gwkj6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:01Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-spot-usw2b-gwkj6-0
    resourceVersion: "3427338767"
    uid: 77e74073-8c22-4021-b63c-24cf1d99188a
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-gwkj6
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-gwkj6
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2b-gwkj6
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:01Z"
      message: ""
      observedGeneration: 668
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 668
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:03Z"
      message: object is awaiting reconciliation
      observedGeneration: 668
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 668
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1434522510058362195"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:01Z"
    generation: 554
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-gwkj6
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-gwkj6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:01Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-nvidia-gpu-spot-usw2b-gwkj6-1
    resourceVersion: "3427338768"
    uid: 7766f5a7-be90-41db-b22e-3dd0c8e3410c
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-gwkj6
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-gwkj6
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2b-gwkj6
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:02Z"
      message: object is awaiting reconciliation
      observedGeneration: 546
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:02Z"
      message: ""
      observedGeneration: 546
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:03Z"
      message: ""
      observedGeneration: 546
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 546
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1647186158247583086"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:44Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-jvnqj
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-jvnqj
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:44Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:44Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:17Z"
    name: node-nvidia-gpu-spot-usw2b-jvnqj-0
    resourceVersion: "3417201090"
    uid: 5131a412-0a7f-44e4-8e6a-3d484807ba27
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-jvnqj
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-jvnqj
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2b-jvnqj
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:39Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "1647186158247583086"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:44Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-jvnqj
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-jvnqj
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:44Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:44Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:46Z"
    name: node-nvidia-gpu-spot-usw2b-jvnqj-1
    resourceVersion: "3417160347"
    uid: e4b3b50b-8147-4fe5-b1b8-776c1f9a41fb
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2b-jvnqj
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2b-jvnqj
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2b-jvnqj
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13395811169802903272"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:47Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-5wgf8
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-5wgf8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:47Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:47Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:37Z"
    name: node-nvidia-gpu-spot-usw2c-5wgf8-0
    resourceVersion: "3417201567"
    uid: e38b7c00-f1e2-4b31-b53a-da8c5296acaa
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-5wgf8
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-5wgf8
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2c-5wgf8
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:47Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:30Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13395811169802903272"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:47Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-5wgf8
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-5wgf8
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:47Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:47Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:49Z"
    name: node-nvidia-gpu-spot-usw2c-5wgf8-1
    resourceVersion: "3417160549"
    uid: 42f37909-1908-44a0-b861-46f92ced3e00
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-5wgf8
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-5wgf8
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2c-5wgf8
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:47Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:47Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15744435764192759317"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:07Z"
    generation: 684
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-67wwh
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-67wwh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:07Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-nvidia-gpu-spot-usw2c-67wwh-0
    resourceVersion: "3427338601"
    uid: 1d8abbd7-b7d6-4ba8-b331-e426070b5507
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-67wwh
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-67wwh
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2c-67wwh
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 674
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:09Z"
      message: ""
      observedGeneration: 674
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:06Z"
      message: object is awaiting reconciliation
      observedGeneration: 674
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 674
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15744435764192759317"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:07Z"
    generation: 571
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-67wwh
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
      node-type: node-nvidia-gpu-spot
      node.kubernetes.io/role: node-nvidia-gpu-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-67wwh
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:07Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-nvidia-gpu-spot-usw2c-67wwh-1
    resourceVersion: "3427338600"
    uid: b7d255ca-1157-4121-b3a7-b0ec860a805c
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-spot-usw2c-67wwh
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu-spot: "true"
          node-type: node-nvidia-gpu-spot
          node.kubernetes.io/role: node-nvidia-gpu-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-spot-usw2c-67wwh
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-spot-usw2c-67wwh
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu-spot
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: ""
      observedGeneration: 564
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:07Z"
      message: object is awaiting reconciliation
      observedGeneration: 564
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:09Z"
      message: ""
      observedGeneration: 564
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 564
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13942474019727098925"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:32Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-8nf24
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-8nf24
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:32Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:32Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T02:00:40Z"
    name: node-nvidia-gpu-usw2a-8nf24-0
    resourceVersion: "3417217590"
    uid: b15d8aa9-ea9e-4042-af0a-f6ab0b7647b8
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-8nf24
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-8nf24
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2a-8nf24
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:39Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "13942474019727098925"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:32Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-8nf24
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-8nf24
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:32Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:32Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:34Z"
    name: node-nvidia-gpu-usw2a-8nf24-1
    resourceVersion: "3417159838"
    uid: f83444c1-e003-4938-8dff-f61643e2519d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-8nf24
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-8nf24
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2a-8nf24
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:34Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "16687383976434758540"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:27Z"
    generation: 693
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-j5qg5
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-j5qg5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:01Z"
    name: node-nvidia-gpu-usw2a-j5qg5-0
    resourceVersion: "3427338622"
    uid: cfdcf337-eeb6-4e11-a686-9f1b01ba25bb
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-j5qg5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-j5qg5
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2a-j5qg5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: ""
      observedGeneration: 680
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 680
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:39:58Z"
      message: object is awaiting reconciliation
      observedGeneration: 680
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 680
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "16687383976434758540"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:27Z"
    generation: 572
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-j5qg5
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-j5qg5
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:01Z"
    name: node-nvidia-gpu-usw2a-j5qg5-1
    resourceVersion: "3427338623"
    uid: 37b92c5d-2b2c-4eb1-9d38-d8cf1fadb944
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2a-j5qg5
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2a-j5qg5
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2a-j5qg5
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: ""
      observedGeneration: 565
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:27Z"
      message: object is awaiting reconciliation
      observedGeneration: 565
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:37Z"
      message: ""
      observedGeneration: 565
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 565
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17437726020320627363"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:04Z"
    generation: 721
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-7whnf
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-7whnf
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:04Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-nvidia-gpu-usw2b-7whnf-0
    resourceVersion: "3427338603"
    uid: 04e2530f-0906-445b-9fba-ace478bafd66
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-7whnf
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-7whnf
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2b-7whnf
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:04Z"
      message: ""
      observedGeneration: 711
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 711
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:01Z"
      message: object is awaiting reconciliation
      observedGeneration: 711
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 711
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17437726020320627363"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:04Z"
    generation: 584
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-7whnf
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-7whnf
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:04Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-nvidia-gpu-usw2b-7whnf-1
    resourceVersion: "3427338604"
    uid: f70d77bf-44b3-4645-b1d5-a1d043ca241d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-7whnf
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-7whnf
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2b-7whnf
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:04Z"
      message: object is awaiting reconciliation
      observedGeneration: 576
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:04Z"
      message: ""
      observedGeneration: 576
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:06Z"
      message: ""
      observedGeneration: 576
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 576
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5463016951132968842"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:30Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-ff8kp
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-ff8kp
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:51:28Z"
    name: node-nvidia-gpu-usw2b-ff8kp-0
    resourceVersion: "3417201287"
    uid: 14a0036c-f50f-474d-bb7d-14c0ec11729b
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-ff8kp
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-ff8kp
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2b-ff8kp
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:30Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5463016951132968842"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:30Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-ff8kp
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-ff8kp
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:32Z"
    name: node-nvidia-gpu-usw2b-ff8kp-1
    resourceVersion: "3417159724"
    uid: baf4be6e-63b5-4c61-9dd9-e46ce055af4d
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2b-ff8kp
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2b-ff8kp
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2b-ff8kp
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:30Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:30Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:32Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5889774768663712701"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:43Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-9rbjq
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-9rbjq
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:43Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:43Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T01:34:11Z"
    name: node-nvidia-gpu-usw2c-9rbjq-0
    resourceVersion: "3427063473"
    uid: f36686b6-927c-46c3-b196-be710539502c
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-9rbjq
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-9rbjq
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2c-9rbjq
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:43Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:51Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "5889774768663712701"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:43Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-9rbjq
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-9rbjq
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:43Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:43Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:46Z"
    name: node-nvidia-gpu-usw2c-9rbjq-1
    resourceVersion: "3417160344"
    uid: 166b48f7-9918-4b6d-8323-587f8a024053
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "256"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-9rbjq
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-9rbjq
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2c-9rbjq
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:43Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:43Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:46Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "16731776281730915620"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:26Z"
    generation: 1434
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-tjbsn
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-tjbsn
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:26Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:51Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:03Z"
    name: node-nvidia-gpu-usw2c-tjbsn-0
    resourceVersion: "3427338750"
    uid: e490e6b8-a37c-4983-abbd-25e775cfb9f3
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-tjbsn
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-tjbsn
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2c-tjbsn
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.2xlarge
          - g4dn.4xlarge
          - g4dn.xlarge
          - g5.2xlarge
          - g5.4xlarge
          - g5.xlarge
          - g6.2xlarge
          - g6.4xlarge
          - g6.xlarge
          - g6e.2xlarge
          - g6e.4xlarge
          - g6e.xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:26Z"
      message: ""
      observedGeneration: 1422
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1422
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:22:55Z"
      message: object is awaiting reconciliation
      observedGeneration: 1422
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:51Z"
      message: ""
      observedGeneration: 1422
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "16731776281730915620"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:26Z"
    generation: 1220
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-nvidia-gpu
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/instance-gpu-manufacturer: nvidia
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-tjbsn
      karpenter.sh/do-not-sync-taints: "true"
      node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
      node-type: node-nvidia-gpu
      node.kubernetes.io/role: node-nvidia-gpu
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-tjbsn
      nodegroup-provider: karpenter
      nodegroupDeployment: node-nvidia-gpu-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:26Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:51Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/instance-gpu-manufacturer: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:karpenter.sh/do-not-sync-taints: {}
            f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/instance-gpu-manufacturer: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:03Z"
    name: node-nvidia-gpu-usw2c-tjbsn-1
    resourceVersion: "3427338747"
    uid: 29abc5f6-0f61-4f43-b87d-b525e7b19443
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-nvidia-gpu
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/instance-gpu-manufacturer: nvidia
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-nvidia-gpu-usw2c-tjbsn
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
          node-type: node-nvidia-gpu
          node.kubernetes.io/role: node-nvidia-gpu
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-usw2c-tjbsn
          nodegroup-provider: karpenter
          nodegroupDeployment: node-nvidia-gpu-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-nvidia-gpu-usw2c-tjbsn
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - g4dn.12xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-nvidia-gpu
        terminationGracePeriod: 96h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:26Z"
      message: object is awaiting reconciliation
      observedGeneration: 1210
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:26Z"
      message: ""
      observedGeneration: 1210
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:35Z"
      message: ""
      observedGeneration: 1210
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:51Z"
      message: ""
      observedGeneration: 1210
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "11522083887248565427"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:37Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:37Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:37Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T00:27:07Z"
    name: node-spot-usw2a-cschs-0
    resourceVersion: "3426966520"
    uid: 0ff78475-1b22-4fb5-802f-9f7136f6a360
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2a-cschs
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.12xlarge
          - c5.9xlarge
          - c6i.12xlarge
          - c6i.16xlarge
          - c6i.8xlarge
          - c7i.12xlarge
          - c7i.16xlarge
          - c7i.8xlarge
          - m5.12xlarge
          - m5.16xlarge
          - m5.8xlarge
          - m5d.12xlarge
          - m5d.16xlarge
          - m5d.8xlarge
          - m6i.12xlarge
          - m6i.16xlarge
          - m6i.8xlarge
          - m6id.12xlarge
          - m6id.16xlarge
          - m6id.8xlarge
          - m7i-flex.12xlarge
          - m7i-flex.16xlarge
          - m7i-flex.8xlarge
          - m7i.12xlarge
          - m7i.16xlarge
          - m7i.8xlarge
          - r5.12xlarge
          - r5.16xlarge
          - r5.8xlarge
          - r6i.12xlarge
          - r6i.16xlarge
          - r6i.8xlarge
          - r7i.12xlarge
          - r7i.16xlarge
          - r7i.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:32Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 1
    resources:
      cpu: "32"
      ephemeral-storage: 203056560Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 259756484Ki
      nodes: "1"
      pods: "250"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "11522083887248565427"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:37Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:37Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:37Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:38Z"
    name: node-spot-usw2a-cschs-1
    resourceVersion: "3417160033"
    uid: 3503e4b4-587c-4f68-b1c5-842d2ab52a2f
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2a-cschs
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m5.4xlarge
          - m5d.4xlarge
          - m6i.4xlarge
          - m6id.4xlarge
          - m7i-flex.4xlarge
          - m7i.4xlarge
          - r5.2xlarge
          - r5.4xlarge
          - r6i.2xlarge
          - r6i.4xlarge
          - r7i.2xlarge
          - r7i.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:37Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:38Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "6729627164803748889"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:14Z"
    generation: 855
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-fqjjv
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-fqjjv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:14Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-spot-usw2a-fqjjv-0
    resourceVersion: "3427338614"
    uid: b35f2d9c-eefb-415b-a230-0bc5dddd9837
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2a-fqjjv
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-fqjjv
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2a-fqjjv
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.12xlarge
          - c5.9xlarge
          - c6i.12xlarge
          - c6i.16xlarge
          - c6i.8xlarge
          - c7i.12xlarge
          - c7i.16xlarge
          - c7i.8xlarge
          - m5.12xlarge
          - m5.16xlarge
          - m5.8xlarge
          - m5d.12xlarge
          - m5d.16xlarge
          - m5d.8xlarge
          - m6i.12xlarge
          - m6i.16xlarge
          - m6i.8xlarge
          - m6id.12xlarge
          - m6id.16xlarge
          - m6id.8xlarge
          - m7i-flex.12xlarge
          - m7i-flex.16xlarge
          - m7i-flex.8xlarge
          - m7i.12xlarge
          - m7i.16xlarge
          - m7i.8xlarge
          - r5.12xlarge
          - r5.16xlarge
          - r5.8xlarge
          - r6i.12xlarge
          - r6i.16xlarge
          - r6i.8xlarge
          - r7i.12xlarge
          - r7i.16xlarge
          - r7i.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 845
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 845
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-27T18:58:39Z"
      message: object is awaiting reconciliation
      observedGeneration: 845
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 845
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "6729627164803748889"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:14Z"
    generation: 706
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2a-fqjjv
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-fqjjv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:14Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-spot-usw2a-fqjjv-1
    resourceVersion: "3427338615"
    uid: 259fdb37-2bde-49cb-9d02-c7f7efb3f162
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2a-fqjjv
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-fqjjv
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2a-fqjjv
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m5.4xlarge
          - m5d.4xlarge
          - m6i.4xlarge
          - m6id.4xlarge
          - m7i-flex.4xlarge
          - m7i.4xlarge
          - r5.2xlarge
          - r5.4xlarge
          - r6i.2xlarge
          - r6i.4xlarge
          - r7i.2xlarge
          - r7i.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 696
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: object is awaiting reconciliation
      observedGeneration: 696
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:28Z"
      message: ""
      observedGeneration: 696
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:49Z"
      message: ""
      observedGeneration: 696
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15423021713922297330"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:12Z"
    generation: 718
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-kw9ms
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-kw9ms
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:12Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-spot-usw2b-kw9ms-0
    resourceVersion: "3427338606"
    uid: 9346e5d8-4ec1-4318-8e3c-5656fcd49861
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2b-kw9ms
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-kw9ms
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2b-kw9ms
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.12xlarge
          - c5.9xlarge
          - c6i.12xlarge
          - c6i.16xlarge
          - c6i.8xlarge
          - c7i.12xlarge
          - c7i.16xlarge
          - c7i.8xlarge
          - m5.12xlarge
          - m5.16xlarge
          - m5.8xlarge
          - m5d.12xlarge
          - m5d.16xlarge
          - m5d.8xlarge
          - m6i.12xlarge
          - m6i.16xlarge
          - m6i.8xlarge
          - m6id.12xlarge
          - m6id.16xlarge
          - m6id.8xlarge
          - m7i-flex.12xlarge
          - m7i-flex.16xlarge
          - m7i-flex.8xlarge
          - m7i.12xlarge
          - m7i.16xlarge
          - m7i.8xlarge
          - r5.12xlarge
          - r5.16xlarge
          - r5.8xlarge
          - r6i.12xlarge
          - r6i.16xlarge
          - r6i.8xlarge
          - r7i.12xlarge
          - r7i.16xlarge
          - r7i.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 707
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:14Z"
      message: ""
      observedGeneration: 707
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:07Z"
      message: object is awaiting reconciliation
      observedGeneration: 707
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 707
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "15423021713922297330"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:23:12Z"
    generation: 577
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-kw9ms
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-kw9ms
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:23:12Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-spot-usw2b-kw9ms-1
    resourceVersion: "3427338605"
    uid: d803d367-7bee-4192-98b8-d89d7c24f052
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2b-kw9ms
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-kw9ms
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2b-kw9ms
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m5.4xlarge
          - m5d.4xlarge
          - m6i.4xlarge
          - m6id.4xlarge
          - m7i-flex.4xlarge
          - m7i.4xlarge
          - r5.2xlarge
          - r5.4xlarge
          - r6i.2xlarge
          - r6i.4xlarge
          - r7i.2xlarge
          - r7i.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: ""
      observedGeneration: 568
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:23:12Z"
      message: object is awaiting reconciliation
      observedGeneration: 568
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:23:14Z"
      message: ""
      observedGeneration: 568
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 568
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9229452721016153549"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:49Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T01:12:38Z"
    name: node-spot-usw2b-xv8zl-0
    resourceVersion: "3427030301"
    uid: 3f9f925e-432a-4cc0-b655-689ed68fe507
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2b-xv8zl
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.12xlarge
          - c5.9xlarge
          - c6i.12xlarge
          - c6i.16xlarge
          - c6i.8xlarge
          - c7i.12xlarge
          - c7i.16xlarge
          - c7i.8xlarge
          - m5.12xlarge
          - m5.16xlarge
          - m5.8xlarge
          - m5d.12xlarge
          - m5d.16xlarge
          - m5d.8xlarge
          - m6i.12xlarge
          - m6i.16xlarge
          - m6i.8xlarge
          - m6id.12xlarge
          - m6id.16xlarge
          - m6id.8xlarge
          - m7i-flex.12xlarge
          - m7i-flex.16xlarge
          - m7i-flex.8xlarge
          - m7i.12xlarge
          - m7i.16xlarge
          - m7i.8xlarge
          - r5.12xlarge
          - r5.16xlarge
          - r5.8xlarge
          - r6i.12xlarge
          - r6i.16xlarge
          - r6i.8xlarge
          - r7i.12xlarge
          - r7i.16xlarge
          - r7i.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:32Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 1
    resources:
      cpu: "64"
      ephemeral-storage: 203056560Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 129785212Ki
      nodes: "1"
      pods: "250"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "9229452721016153549"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:49Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:49Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:52Z"
    name: node-spot-usw2b-xv8zl-1
    resourceVersion: "3417160659"
    uid: c294e224-35bf-4b9c-b059-2959d954afe2
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2b-xv8zl
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m5.4xlarge
          - m5d.4xlarge
          - m6i.4xlarge
          - m6id.4xlarge
          - m7i-flex.4xlarge
          - m7i.4xlarge
          - r5.2xlarge
          - r5.4xlarge
          - r6i.2xlarge
          - r6i.4xlarge
          - r7i.2xlarge
          - r7i.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:49Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:52Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12307986074497143793"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:41Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:41Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:41Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T03:01:28Z"
    name: node-spot-usw2c-ltnzv-0
    resourceVersion: "3427190952"
    uid: 9fcf568b-04f5-430e-8f41-03ae7e9e1dd6
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2c-ltnzv
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.12xlarge
          - c5.9xlarge
          - c6i.12xlarge
          - c6i.16xlarge
          - c6i.8xlarge
          - c7i.12xlarge
          - c7i.16xlarge
          - c7i.8xlarge
          - m5.12xlarge
          - m5.16xlarge
          - m5.8xlarge
          - m5d.12xlarge
          - m5d.16xlarge
          - m5d.8xlarge
          - m6i.12xlarge
          - m6i.16xlarge
          - m6i.8xlarge
          - m6id.12xlarge
          - m6id.16xlarge
          - m6id.8xlarge
          - m7i-flex.12xlarge
          - m7i-flex.16xlarge
          - m7i-flex.8xlarge
          - m7i.12xlarge
          - m7i.16xlarge
          - m7i.8xlarge
          - r5.12xlarge
          - r5.16xlarge
          - r5.8xlarge
          - r6i.12xlarge
          - r6i.16xlarge
          - r6i.8xlarge
          - r7i.12xlarge
          - r7i.16xlarge
          - r7i.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:41Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:33Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 2
    resources:
      cpu: "96"
      ephemeral-storage: 406113120Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 193521312Ki
      nodes: "2"
      pods: "500"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12307986074497143793"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:41Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:41Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:41Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T01:38:44Z"
    name: node-spot-usw2c-ltnzv-1
    resourceVersion: "3417160244"
    uid: 652720a8-bc26-4f8e-9caf-1f8961e82501
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2c-ltnzv
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m5.4xlarge
          - m5d.4xlarge
          - m6i.4xlarge
          - m6id.4xlarge
          - m7i-flex.4xlarge
          - m7i.4xlarge
          - r5.2xlarge
          - r5.4xlarge
          - r6i.2xlarge
          - r6i.4xlarge
          - r7i.2xlarge
          - r7i.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:41Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:41Z"
      message: object is awaiting reconciliation
      observedGeneration: 1
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "8060367589003372970"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:53Z"
    generation: 1279
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-tnfz6
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-tnfz6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:05Z"
    name: node-spot-usw2c-tnfz6-0
    resourceVersion: "3427338810"
    uid: 87439c4e-d3b9-4aa4-9599-cdd70ea99dd6
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "3200"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2c-tnfz6
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-tnfz6
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2c-tnfz6
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.12xlarge
          - c5.9xlarge
          - c6i.12xlarge
          - c6i.16xlarge
          - c6i.8xlarge
          - c7i.12xlarge
          - c7i.16xlarge
          - c7i.8xlarge
          - m5.12xlarge
          - m5.16xlarge
          - m5.8xlarge
          - m5d.12xlarge
          - m5d.16xlarge
          - m5d.8xlarge
          - m6i.12xlarge
          - m6i.16xlarge
          - m6i.8xlarge
          - m6id.12xlarge
          - m6id.16xlarge
          - m6id.8xlarge
          - m7i-flex.12xlarge
          - m7i-flex.16xlarge
          - m7i-flex.8xlarge
          - m7i.12xlarge
          - m7i.16xlarge
          - m7i.8xlarge
          - r5.12xlarge
          - r5.16xlarge
          - r5.8xlarge
          - r6i.12xlarge
          - r6i.16xlarge
          - r6i.8xlarge
          - r7i.12xlarge
          - r7i.16xlarge
          - r7i.8xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 20
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:53Z"
      message: ""
      observedGeneration: 1270
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1270
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T00:44:14Z"
      message: object is awaiting reconciliation
      observedGeneration: 1270
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:53Z"
      message: ""
      observedGeneration: 1270
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "8060367589003372970"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:53Z"
    generation: 1080
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node-spot
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-spot-usw2c-tnfz6
      node-role.kubernetes.compute.zende.sk/node-spot: "true"
      node-type: node-spot
      node.kubernetes.io/role: node-spot
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-tnfz6
      nodegroup-provider: karpenter
      nodegroupDeployment: node-spot-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:53Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node-spot: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node-spot: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:taints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:05Z"
    name: node-spot-usw2c-tnfz6-1
    resourceVersion: "3427338811"
    uid: 83438ef1-8b32-4baf-8287-ede8a87493c9
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node-spot
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-spot-usw2c-tnfz6
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node-spot: "true"
          node-type: node-spot
          node.kubernetes.io/role: node-spot
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-tnfz6
          nodegroup-provider: karpenter
          nodegroupDeployment: node-spot-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-spot-usw2c-tnfz6
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - m5.4xlarge
          - m5d.4xlarge
          - m6i.4xlarge
          - m6id.4xlarge
          - m7i-flex.4xlarge
          - m7i.4xlarge
          - r5.2xlarge
          - r5.4xlarge
          - r6i.2xlarge
          - r6i.4xlarge
          - r7i.2xlarge
          - r7i.4xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        taints:
        - effect: NoSchedule
          key: compute.zende.sk/nodegroup
          value: node-spot
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:53Z"
      message: object is awaiting reconciliation
      observedGeneration: 1072
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-22T22:24:53Z"
      message: ""
      observedGeneration: 1072
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:55Z"
      message: ""
      observedGeneration: 1072
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:55:53Z"
      message: ""
      observedGeneration: 1072
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "12351910804501280472"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:14Z"
    generation: 654
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-fm9ph
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-fm9ph
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:14Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:52Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:04Z"
    name: node-usw2a-fm9ph-0
    resourceVersion: "3427338795"
    uid: 67ac0e4c-c8f7-4414-b4ed-08a17e6d3133
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-usw2a-fm9ph
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-fm9ph
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2a-fm9ph
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:14Z"
      message: ""
      observedGeneration: 646
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:16Z"
      message: ""
      observedGeneration: 646
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:13Z"
      message: object is awaiting reconciliation
      observedGeneration: 646
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:52Z"
      message: ""
      observedGeneration: 646
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "11850957934695291872"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:56Z"
    generation: 2
    labels:
      AvailabilityZone: us-west-2a
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2a-z647p
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2a
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:56Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-27T21:00:27Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:54:55Z"
    name: node-usw2a-z647p-0
    resourceVersion: "3427181521"
    uid: 69ad8568-6057-4b0f-8ce7-a13867e4c859
  spec:
    disruption:
      budgets:
      - nodes: 100%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 1s
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "80"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2a
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-usw2a-z647p
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2a
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2a-z647p
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2a
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:56Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:57Z"
      message: ""
      observedGeneration: 2
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-27T21:03:20Z"
      message: ""
      observedGeneration: 2
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-27T21:00:27Z"
      message: ""
      observedGeneration: 2
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 3
    resources:
      cpu: "24"
      ephemeral-storage: 609169680Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 64301248Ki
      nodes: "3"
      pods: "480"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "16124741842375123321"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:35Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:35Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:35Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:37:47Z"
    name: node-usw2b-n7st7-0
    resourceVersion: "3427156576"
    uid: b2196fcb-a3d4-4b03-9ade-c0eeb3dd9364
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "80"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2b-n7st7
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:35Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:37Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:36Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 3
    resources:
      cpu: "24"
      ephemeral-storage: 609169680Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 64477376Ki
      nodes: "3"
      pods: "480"
      vpc.amazonaws.com/pod-eni: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "3214306959788970356"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:25Z"
    generation: 919
    labels:
      AvailabilityZone: us-west-2b
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2b-vthv2
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-vthv2
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2b
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:48Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:51:00Z"
    name: node-usw2b-vthv2-0
    resourceVersion: "3427338591"
    uid: fe0fe65f-4b71-4418-acc1-cc35cb8bd408
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "80"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2b
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-usw2b-vthv2
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-vthv2
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2b
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2b-vthv2
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2b
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:25Z"
      message: ""
      observedGeneration: 911
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:30Z"
      message: ""
      observedGeneration: 911
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-28T02:23:01Z"
      message: object is awaiting reconciliation
      observedGeneration: 911
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:48Z"
      message: ""
      observedGeneration: 911
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "8110373742427624702"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-22T22:24:56Z"
    generation: 540
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2589
      SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2c-8ngx9
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-8ngx9
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-22T22:24:56Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:55:56Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-28T04:50:59Z"
    name: node-usw2c-8ngx9-0
    resourceVersion: "3427338556"
    uid: d0507415-a8cb-422d-8b7a-595237ababd0
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2589
          SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-usw2c-8ngx9
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-8ngx9
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2c-8ngx9
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-22T22:24:56Z"
      message: ""
      observedGeneration: 532
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-22T22:24:58Z"
      message: ""
      observedGeneration: 532
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:01Z"
      message: object is awaiting reconciliation
      observedGeneration: 532
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-28T02:55:56Z"
      message: ""
      observedGeneration: 532
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "11469553144735938026"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-12-22T23:50:30Z"
    generation: 2
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-hqtql
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hqtql
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-12-22T23:50:30Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-12-23T02:03:59Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-13T14:14:43Z"
    name: node-usw2c-hqtql-0
    resourceVersion: "3397602336"
    uid: 86d8bfff-338b-4304-8af2-67213f7c3d82
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "0"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2573
          SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: "04661458"
          k8s.amazonaws.com/eniConfig: node-usw2c-hqtql
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hqtql
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2c-hqtql
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2025-12-22T23:50:30Z"
      message: ""
      observedGeneration: 2
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-12-23T04:34:18Z"
      message: object is awaiting reconciliation
      observedGeneration: 2
      reason: AwaitingReconciliation
      status: Unknown
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-13T14:14:43Z"
      message: ValidationSucceeded=False, AMIsReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-13T14:14:43Z"
      message: NodeClassReady=False
      observedGeneration: 2
      reason: UnhealthyDependents
      status: "False"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "17861209382580500999"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2026-01-23T01:38:42Z"
    generation: 1
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2590
      SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: b7f462ec
      k8s.amazonaws.com/eniConfig: node-usw2c-hsk2x
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hsk2x
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2026-01-23T01:38:42Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2026-01-23T01:38:42Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-23T17:25:23Z"
    name: node-usw2c-hsk2x-0
    resourceVersion: "3418468067"
    uid: 408e6907-5fe0-4082-855a-2d346a13a581
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "80"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2590
          SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: b7f462ec
          k8s.amazonaws.com/eniConfig: node-usw2c-hsk2x
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-hsk2x
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2c-hsk2x
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2026-01-23T01:38:42Z"
      message: ""
      observedGeneration: 1
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2026-01-23T01:40:35Z"
      message: ""
      observedGeneration: 1
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2026-01-23T01:38:44Z"
      message: ""
      observedGeneration: 1
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 0
    resources:
      cpu: "0"
      ephemeral-storage: "0"
      memory: "0"
      nodes: "0"
      pods: "0"
- apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    annotations:
      karpenter.sh/nodepool-hash: "3158429970100788055"
      karpenter.sh/nodepool-hash-version: v3
    creationTimestamp: "2025-12-22T18:41:28Z"
    generation: 93
    labels:
      AvailabilityZone: us-west-2c
      Environment: staging
      PoddedCluster: "false"
      Release: v2573
      SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
      StackName: k8s-node
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: sfn-kubernetes-sandbox
      compute.zende.sk/ebs-initial-burst: enabled
      compute.zende.sk/kube-proxy-mode: daemonset
      compute.zendesk.com/ec2_image_sha: "04661458"
      k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
      node-role.kubernetes.compute.zende.sk/node: "true"
      node-type: node
      node.kubernetes.io/role: node
      nodegroup-operator.compute.zende.sk/cluster-name: sandbox
      nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
      nodegroup-provider: karpenter
      nodegroupDeployment: node-usw2c
      ownedby: compute
      product: foundation
      project: sfn_kubernetes
      provider: sfn
      service: kubernetes
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    managedFields:
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:karpenter.sh/nodepool-hash: {}
            f:karpenter.sh/nodepool-hash-version: {}
      manager: karpenter
      operation: Update
      time: "2025-12-22T18:41:28Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:AvailabilityZone: {}
            f:Environment: {}
            f:PoddedCluster: {}
            f:Release: {}
            f:SfnKubernetes: {}
            f:StackName: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:compute.zende.sk/ebs-initial-burst: {}
            f:compute.zende.sk/kube-proxy-mode: {}
            f:compute.zendesk.com/ec2_image_sha: {}
            f:k8s.amazonaws.com/eniConfig: {}
            f:node-role.kubernetes.compute.zende.sk/node: {}
            f:node-type: {}
            f:node.kubernetes.io/role: {}
            f:nodegroup-operator.compute.zende.sk/cluster-name: {}
            f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
            f:nodegroup-provider: {}
            f:nodegroupDeployment: {}
            f:ownedby: {}
            f:product: {}
            f:project: {}
            f:provider: {}
            f:service: {}
            f:team: {}
            f:topology.kubernetes.io/zone: {}
        f:spec:
          .: {}
          f:disruption:
            .: {}
            f:budgets: {}
            f:consolidateAfter: {}
            f:consolidationPolicy: {}
          f:limits:
            .: {}
            f:cpu: {}
          f:template:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:k8s.io/cluster-autoscaler-enabled: {}
              f:labels:
                .: {}
                f:AvailabilityZone: {}
                f:Environment: {}
                f:PoddedCluster: {}
                f:Release: {}
                f:SfnKubernetes: {}
                f:StackName: {}
                f:compute.zende.sk/ebs-initial-burst: {}
                f:compute.zende.sk/kube-proxy-mode: {}
                f:compute.zendesk.com/ec2_image_sha: {}
                f:k8s.amazonaws.com/eniConfig: {}
                f:managed-by: {}
                f:node-role.kubernetes.compute.zende.sk/node: {}
                f:node-type: {}
                f:node.kubernetes.io/role: {}
                f:nodegroup-operator.compute.zende.sk/cluster-name: {}
                f:nodegroup-operator.compute.zende.sk/nodegroup-name: {}
                f:nodegroup-provider: {}
                f:nodegroupDeployment: {}
                f:ownedby: {}
                f:product: {}
                f:project: {}
                f:provider: {}
                f:service: {}
                f:team: {}
            f:spec:
              .: {}
              f:expireAfter: {}
              f:nodeClassRef:
                .: {}
                f:group: {}
                f:kind: {}
                f:name: {}
              f:requirements: {}
              f:startupTaints: {}
              f:terminationGracePeriod: {}
          f:weight: {}
      manager: nodegroup-operator
      operation: Update
      time: "2025-12-23T01:49:25Z"
    - apiVersion: karpenter.sh/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          .: {}
          f:conditions: {}
          f:nodeClassObservedGeneration: {}
          f:nodes: {}
          f:resources:
            .: {}
            f:cpu: {}
            f:ephemeral-storage: {}
            f:hugepages-1Gi: {}
            f:hugepages-2Mi: {}
            f:memory: {}
            f:nodes: {}
            f:pods: {}
            f:vpc.amazonaws.com/pod-eni: {}
      manager: karpenter
      operation: Update
      subresource: status
      time: "2026-01-28T02:36:17Z"
    name: node-usw2c-twg9f-0
    resourceVersion: "3427154465"
    uid: d1f2a780-9351-4b41-aa62-c3a049e3d0a6
  spec:
    disruption:
      budgets:
      - nodes: 30%
        reasons:
        - Underutilized
      - nodes: 100%
        reasons:
        - Empty
      consolidateAfter: 10m
      consolidationPolicy: WhenEmptyOrUnderutilized
    limits:
      cpu: "80"
    template:
      metadata:
        annotations:
          k8s.io/cluster-autoscaler-enabled: "false"
        labels:
          AvailabilityZone: us-west-2c
          Environment: staging
          PoddedCluster: "false"
          Release: v2573
          SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
          StackName: k8s-node
          compute.zende.sk/ebs-initial-burst: enabled
          compute.zende.sk/kube-proxy-mode: daemonset
          compute.zendesk.com/ec2_image_sha: "04661458"
          k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
          managed-by: karpenter
          node-role.kubernetes.compute.zende.sk/node: "true"
          node-type: node
          node.kubernetes.io/role: node
          nodegroup-operator.compute.zende.sk/cluster-name: sandbox
          nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
          nodegroup-provider: karpenter
          nodegroupDeployment: node-usw2c
          ownedby: compute
          product: foundation
          project: sfn_kubernetes
          provider: sfn
          service: kubernetes
          team: compute
      spec:
        expireAfter: Never
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: node-usw2c-twg9f
        requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - reserved
          - on-demand
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-west-2c
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
          - c5.2xlarge
          - c6i.2xlarge
          - c7i.2xlarge
          - m5.2xlarge
          - m5d.2xlarge
          - m6i.2xlarge
          - m6id.2xlarge
          - m7i-flex.2xlarge
          - m7i.2xlarge
          - r5.2xlarge
          - r6i.2xlarge
          - r7i.2xlarge
        startupTaints:
        - effect: NoSchedule
          key: kube-node-monitor
          value: notready
        terminationGracePeriod: 1h0m0s
    weight: 10
  status:
    conditions:
    - lastTransitionTime: "2025-12-22T18:41:28Z"
      message: ""
      observedGeneration: 93
      reason: ValidationSucceeded
      status: "True"
      type: ValidationSucceeded
    - lastTransitionTime: "2025-12-22T18:41:30Z"
      message: ""
      observedGeneration: 93
      reason: NodeClassReady
      status: "True"
      type: NodeClassReady
    - lastTransitionTime: "2025-12-23T04:41:44Z"
      message: ""
      observedGeneration: 93
      reason: NodeRegistrationHealthy
      status: "True"
      type: NodeRegistrationHealthy
    - lastTransitionTime: "2025-12-23T04:34:18Z"
      message: ""
      observedGeneration: 93
      reason: Ready
      status: "True"
      type: Ready
    nodeClassObservedGeneration: 1
    nodes: 5
    resources:
      cpu: "40"
      ephemeral-storage: 1015282800Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 80408248Ki
      nodes: "5"
      pods: "550"
      vpc.amazonaws.com/pod-eni: "0"
nodes:
- allocatable:
    cpu: 31600m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 245914052Ki
    pods: "250"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-spot
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: r6i.8xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: spot
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "true"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "false"
    converged: "true"
    ec2-instance-id: i-09203d47c303c0b85
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-spot-usw2a-cschs
    karpenter.k8s.aws/ec2nodeclass: node-spot-usw2a-cschs
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: r
    karpenter.k8s.aws/instance-cpu: "32"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: r6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "262144"
    karpenter.k8s.aws/instance-network-bandwidth: "12500"
    karpenter.k8s.aws/instance-size: 8xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: spot
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-spot-usw2a-cschs-0
    karpenter.sh/registered: "true"
    kube-node-monitor.zendesk.com/termination-state: delete-me
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-224-12.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-spot: "true"
    node-role.kubernetes.io/node-spot: node-spot
    node-type: node-spot
    node.kubernetes.io/instance-type: r6i.8xlarge
    node.kubernetes.io/role: node-spot
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2a-cschs
    nodegroup-provider: karpenter
    nodegroupDeployment: node-spot-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-224-12.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-09203d47c303c0b85
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-spot
  - effect: NoSchedule
    key: node.kubernetes.io/unschedulable
    timeAdded: "2026-01-28T03:42:02Z"
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    timeAdded: "2026-01-28T03:44:52Z"
  - effect: NoSchedule
    key: node.kubernetes.io/unreachable
    timeAdded: "2026-01-28T03:44:53Z"
- allocatable:
    cpu: 3670m
    ephemeral-storage: 211121968Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11941988Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2589
    SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
    StackName: k8s-node-nvidia-gpu
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: g4dn.xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/drain-blocked: "true"
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/instance-gpu-manufacturer: dra
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zende.sk/termination-sensitive: "true"
    compute.zende.sk/termination-sensitive-node: "true"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-07582b05c1d7c23a6
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    karpenter.k8s.aws/ec2nodeclass: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: g
    karpenter.k8s.aws/instance-cpu: "4"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "3500"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: g4dn
    karpenter.k8s.aws/instance-generation: "4"
    karpenter.k8s.aws/instance-gpu-count: "1"
    karpenter.k8s.aws/instance-gpu-manufacturer: nvidia
    karpenter.k8s.aws/instance-gpu-memory: "16384"
    karpenter.k8s.aws/instance-gpu-name: t4
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-local-nvme: "125"
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "5000"
    karpenter.k8s.aws/instance-size: xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/do-not-sync-taints: "true"
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-224-171.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
    node-role.kubernetes.io/node-nvidia-gpu: node-nvidia-gpu
    node-type: node-nvidia-gpu
    node.kubernetes.io/instance-type: g4dn.xlarge
    node.kubernetes.io/role: node-nvidia-gpu
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    nodegroup-provider: karpenter
    nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-224-171.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-07582b05c1d7c23a6
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-nvidia-gpu
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11785972Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c5.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0b5fecdbd22bdd3e3
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-usw2a-z647p
    karpenter.k8s.aws/ec2nodeclass: node-usw2a-z647p
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3400"
    karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: c5
    karpenter.k8s.aws/instance-generation: "5"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "2500"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2a-z647p-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-224-75.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c5.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-224-75.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-0b5fecdbd22bdd3e3
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11785968Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c5.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0c394e6fb29b43132
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-usw2a-z647p
    karpenter.k8s.aws/ec2nodeclass: node-usw2a-z647p
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3400"
    karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: c5
    karpenter.k8s.aws/instance-generation: "5"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "2500"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2a-z647p-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-225-125.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c5.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-225-125.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-0c394e6fb29b43132
- allocatable:
    cpu: 3670m
    ephemeral-storage: 211121968Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11941984Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2589
    SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
    StackName: k8s-node-nvidia-gpu
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: g4dn.xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/instance-gpu-manufacturer: dra
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-02ef8ae4559f58e80
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    karpenter.k8s.aws/ec2nodeclass: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: g
    karpenter.k8s.aws/instance-cpu: "4"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "3500"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: g4dn
    karpenter.k8s.aws/instance-generation: "4"
    karpenter.k8s.aws/instance-gpu-count: "1"
    karpenter.k8s.aws/instance-gpu-manufacturer: nvidia
    karpenter.k8s.aws/instance-gpu-memory: "16384"
    karpenter.k8s.aws/instance-gpu-name: t4
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-local-nvme: "125"
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "5000"
    karpenter.k8s.aws/instance-size: xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/do-not-sync-taints: "true"
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-225-140.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
    node-role.kubernetes.io/node-nvidia-gpu: node-nvidia-gpu
    node-type: node-nvidia-gpu
    node.kubernetes.io/instance-type: g4dn.xlarge
    node.kubernetes.io/role: node-nvidia-gpu
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    nodegroup-provider: karpenter
    nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-225-140.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-02ef8ae4559f58e80
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-nvidia-gpu
- allocatable:
    cpu: 3670m
    ephemeral-storage: 211121968Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11941984Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2589
    SfnKubernetes: 7619591bc433d7dc286332e1d8aea7c9df2420b7
    StackName: k8s-node-nvidia-gpu
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: g4dn.xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/drain-blocked: "true"
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/instance-gpu-manufacturer: dra
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zende.sk/termination-sensitive: "true"
    compute.zende.sk/termination-sensitive-node: "true"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0b14e96587341e0d7
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    karpenter.k8s.aws/ec2nodeclass: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: g
    karpenter.k8s.aws/instance-cpu: "4"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "3500"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: g4dn
    karpenter.k8s.aws/instance-generation: "4"
    karpenter.k8s.aws/instance-gpu-count: "1"
    karpenter.k8s.aws/instance-gpu-manufacturer: nvidia
    karpenter.k8s.aws/instance-gpu-memory: "16384"
    karpenter.k8s.aws/instance-gpu-name: t4
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-local-nvme: "125"
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "5000"
    karpenter.k8s.aws/instance-size: xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/do-not-sync-taints: "true"
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-nvidia-gpu-rajeesh-usw2a-8ck9g-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-225-151.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-nvidia-gpu: "true"
    node-role.kubernetes.io/node-nvidia-gpu: node-nvidia-gpu
    node-type: node-nvidia-gpu
    node.kubernetes.io/instance-type: g4dn.xlarge
    node.kubernetes.io/role: node-nvidia-gpu
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-nvidia-gpu-rajeesh-usw2a-8ck9g
    nodegroup-provider: karpenter
    nodegroupDeployment: node-nvidia-gpu-rajeesh-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-225-151.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-0b14e96587341e0d7
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-nvidia-gpu
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200937648Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    hugepages-32Mi: "0"
    hugepages-64Ki: "0"
    memory: 26914140Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-arm
    beta.kubernetes.io/arch: arm64
    beta.kubernetes.io/instance-type: m6g.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0be8cb19d8c2b2a0f
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-arm-usw2a-thmxj
    karpenter.k8s.aws/ec2nodeclass: node-arm-usw2a-thmxj
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: m
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: aws
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: m6g
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "32768"
    karpenter.k8s.aws/instance-network-bandwidth: "2500"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-arm-usw2a-thmxj-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: arm64
    kubernetes.io/hostname: ip-172-30-225-219.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-arm: "true"
    node-role.kubernetes.io/node-arm: node-arm
    node-type: node-arm
    node.kubernetes.io/instance-type: m6g.2xlarge
    node.kubernetes.io/role: node-arm
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2a-thmxj
    nodegroup-provider: karpenter
    nodegroupDeployment: node-arm-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-225-219.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-0be8cb19d8c2b2a0f
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-arm
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 27095772Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2a
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: m7i-flex.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-010877a189d01d91a
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2a
    k8s.amazonaws.com/eniConfig: node-usw2a-z647p
    karpenter.k8s.aws/ec2nodeclass: node-usw2a-z647p
    karpenter.k8s.aws/instance-capability-flex: "true"
    karpenter.k8s.aws/instance-category: m
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3200"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: m7i-flex
    karpenter.k8s.aws/instance-generation: "7"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "32768"
    karpenter.k8s.aws/instance-network-bandwidth: "1562"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2a-z647p-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-225-88.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: m7i-flex.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2a-z647p
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2a
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2a
    topology.k8s.aws/zone-id: usw2-az1
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2a
  name: ip-172-30-225-88.us-west-2.compute.internal
  providerID: aws:///us-west-2a/i-010877a189d01d91a
- allocatable:
    cpu: 63520m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 118718844Ki
    pods: "250"
  labels:
    AvailabilityZone: us-west-2b
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-spot
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c7i.16xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: spot
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-022ad1b23bc585a34
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2b
    k8s.amazonaws.com/eniConfig: node-spot-usw2b-xv8zl
    karpenter.k8s.aws/ec2nodeclass: node-spot-usw2b-xv8zl
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "64"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3200"
    karpenter.k8s.aws/instance-ebs-bandwidth: "20000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c7i
    karpenter.k8s.aws/instance-generation: "7"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "131072"
    karpenter.k8s.aws/instance-network-bandwidth: "25000"
    karpenter.k8s.aws/instance-size: 16xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: spot
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-spot-usw2b-xv8zl-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-226-252.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-spot: "true"
    node-role.kubernetes.io/node-spot: node-spot
    node-type: node-spot
    node.kubernetes.io/instance-type: c7i.16xlarge
    node.kubernetes.io/role: node-spot
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2b-xv8zl
    nodegroup-provider: karpenter
    nodegroupDeployment: node-spot-usw2b
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2b
    topology.k8s.aws/zone-id: usw2-az2
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2b
  name: ip-172-30-226-252.us-west-2.compute.internal
  providerID: aws:///us-west-2b/i-022ad1b23bc585a34
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-spot
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864820Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2b
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-00dfd5a02430664e8
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2b
    k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
    karpenter.k8s.aws/ec2nodeclass: node-usw2b-n7st7
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2b-n7st7-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-226-5.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2b
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2b
    topology.k8s.aws/zone-id: usw2-az2
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2b
  name: ip-172-30-226-5.us-west-2.compute.internal
  providerID: aws:///us-west-2b/i-00dfd5a02430664e8
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864816Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2b
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-09517c0dfc55bfd1e
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2b
    k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
    karpenter.k8s.aws/ec2nodeclass: node-usw2b-n7st7
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2b-n7st7-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-226-86.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2b
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2b
    topology.k8s.aws/zone-id: usw2-az2
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2b
  name: ip-172-30-226-86.us-west-2.compute.internal
  providerID: aws:///us-west-2b/i-09517c0dfc55bfd1e
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200937648Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    hugepages-32Mi: "0"
    hugepages-64Ki: "0"
    memory: 26914148Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2b
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-arm
    beta.kubernetes.io/arch: arm64
    beta.kubernetes.io/instance-type: m6g.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-064c2e3e5ae6fa9ad
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2b
    k8s.amazonaws.com/eniConfig: node-arm-usw2b-xhg9c
    karpenter.k8s.aws/ec2nodeclass: node-arm-usw2b-xhg9c
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: m
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: aws
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: m6g
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "32768"
    karpenter.k8s.aws/instance-network-bandwidth: "2500"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-arm-usw2b-xhg9c-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: arm64
    kubernetes.io/hostname: ip-172-30-227-233.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-arm: "true"
    node-role.kubernetes.io/node-arm: node-arm
    node-type: node-arm
    node.kubernetes.io/instance-type: m6g.2xlarge
    node.kubernetes.io/role: node-arm
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2b-xhg9c
    nodegroup-provider: karpenter
    nodegroupDeployment: node-arm-usw2b
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2b
    topology.k8s.aws/zone-id: usw2-az2
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2b
  name: ip-172-30-227-233.us-west-2.compute.internal
  providerID: aws:///us-west-2b/i-064c2e3e5ae6fa9ad
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-arm
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 27095772Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2b
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: m7i-flex.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0af7d5e8a274b2b88
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2b
    k8s.amazonaws.com/eniConfig: node-usw2b-n7st7
    karpenter.k8s.aws/ec2nodeclass: node-usw2b-n7st7
    karpenter.k8s.aws/instance-capability-flex: "true"
    karpenter.k8s.aws/instance-category: m
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3200"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: m7i-flex
    karpenter.k8s.aws/instance-generation: "7"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "32768"
    karpenter.k8s.aws/instance-network-bandwidth: "1562"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2b-n7st7-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-227-44.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: m7i-flex.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2b-n7st7
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2b
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2b
    topology.k8s.aws/zone-id: usw2-az2
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2b
  name: ip-172-30-227-44.us-west-2.compute.internal
  providerID: aws:///us-west-2b/i-0af7d5e8a274b2b88
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864820Ki
    pods: "110"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2573
    SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: "04661458"
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0f8e0c5e9e6e06171
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
    karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2c-twg9f-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-228-184.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v737
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-228-184.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-0f8e0c5e9e6e06171
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200937648Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    hugepages-32Mi: "0"
    hugepages-64Ki: "0"
    memory: 26914140Ki
    pods: "160"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-arm
    beta.kubernetes.io/arch: arm64
    beta.kubernetes.io/instance-type: m6g.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-0a58c862ae9a8aadd
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-arm-usw2c-rpkp2
    karpenter.k8s.aws/ec2nodeclass: node-arm-usw2c-rpkp2
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: m
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: aws
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "2500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "4750"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: m6g
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "32768"
    karpenter.k8s.aws/instance-network-bandwidth: "2500"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-arm-usw2c-rpkp2-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: arm64
    kubernetes.io/hostname: ip-172-30-228-222.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-arm: "true"
    node-role.kubernetes.io/node-arm: node-arm
    node-type: node-arm
    node.kubernetes.io/instance-type: m6g.2xlarge
    node.kubernetes.io/role: node-arm
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-arm-usw2c-rpkp2
    nodegroup-provider: karpenter
    nodegroupDeployment: node-arm-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-228-222.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-0a58c862ae9a8aadd
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-arm
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864816Ki
    pods: "110"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2573
    SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: "04661458"
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-04e3afe8284718809
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
    karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2c-twg9f-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-228-48.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v737
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-228-48.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-04e3afe8284718809
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864812Ki
    pods: "110"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2573
    SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: "04661458"
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-06bfb6b4b3967a8e2
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
    karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2c-twg9f-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-229-136.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v737
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-229-136.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-06bfb6b4b3967a8e2
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864820Ki
    pods: "110"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2573
    SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: "04661458"
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-00c5b7259f4923f4f
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
    karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2c-twg9f-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-229-157.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v737
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-229-157.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-00c5b7259f4923f4f
- allocatable:
    cpu: 7660m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 11864820Ki
    pods: "110"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2573
    SfnKubernetes: fd5bb85d24b0bb5e946f3633beee2c50c4e9c925
    StackName: k8s-node
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c6i.2xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: on-demand
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: "04661458"
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-00d9ece9adca2e6a7
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-usw2c-twg9f
    karpenter.k8s.aws/ec2nodeclass: node-usw2c-twg9f
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "8"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3500"
    karpenter.k8s.aws/instance-ebs-bandwidth: "10000"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "true"
    karpenter.k8s.aws/instance-family: c6i
    karpenter.k8s.aws/instance-generation: "6"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "16384"
    karpenter.k8s.aws/instance-network-bandwidth: "3125"
    karpenter.k8s.aws/instance-size: 2xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: on-demand
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-usw2c-twg9f-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-229-182.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node: "true"
    node-role.kubernetes.io/node: node
    node-type: node
    node.kubernetes.io/instance-type: c6i.2xlarge
    node.kubernetes.io/role: node
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v737
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-usw2c-twg9f
    nodegroup-provider: karpenter
    nodegroupDeployment: node-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-229-182.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-00d9ece9adca2e6a7
- allocatable:
    cpu: 47560m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 87675724Ki
    pods: "250"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-spot
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c5.12xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: spot
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-092f11a650f55f796
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
    karpenter.k8s.aws/ec2nodeclass: node-spot-usw2c-ltnzv
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "48"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3600"
    karpenter.k8s.aws/instance-ebs-bandwidth: "9500"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: c5
    karpenter.k8s.aws/instance-generation: "5"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "98304"
    karpenter.k8s.aws/instance-network-bandwidth: "12000"
    karpenter.k8s.aws/instance-size: 12xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: spot
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-spot-usw2c-ltnzv-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-229-54.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-spot: "true"
    node-role.kubernetes.io/node-spot: node-spot
    node-type: node-spot
    node.kubernetes.io/instance-type: c5.12xlarge
    node.kubernetes.io/role: node-spot
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
    nodegroup-provider: karpenter
    nodegroupDeployment: node-spot-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-229-54.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-092f11a650f55f796
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-spot
- allocatable:
    cpu: 47560m
    ephemeral-storage: 200959408Ki
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    memory: 87675732Ki
    pods: "250"
  labels:
    AvailabilityZone: us-west-2c
    Environment: staging
    PoddedCluster: "false"
    Release: v2590
    SfnKubernetes: e94a235199dde75b7f68bb0075a0a7a91367606f
    StackName: k8s-node-spot
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/instance-type: c5.12xlarge
    beta.kubernetes.io/os: linux
    compute.zende.sk/ebs-initial-burst: enabled
    compute.zende.sk/ec2-lifecycle: spot
    compute.zende.sk/kernel-release: 6.8.0-1044-aws
    compute.zende.sk/kube-proxy-mode: daemonset
    compute.zende.sk/node-problem-detected: "false"
    compute.zende.sk/os-description: ubuntu-22.04.5-lts
    compute.zende.sk/os-id: ubuntu
    compute.zende.sk/os-release: "22.04"
    compute.zendesk.com/ec2_image_sha: b7f462ec
    convergeable: "true"
    converged: "true"
    ec2-instance-id: i-06745aa58d1ddcc0c
    failure-domain.beta.kubernetes.io/region: us-west-2
    failure-domain.beta.kubernetes.io/zone: us-west-2c
    k8s.amazonaws.com/eniConfig: node-spot-usw2c-ltnzv
    karpenter.k8s.aws/ec2nodeclass: node-spot-usw2c-ltnzv
    karpenter.k8s.aws/instance-capability-flex: "false"
    karpenter.k8s.aws/instance-category: c
    karpenter.k8s.aws/instance-cpu: "48"
    karpenter.k8s.aws/instance-cpu-manufacturer: intel
    karpenter.k8s.aws/instance-cpu-sustained-clock-speed-mhz: "3600"
    karpenter.k8s.aws/instance-ebs-bandwidth: "9500"
    karpenter.k8s.aws/instance-encryption-in-transit-supported: "false"
    karpenter.k8s.aws/instance-family: c5
    karpenter.k8s.aws/instance-generation: "5"
    karpenter.k8s.aws/instance-hypervisor: nitro
    karpenter.k8s.aws/instance-memory: "98304"
    karpenter.k8s.aws/instance-network-bandwidth: "12000"
    karpenter.k8s.aws/instance-size: 12xlarge
    karpenter.k8s.aws/instance-tenancy: default
    karpenter.sh/capacity-type: spot
    karpenter.sh/initialized: "true"
    karpenter.sh/nodepool: node-spot-usw2c-ltnzv-0
    karpenter.sh/registered: "true"
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: ip-172-30-229-7.us-west-2.compute.internal
    kubernetes.io/os: linux
    managed-by: karpenter
    node-role.kubernetes.compute.zende.sk/node-spot: "true"
    node-role.kubernetes.io/node-spot: node-spot
    node-type: node-spot
    node.kubernetes.io/instance-type: c5.12xlarge
    node.kubernetes.io/role: node-spot
    nodegroup-operator.compute.zende.sk/cluster-name: sandbox
    nodegroup-operator.compute.zende.sk/controller-version: v740
    nodegroup-operator.compute.zende.sk/nodegroup-name: node-spot-usw2c-ltnzv
    nodegroup-provider: karpenter
    nodegroupDeployment: node-spot-usw2c
    ownedby: compute
    product: foundation
    project: sfn_kubernetes
    provider: sfn
    service: kubernetes
    stack-name: ""
    team: compute
    topology.ebs.csi.aws.com/zone: us-west-2c
    topology.k8s.aws/zone-id: usw2-az3
    topology.kubernetes.io/region: us-west-2
    topology.kubernetes.io/zone: us-west-2c
  name: ip-172-30-229-7.us-west-2.compute.internal
  providerID: aws:///us-west-2c/i-06745aa58d1ddcc0c
  taints:
  - effect: NoSchedule
    key: compute.zende.sk/nodegroup
    value: node-spot
pods:
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: argocd-application-controller
      apps.kubernetes.io/pod-index: "0"
      branch: dreuss-sandbox-cluster
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: argocd-application-controller-65f76c5c
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K4WDNEQRDQFS8J4PW9YXPGFH
      deploy_phase: ""
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: engineering-productivity
      project: argocd-deploy
      revision: c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      role: argocd-deploy
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: argocd-deploy
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: argocd-application-controller-0
      tag: dreuss-sandbox-cluster
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: argocd-deploy
      tags.datadoghq.com/version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      team: deploy-platform
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01K4WDNEQRDQFS8J4PW9YXPGFH
      version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
    name: argocd-application-controller-0
    namespace: argocd-deploy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: argocd-application-controller
      uid: 7793329e-cc9a-4512-85b9-734a0f4c7013
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: argocd-application-controller
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/part-of: argocd
            topologyKey: kubernetes.io/hostname
          weight: 5
    containers:
    - name: argocd-application-controller
      resources:
        limits:
          cpu: "2"
          memory: 5G
        requests:
          cpu: 511m
          memory: 2G
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - maxSkew: 1
      nodeTaintsPolicy: Ignore
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:39:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:39:13Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: argocd-applicationset-controller
      branch: dreuss-sandbox-cluster
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K4WDNEQRDQFS8J4PW9YXPGFH
      deploy_phase: ""
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7484d784fb
      product: engineering-productivity
      project: argocd-deploy
      revision: c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      role: argocd-deploy
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: argocd-deploy
      sidecar.istio.io/inject: "false"
      tag: dreuss-sandbox-cluster
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: argocd-deploy
      tags.datadoghq.com/version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      team: deploy-platform
      topology.kubernetes.io/zone: us-west-2b
      track: 01K4WDNEQRDQFS8J4PW9YXPGFH
      version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
    name: argocd-applicationset-controller-7484d784fb-lqhwf
    namespace: argocd-deploy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: argocd-applicationset-controller-7484d784fb
      uid: 7bcaed03-5e33-40e5-98a3-baa9119425c3
  spec:
    containers:
    - name: argocd-applicationset-controller
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7484d784fb
        matchLabels:
          app.kubernetes.io/name: argocd-applicationset-controller
          project: argocd-deploy
          role: argocd-deploy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: argocd-commit-server
      branch: dreuss-sandbox-cluster
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K4WDNEQRDQFS8J4PW9YXPGFH
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 796798bf68
      product: engineering-productivity
      project: argocd-deploy
      revision: c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      role: argocd-deploy
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: argocd-deploy
      sidecar.istio.io/inject: "false"
      tag: dreuss-sandbox-cluster
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: argocd-deploy
      tags.datadoghq.com/version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      team: deploy-platform
      topology.kubernetes.io/zone: us-west-2a
      track: 01K4WDNEQRDQFS8J4PW9YXPGFH
      version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
    name: argocd-commit-server-796798bf68-t5q7v
    namespace: argocd-deploy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: argocd-commit-server-796798bf68
      uid: d90730f6-9b4a-493d-8b4b-bf02aef2e0da
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: argocd-commit-server
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/part-of: argocd
            topologyKey: kubernetes.io/hostname
          weight: 5
    containers:
    - name: argocd-commit-server
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 796798bf68
        matchLabels:
          app.kubernetes.io/name: argocd-commit-server
          project: argocd-deploy
          role: argocd-deploy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T03:20:21Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T03:19:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: argocd-redis
      branch: dreuss-sandbox-cluster
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K4WDNEQRDQFS8J4PW9YXPGFH
      deploy_phase: ""
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 578f77798c
      product: engineering-productivity
      project: argocd-deploy
      revision: c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      role: argocd-redis
      service: argocd-deploy
      sidecar.istio.io/inject: "false"
      tag: dreuss-sandbox-cluster
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: argocd-deploy-argocd-redis
      tags.datadoghq.com/version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      team: deploy-platform
      topology.kubernetes.io/zone: us-west-2c
      track: 01K4WDNEQRDQFS8J4PW9YXPGFH
      version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
    name: argocd-redis-578f77798c-cs8fq
    namespace: argocd-deploy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: argocd-redis-578f77798c
      uid: 34c61d7d-6d22-47f5-b59b-9c1199722452
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: argocd-redis
            topologyKey: kubernetes.io/hostname
          weight: 100
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/part-of: argocd
            topologyKey: kubernetes.io/hostname
          weight: 5
    containers:
    - name: redis
      resources:
        limits:
          cpu: "2"
          memory: 3G
        requests:
          cpu: 11m
          memory: 2G
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 578f77798c
        matchLabels:
          app.kubernetes.io/name: argocd-redis
          project: argocd-deploy
          role: argocd-redis
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: argocd-repo-server
      branch: dreuss-sandbox-cluster
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K4WDNEQRDQFS8J4PW9YXPGFH
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7bd494649d
      product: engineering-productivity
      project: argocd-deploy
      revision: c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      role: argocd-deploy
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: argocd-deploy
      sidecar.istio.io/inject: "false"
      tag: dreuss-sandbox-cluster
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: argocd-deploy
      tags.datadoghq.com/version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      team: deploy-platform
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01K4WDNEQRDQFS8J4PW9YXPGFH
      version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
    name: argocd-repo-server-7bd494649d-bjv9j
    namespace: argocd-deploy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: argocd-repo-server-7bd494649d
      uid: c9c3f59f-26d3-4105-af65-413f4b317c9a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: argocd-repo-server
            topologyKey: topology.kubernetes.io/zone
          weight: 100
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: argocd-repo-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: argocd-repo-server
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    - name: sed-plugin
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    - name: lets-plugin
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    - name: init-git-repo
      resources:
        limits:
          cpu: "2"
          memory: 128M
        requests:
          cpu: 100m
          memory: 128M
    - name: copyutil
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7bd494649d
        matchLabels:
          app.kubernetes.io/name: argocd-repo-server
          project: argocd-deploy
          role: argocd-deploy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:11:57Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:11:23Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: argocd-repo-server
      branch: dreuss-sandbox-cluster
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K4WDNEQRDQFS8J4PW9YXPGFH
      deploy_phase: ""
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 7bd494649d
      product: engineering-productivity
      project: argocd-deploy
      revision: c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      role: argocd-deploy
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: argocd-deploy
      sidecar.istio.io/inject: "false"
      tag: dreuss-sandbox-cluster
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: argocd-deploy
      tags.datadoghq.com/version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
      team: deploy-platform
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01K4WDNEQRDQFS8J4PW9YXPGFH
      version: dreuss-sandbox-cluster-c8e6d0680472ade38bda90cfd612bc45dd02d8e6
    name: argocd-repo-server-7bd494649d-k72l7
    namespace: argocd-deploy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: argocd-repo-server-7bd494649d
      uid: c9c3f59f-26d3-4105-af65-413f4b317c9a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: argocd-repo-server
            topologyKey: topology.kubernetes.io/zone
          weight: 100
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: argocd-repo-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: argocd-repo-server
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    - name: sed-plugin
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    - name: lets-plugin
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    - name: init-git-repo
      resources:
        limits:
          cpu: "2"
          memory: 128M
        requests:
          cpu: 100m
          memory: 128M
    - name: copyutil
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7bd494649d
        matchLabels:
          app.kubernetes.io/name: argocd-repo-server
          project: argocd-deploy
          role: argocd-deploy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 556ff665cf
      topology.kubernetes.io/zone: us-west-2b
    name: cert-manager-556ff665cf-wcjmb
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-556ff665cf
      uid: 8db4a939-5c68-4566-9e59-fc2a3c8eb8be
  spec:
    containers:
    - name: cert-manager
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 556ff665cf
        matchLabels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cert-manager
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:19Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 5bfdffddcd
      topology.kubernetes.io/zone: us-west-2b
    name: cert-manager-cainjector-5bfdffddcd-pvqgr
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-5bfdffddcd
      uid: d9f8515c-cc5f-4532-bb16-ece1de211d1a
  spec:
    containers:
    - name: cert-manager
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5bfdffddcd
        matchLabels:
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: cainjector
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:20Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 667454888c
      topology.kubernetes.io/zone: us-west-2b
    name: cert-manager-webhook-667454888c-6t7lr
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-667454888c
      uid: 92f04ce9-20c9-46b3-afdb-e9df47e3ae38
  spec:
    containers:
    - name: cert-manager
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 667454888c
        matchLabels:
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/name: webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: controller-manager
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 684ccb6bb6
      topology.kubernetes.io/zone: us-west-2b
    name: chaos-controller-manager-684ccb6bb6-42242
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: chaos-controller-manager-684ccb6bb6
      uid: 7229980d-56c7-4203-9e7c-a4f6d4c42eb8
  spec:
    containers:
    - name: chaos-mesh
      resources:
        requests:
          cpu: 11m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 684ccb6bb6
        matchLabels:
          app.kubernetes.io/component: controller-manager
          app.kubernetes.io/instance: chaos-mesh
          app.kubernetes.io/name: chaos-mesh
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: controller-manager
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 684ccb6bb6
      topology.kubernetes.io/zone: us-west-2a
    name: chaos-controller-manager-684ccb6bb6-k9r6j
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: chaos-controller-manager-684ccb6bb6
      uid: 7229980d-56c7-4203-9e7c-a4f6d4c42eb8
  spec:
    containers:
    - name: chaos-mesh
      resources:
        requests:
          cpu: 11m
          memory: 256Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 684ccb6bb6
        matchLabels:
          app.kubernetes.io/component: controller-manager
          app.kubernetes.io/instance: chaos-mesh
          app.kubernetes.io/name: chaos-mesh
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: controller-manager
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 684ccb6bb6
      topology.kubernetes.io/zone: us-west-2c
    name: chaos-controller-manager-684ccb6bb6-x57sx
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: chaos-controller-manager-684ccb6bb6
      uid: 7229980d-56c7-4203-9e7c-a4f6d4c42eb8
  spec:
    containers:
    - name: chaos-mesh
      resources:
        requests:
          cpu: 11m
          memory: 256Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 684ccb6bb6
        matchLabels:
          app.kubernetes.io/component: controller-manager
          app.kubernetes.io/instance: chaos-mesh
          app.kubernetes.io/name: chaos-mesh
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2c
    name: chaos-daemon-7mb6v
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-48.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:24Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2a
    name: chaos-daemon-9tlgx
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-75.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:11:14Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:11:02Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2c
    name: chaos-daemon-f4scl
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-182.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2b
    name: chaos-daemon-h97c9
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-5.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2a
    name: chaos-daemon-hfzkx
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-125.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:22:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:57Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2b
    name: chaos-daemon-hhrqr
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-86.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:24Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2c
    name: chaos-daemon-jxpmx
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-157.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2a
    name: chaos-daemon-pq568
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-88.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2b
    name: chaos-daemon-tb6jf
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-44.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2c
    name: chaos-daemon-wc2bz
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-136.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:50Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:40Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-daemon
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 546c76459b
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      topology.kubernetes.io/zone: us-west-2c
    name: chaos-daemon-z4xv2
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: chaos-daemon
      uid: 50768f1e-77b7-4a11-b446-89a68b0eeff6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-184.us-west-2.compute.internal
    containers:
    - name: chaos-daemon
      resources: {}
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:43Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-dashboard
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 548db78966
      topology.kubernetes.io/zone: us-west-2b
    name: chaos-dashboard-548db78966-6qgmz
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: chaos-dashboard-548db78966
      uid: 32fc139e-d3ad-4bba-9b1b-cf9f20d77088
  spec:
    containers:
    - name: chaos-dashboard
      resources:
        requests:
          cpu: 11m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 548db78966
        matchLabels:
          app.kubernetes.io/component: chaos-dashboard
          app.kubernetes.io/instance: chaos-mesh
          app.kubernetes.io/name: chaos-mesh
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: chaos-dns-server
      app.kubernetes.io/instance: chaos-mesh
      app.kubernetes.io/name: chaos-mesh
      app.kubernetes.io/part-of: chaos-mesh
      app.kubernetes.io/version: 2.7.2
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 5c59f5b78b
      topology.kubernetes.io/zone: us-west-2b
    name: chaos-dns-server-5c59f5b78b-wgxgk
    namespace: chaos-mesh
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: chaos-dns-server-5c59f5b78b
      uid: 37be5fef-c4e5-4d1c-a891-c59166acef81
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - chaos-dns-server
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: chaos-dns-server
      resources:
        requests:
          cpu: 11m
          memory: 70Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5c59f5b78b
        matchLabels:
          app.kubernetes.io/component: chaos-dns-server
          app.kubernetes.io/instance: chaos-mesh
          app.kubernetes.io/name: chaos-mesh
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T22:28:58Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T22:28:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: console
      app.kubernetes.io/instance: console
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: cicd-toolkit-test
      app.kubernetes.io/part-of: engineering-productivity
      app.kubernetes.io/version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      branch: dreuss-test-lets-deploy-monitor
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K60KG44JRJC23JTQF43CHJYD
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 56859774fb
      product: engineering-productivity
      project: cicd-toolkit-test
      revision: ab8125ac67b7e3e08d58a62f1155ccb7d0b7aa03
      role: console
      security.istio.io/tlsMode: istio
      service: cicd-toolkit-test
      service.istio.io/canonical-name: cicd-toolkit-test-console
      service.istio.io/canonical-revision: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      sidecar.istio.io/inject: "true"
      tag: dreuss-test-lets-deploy-monitor
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: cicd-toolkit-test-console
      tags.datadoghq.com/istio-proxy.version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      tags.datadoghq.com/service: cicd-toolkit-test-console
      tags.datadoghq.com/version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      team: deploy-platform
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2a
      track: 01K60KG44JRJC23JTQF43CHJYD
      version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
    name: cicd-toolkit-test-console-56859774fb-pwvc5
    namespace: cicd-toolkit-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cicd-toolkit-test-console-56859774fb
      uid: 49734c56-d7ad-4172-b71a-0e16b8941f96
  spec:
    containers:
    - name: cicd-toolkit-test-console
      resources:
        limits:
          cpu: 20m
          memory: 75Mi
        requests:
          cpu: 10m
          memory: 25Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 56859774fb
        matchLabels:
          project: cicd-toolkit-test
          role: console
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:40:44Z"
      message: 'containers with unready status: [cicd-toolkit-test-console]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:40:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: new-console
      app.kubernetes.io/instance: new-console
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: cicd-toolkit-test
      app.kubernetes.io/part-of: engineering-productivity
      app.kubernetes.io/version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      branch: dreuss-test-lets-deploy-monitor
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K60KG44JRJC23JTQF43CHJYD
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 54f878f65b
      product: engineering-productivity
      project: cicd-toolkit-test
      revision: ab8125ac67b7e3e08d58a62f1155ccb7d0b7aa03
      role: new-console
      security.istio.io/tlsMode: istio
      service: cicd-toolkit-test
      service.istio.io/canonical-name: cicd-toolkit-test-new-console
      service.istio.io/canonical-revision: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      sidecar.istio.io/inject: "true"
      tag: dreuss-test-lets-deploy-monitor
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: cicd-toolkit-test-new-console
      tags.datadoghq.com/istio-proxy.version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      tags.datadoghq.com/service: cicd-toolkit-test-new-console
      tags.datadoghq.com/version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      team: deploy-platform
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2a
      track: 01K60KG44JRJC23JTQF43CHJYD
      version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
    name: cicd-toolkit-test-new-console-54f878f65b-l5ps2
    namespace: cicd-toolkit-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cicd-toolkit-test-new-console-54f878f65b
      uid: 444a2db3-907d-405a-b2b8-3e8f1a977b10
  spec:
    containers:
    - name: cicd-toolkit-test-new-console
      resources:
        limits:
          cpu: 20m
          memory: 75Mi
        requests:
          cpu: 10m
          memory: 25Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54f878f65b
        matchLabels:
          project: cicd-toolkit-test
          role: new-console
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:40:44Z"
      message: 'containers with unready status: [cicd-toolkit-test-new-console]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:40:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: podinfo
      app.kubernetes.io/instance: podinfo
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: cicd-toolkit-test
      app.kubernetes.io/part-of: engineering-productivity
      app.kubernetes.io/version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      branch: dreuss-test-lets-deploy-monitor
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K60KG44JRJC23JTQF43CHJYD
      deploy_phase: ""
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 8588d55558
      product: engineering-productivity
      project: cicd-toolkit-test
      revision: ab8125ac67b7e3e08d58a62f1155ccb7d0b7aa03
      role: podinfo
      security.istio.io/tlsMode: istio
      service: cicd-toolkit-test
      service.istio.io/canonical-name: cicd-toolkit-test-podinfo
      service.istio.io/canonical-revision: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      sidecar.istio.io/inject: "true"
      tag: dreuss-test-lets-deploy-monitor
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: cicd-toolkit-test-podinfo
      tags.datadoghq.com/istio-proxy.version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      tags.datadoghq.com/service: cicd-toolkit-test-podinfo
      tags.datadoghq.com/version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
      team: deploy-platform
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01K60KG44JRJC23JTQF43CHJYD
      version: dreuss-test-lets-deploy-monitor-ab8125ac67b7e3e08d58a62f1155ccb
    name: podinfo-8588d55558-k9zzh
    namespace: cicd-toolkit-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: podinfo-8588d55558
      uid: 5bef316d-b1a0-413c-9b8c-e3709ec5d5a2
  spec:
    containers:
    - name: podinfo
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: "1"
          memory: 512Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8588d55558
        matchLabels:
          project: cicd-toolkit-test
          role: podinfo
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:42:44Z"
      message: 'containers with unready status: [podinfo]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:42:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pkg.crossplane.io/provider: aws-ec2-provider
      pkg.crossplane.io/revision: aws-ec2-provider-a8a9f2ec0d4d
      pod-template-hash: 57c5977d99
      topology.kubernetes.io/zone: us-west-2b
    name: aws-ec2-provider-a8a9f2ec0d4d-57c5977d99-mxxr4
    namespace: compute-crossplane
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: aws-ec2-provider-a8a9f2ec0d4d-57c5977d99
      uid: a4dba479-9faf-47dc-8053-a19e0326b25e
  spec:
    containers:
    - name: package-runtime
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57c5977d99
        matchLabels:
          pkg.crossplane.io/provider: aws-ec2-provider
          pkg.crossplane.io/revision: aws-ec2-provider-a8a9f2ec0d4d
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:01:43Z"
      message: 'containers with unready status: [package-runtime]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:48:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pkg.crossplane.io/provider: aws-efs-provider
      pkg.crossplane.io/revision: aws-efs-provider-25aedc2884da
      pod-template-hash: 7bc87644bd
      topology.kubernetes.io/zone: us-west-2b
    name: aws-efs-provider-25aedc2884da-7bc87644bd-xgtlg
    namespace: compute-crossplane
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: aws-efs-provider-25aedc2884da-7bc87644bd
      uid: fa49328a-40f4-4174-bacd-428d89e50149
  spec:
    containers:
    - name: package-runtime
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7bc87644bd
        matchLabels:
          pkg.crossplane.io/provider: aws-efs-provider
          pkg.crossplane.io/revision: aws-efs-provider-25aedc2884da
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:16:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:15:53Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: crossplane
      app.kubernetes.io/component: crossplane
      app.kubernetes.io/instance: crossplane
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: compute-crossplane
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v3
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KFKCX5SEXH7JSADZXD1W90A9
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 68f8c8b6d7
      product: foundation
      project: compute-crossplane
      release: crossplane
      revision: ced6be5d22a0f8032bd910bb0efe87a846394ef4
      role: crossplane
      service: compute-crossplane
      sidecar.istio.io/inject: "false"
      tag: v3
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: compute-crossplane
      tags.datadoghq.com/version: v3
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFKCX5SEXH7JSADZXD1W90A9
      version: v3
    name: crossplane-68f8c8b6d7-pxpr8
    namespace: compute-crossplane
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: crossplane-68f8c8b6d7
      uid: 25636fda-57a6-4f1a-8545-3b236f936d14
  spec:
    containers:
    - name: crossplane
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 126m
          memory: 256M
    initContainers:
    - name: crossplane-init
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 68f8c8b6d7
        matchLabels:
          app: crossplane
          project: compute-crossplane
          release: crossplane
          role: crossplane
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T08:03:27Z"
      observedGeneration: 75
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T08:03:22Z"
      observedGeneration: 75
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: crossplane-rbac-manager
      app.kubernetes.io/component: crossplane
      app.kubernetes.io/instance: crossplane
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: compute-crossplane
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v3
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KFKCX5SEXH7JSADZXD1W90A9
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 6dfb58b45f
      product: foundation
      project: compute-crossplane
      release: crossplane
      revision: ced6be5d22a0f8032bd910bb0efe87a846394ef4
      role: crossplane
      service: compute-crossplane
      sidecar.istio.io/inject: "false"
      tag: v3
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: compute-crossplane
      tags.datadoghq.com/version: v3
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFKCX5SEXH7JSADZXD1W90A9
      version: v3
    name: crossplane-rbac-manager-6dfb58b45f-ksd8b
    namespace: compute-crossplane
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: crossplane-rbac-manager-6dfb58b45f
      uid: ac5f7b18-ab55-49da-966a-2253afe8ebe6
  spec:
    containers:
    - name: crossplane
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    initContainers:
    - name: crossplane-init
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6dfb58b45f
        matchLabels:
          app: crossplane-rbac-manager
          project: compute-crossplane
          release: crossplane
          role: crossplane
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:06:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:06:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pkg.crossplane.io/provider: upbound-provider-family-aws
      pkg.crossplane.io/revision: upbound-provider-family-aws-a22fe515ffa9
      pod-template-hash: 844c46779d
      topology.kubernetes.io/zone: us-west-2b
    name: upbound-provider-family-aws-a22fe515ffa9-844c46779d-fkjjc
    namespace: compute-crossplane
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: upbound-provider-family-aws-a22fe515ffa9-844c46779d
      uid: 62cec803-64e9-45c7-897c-87ce902463c1
  spec:
    containers:
    - name: package-runtime
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 844c46779d
        matchLabels:
          pkg.crossplane.io/provider: upbound-provider-family-aws
          pkg.crossplane.io/revision: upbound-provider-family-aws-a22fe515ffa9
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:00:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:00:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: controller
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: cron-scaler
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v122
      branch: v122
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/vpa-opt-in: "true"
      control-plane: controller-manager
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KCM2J3RJ1HN8J0WTWPC0KHSR
      deploy_phase: phase-1
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 647bb959bb
      product: foundation
      project: cron-scaler
      revision: e7c0e9eb2137097f28f981aff158fa78b1a9659a
      role: controller
      service: cron-scaler
      sidecar.istio.io/inject: "false"
      tag: v122
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: cron-scaler-controller
      tags.datadoghq.com/version: v122
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KCM2J3RJ1HN8J0WTWPC0KHSR
      version: v122
    name: cron-scaler-controller-manager-647bb959bb-q5hwk
    namespace: cron-scaler-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cron-scaler-controller-manager-647bb959bb
      uid: 1d2b9121-0ded-4488-a33a-02109f77c593
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 647bb959bb
        matchLabels:
          control-plane: controller-manager
          project: cron-scaler
          role: controller
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T21:08:18Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T21:08:05Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: controller
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: cron-scaler
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v122
      branch: v122
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/vpa-opt-in: "true"
      control-plane: controller-manager
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KCM2J3RJ1HN8J0WTWPC0KHSR
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 647bb959bb
      product: foundation
      project: cron-scaler
      revision: e7c0e9eb2137097f28f981aff158fa78b1a9659a
      role: controller
      service: cron-scaler
      sidecar.istio.io/inject: "false"
      tag: v122
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: cron-scaler-controller
      tags.datadoghq.com/version: v122
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KCM2J3RJ1HN8J0WTWPC0KHSR
      version: v122
    name: cron-scaler-controller-manager-647bb959bb-tbzf2
    namespace: cron-scaler-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cron-scaler-controller-manager-647bb959bb
      uid: 1d2b9121-0ded-4488-a33a-02109f77c593
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 647bb959bb
        matchLabels:
          control-plane: controller-manager
          project: cron-scaler
          role: controller
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:13:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:12:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-2nfc5
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-88.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:27:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-47lxs
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-86.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0f04554f198c692f3
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-4cq7x
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-249.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-227-249.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:25Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-4fglk
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:59:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-4k4tx
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-182.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:28:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-4klfr
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:03:55Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-08c30b7d86243ffe6
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-5cq7h
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-160.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-160.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:50:22Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-09604efed6dbf1fba
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-5tfl5
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-47.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-47.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:50:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:36Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-6gvm4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-222.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-6vqgn
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-233.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:58Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-03416309429e0355e
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-77x5t
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-180.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-180.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-083870684c4116622
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-8667m
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-59.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-59.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-8sz4l
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-219.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:20Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-022ad1b23bc585a34
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-9n2w4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-252.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:56Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-031d33af9d554291f
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-b7ljh
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-125.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-125.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:12Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-d8bnw
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-136.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:59:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:23Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-dbqqq
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-75.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:11:24Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:10:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-089f56965bd0d4ef1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-dkb2n
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-207.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-227-207.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:02:18Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-drkzt
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-157.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:13Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-gpqb5
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-184.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:32:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-hx2n7
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-221.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:12Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-kpwvn
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-94.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-02ef8ae4559f58e80
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-kr7fs
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-06745aa58d1ddcc0c
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-ld59f
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-7.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:16:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:15:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-mqff2
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-5.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-ps7cf
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-44.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-09203d47c303c0b85
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-qkx6z
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-12.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:44:53Z"
      observedGeneration: 1
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T00:26:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0cc30e7b1b8984d2d
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-r5r42
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-153.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-227-153.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:00:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-02eb8fb6af2c9b25e
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-tmvgj
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-66.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-227-66.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:48Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-v2tm4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-48.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0e39ec8dde59fddd9
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-vdjzb
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-71.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-226-71.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:57Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0b960eeb84dec48cf
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-wl52h
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-237.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-228-237.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:08:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:57Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-092f11a650f55f796
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-xdh9t
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-54.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: aws-cni
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: v210
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 58c6cbdbc6
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFGYFMTMKWFSJRPADN9TYSX1
      deploy_phase: phase-1
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "505"
      product: foundation
      project: aws-cni
      revision: 27799e0ad5973c3824a26142d125eeeb0aabed66
      role: app-server
      service: aws-cni
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: aws-cni
      tags.datadoghq.com/version: v210
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFGYFMTMKWFSJRPADN9TYSX1
      version: v210
    name: aws-cni-zp5hp
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: aws-cni
      uid: bbfa9558-fb84-11e9-94e9-06cb951c3498
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-125.us-west-2.compute.internal
    containers:
    - name: aws-cni
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 10m
          memory: 96Mi
    initContainers:
    - name: aws-vpc-cni-init
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 32Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:22:22Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      admission.datadoghq.com/enabled: "false"
      app.kubernetes.io/component: cluster-agent
      app.kubernetes.io/instance: cluster-agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-cluster-agent
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v178
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KF0C5Z2XZEZD43AGSHYDJCDE
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 547d94b6bc
      product: foundation
      project: datadog-cluster-agent
      revision: 25ad4eb37ae4dfeffc8973f1c19d94f5dfe2a538
      role: cluster-agent
      service: datadog-cluster-agent
      sidecar.istio.io/inject: "false"
      tag: v178
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-cluster-agent
      tags.datadoghq.com/version: v178
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KF0C5Z2XZEZD43AGSHYDJCDE
      version: v178
    name: datadog-cluster-agent-547d94b6bc-msxj4
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: datadog-cluster-agent-547d94b6bc
      uid: 03197c4b-072a-4c2c-9822-861e9f237e55
  spec:
    containers:
    - name: cluster-agent
      resources:
        limits:
          cpu: "4"
          memory: 6000Mi
        requests:
          cpu: 100m
          memory: 4000Mi
    initContainers:
    - name: init-volume
      resources:
        limits:
          cpu: 500m
          memory: 500Mi
        requests:
          cpu: 50m
          memory: 64Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: high
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      admission.datadoghq.com/enabled: "false"
      app.kubernetes.io/component: cluster-agent
      app.kubernetes.io/instance: cluster-agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-cluster-agent
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v178
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KF0C5Z2XZEZD43AGSHYDJCDE
      deploy_phase: phase-1
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 547d94b6bc
      product: foundation
      project: datadog-cluster-agent
      revision: 25ad4eb37ae4dfeffc8973f1c19d94f5dfe2a538
      role: cluster-agent
      service: datadog-cluster-agent
      sidecar.istio.io/inject: "false"
      tag: v178
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-cluster-agent
      tags.datadoghq.com/version: v178
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KF0C5Z2XZEZD43AGSHYDJCDE
      version: v178
    name: datadog-cluster-agent-547d94b6bc-sqghn
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: datadog-cluster-agent-547d94b6bc
      uid: 03197c4b-072a-4c2c-9822-861e9f237e55
  spec:
    containers:
    - name: cluster-agent
      resources:
        limits:
          cpu: "4"
          memory: 6000Mi
        requests:
          cpu: 100m
          memory: 4000Mi
    initContainers:
    - name: init-volume
      resources:
        limits:
          cpu: 500m
          memory: 500Mi
        requests:
          cpu: 50m
          memory: 64Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: high
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-eventrouter
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v83
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      deploy_group: sandbox
      deploy_id: 01KFEE3JNPG4M90ZQJH38YRV01
      deploy_phase: phase-1
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 56dd4f4d44
      product: foundation
      project: kube-eventrouter
      revision: dbbd165cb3fa2a6ddd6e37ecb06476fc189d6efb
      role: agent
      service: kube-eventrouter
      sidecar.istio.io/inject: "false"
      tag: v83
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-eventrouter
      tags.datadoghq.com/version: v83
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFEE3JNPG4M90ZQJH38YRV01
      version: v83
    name: kube-eventrouter-56dd4f4d44-frhs8
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-eventrouter-56dd4f4d44
      uid: ecdc00fa-3b68-489a-8c6c-0d621e19e50e
  spec:
    containers:
    - name: eventrouter
      resources:
        limits:
          cpu: "2"
          memory: 250Mi
        requests:
          cpu: 11m
          memory: 250Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T21:08:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T21:08:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/instance: external-secrets
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: external-secrets
      app.kubernetes.io/version: v0.9.11
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KEXCR6RJQPJAWBXRQ8VGDFHE
      deploy_phase: phase-1
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      helm.sh/chart: external-secrets-0.9.11
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      opa-gatekeeper.zendesk.com/recommend-probes: "false"
      opa-gatekeeper.zendesk.com/require-high-availability: "false"
      opa-gatekeeper.zendesk.com/require-pod-disruption-budget: "false"
      opa-gatekeeper.zendesk.com/required-probes: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      pod-template-hash: 7c994b6745
      product: foundation
      project: external-secrets
      revision: 65d4ea0dfaa7e997400a38739c502602d70a5563
      role: controller
      service: external-secrets
      sidecar.istio.io/inject: "false"
      tag: v29
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: external-secrets
      tags.datadoghq.com/version: v29
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEXCR6RJQPJAWBXRQ8VGDFHE
      version: v29
    name: external-secrets-7c994b6745-jxsmk
    namespace: external-secrets
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: external-secrets-7c994b6745
      uid: 2c2700c8-afff-4117-bb60-94026718a2e9
  spec:
    containers:
    - name: external-secrets
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7c994b6745
        matchLabels:
          app.kubernetes.io/instance: external-secrets
          app.kubernetes.io/name: external-secrets
          project: external-secrets
          role: controller
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/instance: external-secrets
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: external-secrets-cert-controller
      app.kubernetes.io/version: v0.9.11
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KEXCR6RJQPJAWBXRQ8VGDFHE
      deploy_phase: phase-1
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      helm.sh/chart: external-secrets-0.9.11
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      opa-gatekeeper.zendesk.com/recommend-probes: "false"
      opa-gatekeeper.zendesk.com/require-high-availability: "false"
      opa-gatekeeper.zendesk.com/require-pod-disruption-budget: "false"
      opa-gatekeeper.zendesk.com/required-probes: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      pod-template-hash: 59fdff6677
      product: foundation
      project: external-secrets
      revision: 65d4ea0dfaa7e997400a38739c502602d70a5563
      role: cert-controller
      service: external-secrets
      sidecar.istio.io/inject: "false"
      tag: v29
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: external-secrets
      tags.datadoghq.com/version: v29
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEXCR6RJQPJAWBXRQ8VGDFHE
      version: v29
    name: external-secrets-cert-controller-59fdff6677-n54hm
    namespace: external-secrets
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: external-secrets-cert-controller-59fdff6677
      uid: d4969e5d-253f-4680-8cd5-10cc6628dcaa
  spec:
    containers:
    - name: cert-controller
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 59fdff6677
        matchLabels:
          app.kubernetes.io/instance: external-secrets
          app.kubernetes.io/name: external-secrets-cert-controller
          project: external-secrets
          role: cert-controller
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/instance: external-secrets
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: external-secrets-webhook
      app.kubernetes.io/version: v0.9.11
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KEXCR6RJQPJAWBXRQ8VGDFHE
      deploy_phase: phase-1
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      helm.sh/chart: external-secrets-0.9.11
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      opa-gatekeeper.zendesk.com/recommend-probes: "false"
      opa-gatekeeper.zendesk.com/require-high-availability: "false"
      opa-gatekeeper.zendesk.com/require-pod-disruption-budget: "false"
      opa-gatekeeper.zendesk.com/required-probes: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      pod-template-hash: 5c475cbfb9
      product: foundation
      project: external-secrets
      revision: 65d4ea0dfaa7e997400a38739c502602d70a5563
      role: webhook
      service: external-secrets
      sidecar.istio.io/inject: "false"
      tag: v29
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: external-secrets
      tags.datadoghq.com/version: v29
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEXCR6RJQPJAWBXRQ8VGDFHE
      version: v29
    name: external-secrets-webhook-5c475cbfb9-62gr6
    namespace: external-secrets
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: external-secrets-webhook-5c475cbfb9
      uid: c6e12379-a719-412b-bd20-1f2ae8e47ed7
  spec:
    containers:
    - name: webhook
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5c475cbfb9
        matchLabels:
          app.kubernetes.io/instance: external-secrets
          app.kubernetes.io/name: external-secrets-webhook
          project: external-secrets
          role: webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:56Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-flink-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v179
      branch: main
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01JHRHNNW0VKXV8H79CG03K8JZ
      deploy_phase: phase-sandbox
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 68d6596b
      product: foundation
      project: zendesk-flink-operator
      revision: e6e91abe4251d53eccfc5a838cee8f6f37083cf5
      role: operator
      security.istio.io/tlsMode: istio
      service: zendesk-flink-operator
      service.istio.io/canonical-name: zendesk-flink-operator-operator
      service.istio.io/canonical-revision: v179
      sidecar.istio.io/inject: "true"
      tag: v179
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: zendesk-flink-operator-operator
      tags.datadoghq.com/istio-proxy.version: v179
      tags.datadoghq.com/service: zendesk-flink-operator-operator
      tags.datadoghq.com/version: v179
      team: foundation-analytics-prism
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01JHRHNNW0VKXV8H79CG03K8JZ
      version: v179
      zende.sk/dialtone: "true"
    name: zendesk-flink-operator-68d6596b-hzj9w
    namespace: fdn-analytics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-flink-operator-68d6596b
      uid: 4fdfdd2c-cd78-480c-921d-5130eba6fbdb
  spec:
    containers:
    - name: zendesk-flink-operator
      resources:
        limits:
          cpu: 300m
          memory: 512Mi
        requests:
          cpu: 11m
          memory: 216Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 68d6596b
        matchLabels:
          project: zendesk-flink-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:16Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: audit
      app.kubernetes.io/instance: audit
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: opa-gatekeeper
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1234
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFY305SDM0N5XSK43D4N7JPX
      deploy_phase: phase-1
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 779696f6f7
      product: foundation
      project: opa-gatekeeper
      revision: 360e0aa443d5009eeeaa7068bc1472118bb3d10b
      role: audit
      service: opa-gatekeeper
      sidecar.istio.io/inject: "false"
      tag: v1234
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: opa-gatekeeper
      tags.datadoghq.com/version: v1234
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFY305SDM0N5XSK43D4N7JPX
      version: v1234
    name: gatekeeper-audit-779696f6f7-zsl2v
    namespace: gatekeeper-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gatekeeper-audit-779696f6f7
      uid: 4b48e89b-b04d-4c49-98f4-546d1629327f
  spec:
    containers:
    - name: audit
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: 11m
          memory: 256Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 779696f6f7
        matchLabels:
          project: opa-gatekeeper
          role: audit
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T21:25:12Z"
      observedGeneration: 3
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T21:24:40Z"
      observedGeneration: 3
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: gatekeeper
      app.kubernetes.io/instance: gatekeeper
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: opa-gatekeeper
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1234
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFY305SDM0N5XSK43D4N7JPX
      deploy_phase: phase-1
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 95c46b649
      product: foundation
      project: opa-gatekeeper
      revision: 360e0aa443d5009eeeaa7068bc1472118bb3d10b
      role: gatekeeper
      service: opa-gatekeeper
      sidecar.istio.io/inject: "false"
      tag: v1234
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: opa-gatekeeper
      tags.datadoghq.com/version: v1234
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFY305SDM0N5XSK43D4N7JPX
      version: v1234
    name: gatekeeper-controller-manager-95c46b649-6jqnq
    namespace: gatekeeper-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gatekeeper-controller-manager-95c46b649
      uid: 6cedb9dc-4e3d-4e9c-95c2-ad18f8b664f3
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 6Gi
        requests:
          cpu: 200m
          memory: 1536Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: high
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 95c46b649
        matchLabels:
          project: opa-gatekeeper
          role: gatekeeper
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: gatekeeper
      app.kubernetes.io/instance: gatekeeper
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: opa-gatekeeper
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1234
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFY305SDM0N5XSK43D4N7JPX
      deploy_phase: phase-1
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 95c46b649
      product: foundation
      project: opa-gatekeeper
      revision: 360e0aa443d5009eeeaa7068bc1472118bb3d10b
      role: gatekeeper
      service: opa-gatekeeper
      sidecar.istio.io/inject: "false"
      tag: v1234
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: opa-gatekeeper
      tags.datadoghq.com/version: v1234
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFY305SDM0N5XSK43D4N7JPX
      version: v1234
    name: gatekeeper-controller-manager-95c46b649-b4kpg
    namespace: gatekeeper-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gatekeeper-controller-manager-95c46b649
      uid: 6cedb9dc-4e3d-4e9c-95c2-ad18f8b664f3
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 6Gi
        requests:
          cpu: 200m
          memory: 1536Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: high
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 95c46b649
        matchLabels:
          project: opa-gatekeeper
          role: gatekeeper
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:11:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:10:43Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: gatekeeper
      app.kubernetes.io/instance: gatekeeper
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: opa-gatekeeper
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1234
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KFY305SDM0N5XSK43D4N7JPX
      deploy_phase: phase-1
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 95c46b649
      product: foundation
      project: opa-gatekeeper
      revision: 360e0aa443d5009eeeaa7068bc1472118bb3d10b
      role: gatekeeper
      service: opa-gatekeeper
      sidecar.istio.io/inject: "false"
      tag: v1234
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: opa-gatekeeper
      tags.datadoghq.com/version: v1234
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFY305SDM0N5XSK43D4N7JPX
      version: v1234
    name: gatekeeper-controller-manager-95c46b649-cslt2
    namespace: gatekeeper-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gatekeeper-controller-manager-95c46b649
      uid: 6cedb9dc-4e3d-4e9c-95c2-ad18f8b664f3
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 6Gi
        requests:
          cpu: 200m
          memory: 1536Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 100
    priorityClassName: high
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 95c46b649
        matchLabels:
          project: opa-gatekeeper
          role: gatekeeper
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:01:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:00:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/termination-sensitive: "true"
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      scheduler.compute.zende.sk/opt-out: approved-by-compute
      spot-placement-policy: on-demand
      topology.kubernetes.io/zone: us-west-2a
    name: pod0
    namespace: gpu-test5
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: compute.zende.sk/termination-sensitive
              operator: In
              values:
              - "true"
          weight: 10
        - preference:
            matchExpressions:
            - key: compute.zendesk.com/ec2_image_sha
              operator: In
              values:
              - b7f462ec
          weight: 100
    containers:
    - name: mps-ctr0
      resources:
        claims:
        - name: shared-gpus
          request: mps-gpu
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/termination-sensitive: "true"
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      scheduler.compute.zende.sk/opt-out: approved-by-compute
      spot-placement-policy: on-demand
      topology.kubernetes.io/zone: us-west-2a
    name: pod1
    namespace: gpu-test5
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: compute.zende.sk/termination-sensitive
              operator: In
              values:
              - "true"
          weight: 10
        - preference:
            matchExpressions:
            - key: compute.zendesk.com/ec2_image_sha
              operator: In
              values:
              - b7f462ec
          weight: 100
    containers:
    - name: mps-ctr0
      resources:
        claims:
        - name: shared-gpus
          request: mps-gpu
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/termination-sensitive: "true"
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      scheduler.compute.zende.sk/opt-out: approved-by-compute
      spot-placement-policy: on-demand
      topology.kubernetes.io/zone: us-west-2a
    name: pod2
    namespace: gpu-test5
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: compute.zende.sk/termination-sensitive
              operator: In
              values:
              - "true"
          weight: 10
        - preference:
            matchExpressions:
            - key: compute.zendesk.com/ec2_image_sha
              operator: In
              values:
              - b7f462ec
          weight: 100
    containers:
    - name: mps-ctr0
      resources:
        claims:
        - name: shared-gpus
          request: mps-gpu
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v126
      branch: v126
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KCPZDGW2V2PFKNAP2HKGEZ1B
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 79cb654
      product: foundation
      project: ingress-nginx
      revision: 5c045bbc2a5c4ae7e3f63ad904754af175c5d0a6
      role: app-server
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: ingress-nginx
      sidecar.istio.io/inject: "false"
      tag: v126
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: ingress-nginx
      tags.datadoghq.com/version: v126
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KCPZDGW2V2PFKNAP2HKGEZ1B
      version: v126
    name: nginx-ingress-controller-79cb654-jf94m
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nginx-ingress-controller-79cb654
      uid: c3286d66-a599-41d6-9fcf-14b4d351abba
  spec:
    containers:
    - name: nginx-ingress-controller
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 11m
          memory: 200Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79cb654
        matchLabels:
          project: ingress-nginx
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:54:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: httpbin
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 74cd48587f
      service: istio
      topology.kubernetes.io/zone: us-west-2b
    name: httpbin-74cd48587f-mprpd
    namespace: istio-ambient-testing
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: httpbin-74cd48587f
      uid: 5069a763-0a2f-426b-b0e4-668eaf684a3a
  spec:
    containers:
    - name: httpbin
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 74cd48587f
        matchLabels:
          app: httpbin
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: sleep
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      istio.io/dataplane-mode: ambient
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7cc445b75f
      topology.kubernetes.io/zone: us-west-2b
    name: sleep-7cc445b75f-zr2lj
    namespace: istio-ambient-testing
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sleep-7cc445b75f
      uid: 7771edd6-e850-4147-a772-1f1a03490607
  spec:
    containers:
    - name: sleep
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7cc445b75f
        matchLabels:
          app: sleep
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:42Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:19Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-00c5b7259f4923f4f
      gateway.istio.io/managed: istio.io-mesh-controller
      gateway.networking.k8s.io/gateway-name: waypoint
      istio.io/dataplane-mode: none
      istio.io/waypoint-for: all
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 65f94f99d8
      service.istio.io/canonical-name: waypoint
      service.istio.io/canonical-revision: latest
      sidecar.istio.io/inject: "false"
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
    name: waypoint-65f94f99d8-5l7np
    namespace: istio-ambient-testing
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: waypoint-65f94f99d8
      uid: 97fd898a-b24b-4856-9e06-e2591f423603
  spec:
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 128Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 65f94f99d8
        matchLabels:
          gateway.networking.k8s.io/gateway-name: waypoint
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: httpbin
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      istio.io/dataplane-mode: none
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 5f56fb6bf8
      security.istio.io/tlsMode: istio
      service.istio.io/canonical-name: httpbin
      service.istio.io/canonical-revision: latest
      sidecar.istio.io/inject: "true"
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
    name: httpbin-5f56fb6bf8-8n7xj
    namespace: istio-demo
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: httpbin-5f56fb6bf8
      uid: d38b3b40-97ee-4af0-8c99-fa84325bb214
  spec:
    containers:
    - name: httpbin
      resources: {}
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5f56fb6bf8
        matchLabels:
          app: httpbin
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: init-checker
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 56d69d87cb
      security.istio.io/tlsMode: istio
      service.istio.io/canonical-name: init-checker
      service.istio.io/canonical-revision: latest
      sidecar.istio.io/inject: "true"
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
    name: init-checker-56d69d87cb-9ctc5
    namespace: istio-ecr-proxy
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: init-checker-56d69d87cb
      uid: 23c6c851-48b3-4b57-bd2b-abd069f36544
  spec:
    containers:
    - name: main-app
      resources: {}
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: google-check
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 56d69d87cb
        matchLabels:
          app: init-checker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:43Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istio-ingressgateway
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-ingressgateway
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4-zruchi-send-traces-to-otel-collector
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: foundation
      ec2-instance-id: i-0af7d5e8a274b2b88
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      helm.sh/chart: istio-ingress-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: ingressgateway
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      operator.istio.io/component: IngressGateways
      pod-template-hash: 686cc844f6
      product: foundation
      project: istio-gateways
      role: istio
      service: istio-gateways
      service.istio.io/canonical-name: istio-ingressgateway
      service.istio.io/canonical-revision: 1-27-4
      sidecar.istio.io/inject: "true"
      team: network
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      visibility: private
    name: istio-ingressgateway-686cc844f6-hhcww
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istio-ingressgateway-686cc844f6
      uid: 168a3913-cb84-455a-8eb6-33e47e9c8afb
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 686cc844f6
        matchLabels:
          app: istio-ingressgateway
          istio: ingressgateway
          visibility: private
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istio-ingressgateway
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:58:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:56:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istio-ingressgateway
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-ingressgateway
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4-zruchi-send-traces-to-otel-collector
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: foundation
      ec2-instance-id: i-00d9ece9adca2e6a7
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      helm.sh/chart: istio-ingress-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: ingressgateway
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      operator.istio.io/component: IngressGateways
      pod-template-hash: 686cc844f6
      product: foundation
      project: istio-gateways
      role: istio
      service: istio-gateways
      service.istio.io/canonical-name: istio-ingressgateway
      service.istio.io/canonical-revision: 1-27-4
      sidecar.istio.io/inject: "true"
      team: network
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      visibility: private
    name: istio-ingressgateway-686cc844f6-rlngt
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istio-ingressgateway-686cc844f6
      uid: 168a3913-cb84-455a-8eb6-33e47e9c8afb
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 686cc844f6
        matchLabels:
          app: istio-ingressgateway
          istio: ingressgateway
          visibility: private
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istio-ingressgateway
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:31:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istio-multicluster-gateway
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-multicluster-gateway
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4-zruchi-send-traces-to-otel-collector
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: foundation
      ec2-instance-id: i-0af7d5e8a274b2b88
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      helm.sh/chart: istio-ingress-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: multicluster-gateway
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      operator.istio.io/component: IngressGateways
      pod-template-hash: 66c46d4b44
      product: foundation
      project: istio-gateways
      role: istio
      service: istio-gateways
      service.istio.io/canonical-name: istio-multicluster-gateway
      service.istio.io/canonical-revision: 1-27-4
      sidecar.istio.io/inject: "true"
      team: network
      topology.istio.io/network: sandbox
      topology.kubernetes.io/zone: us-west-2b
      visibility: private
    name: istio-multicluster-gateway-66c46d4b44-dknbv
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istio-multicluster-gateway-66c46d4b44
      uid: ae61c489-fafb-4e05-820c-bb2da52c84a5
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 66c46d4b44
        matchLabels:
          app: istio-multicluster-gateway
          istio: multicluster-gateway
          topology.istio.io/network: sandbox
          visibility: private
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istio-multicluster-gateway
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:58:20Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:56:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istio-multicluster-gateway
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-multicluster-gateway
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4-zruchi-send-traces-to-otel-collector
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: foundation
      ec2-instance-id: i-010877a189d01d91a
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      helm.sh/chart: istio-ingress-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: multicluster-gateway
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      operator.istio.io/component: IngressGateways
      pod-template-hash: 66c46d4b44
      product: foundation
      project: istio-gateways
      role: istio
      service: istio-gateways
      service.istio.io/canonical-name: istio-multicluster-gateway
      service.istio.io/canonical-revision: 1-27-4
      sidecar.istio.io/inject: "true"
      team: network
      topology.istio.io/network: sandbox
      topology.kubernetes.io/zone: us-west-2a
      visibility: private
    name: istio-multicluster-gateway-66c46d4b44-tbnkv
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istio-multicluster-gateway-66c46d4b44
      uid: ae61c489-fafb-4e05-820c-bb2da52c84a5
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 66c46d4b44
        matchLabels:
          app: istio-multicluster-gateway
          istio: multicluster-gateway
          topology.istio.io/network: sandbox
          visibility: private
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istio-multicluster-gateway
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T03:20:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T03:19:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istio-on-ec2-ingressgateway
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-on-ec2-ingressgateway
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4-zruchi-send-traces-to-otel-collector
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: foundation
      ec2-instance-id: i-010877a189d01d91a
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      helm.sh/chart: istio-ingress-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: istio-on-ec2-ingressgateway
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      operator.istio.io/component: IngressGateways
      pod-template-hash: 7f9bdff8b6
      product: foundation
      project: istio-gateways
      role: istio
      service: istio-gateways
      service.istio.io/canonical-name: istio-on-ec2-ingressgateway
      service.istio.io/canonical-revision: 1-27-4
      sidecar.istio.io/inject: "true"
      team: network
      topology.istio.io/network: sandbox
      topology.kubernetes.io/zone: us-west-2a
      visibility: private
    name: istio-on-ec2-ingressgateway-7f9bdff8b6-4twjz
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istio-on-ec2-ingressgateway-7f9bdff8b6
      uid: 130823d6-2792-48fe-8122-1911a165cfdf
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7f9bdff8b6
        matchLabels:
          app: istio-on-ec2-ingressgateway
          istio: istio-on-ec2-ingressgateway
          topology.istio.io/network: sandbox
          visibility: private
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istio-on-ec2-ingressgateway
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:13:12Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:12:05Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istio-on-ec2-ingressgateway
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-on-ec2-ingressgateway
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4-zruchi-send-traces-to-otel-collector
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: foundation
      ec2-instance-id: i-0af7d5e8a274b2b88
      elbv2.k8s.aws/pod-readiness-gate-inject: enabled
      helm.sh/chart: istio-ingress-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: istio-on-ec2-ingressgateway
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      operator.istio.io/component: IngressGateways
      pod-template-hash: 7f9bdff8b6
      product: foundation
      project: istio-gateways
      role: istio
      service: istio-gateways
      service.istio.io/canonical-name: istio-on-ec2-ingressgateway
      service.istio.io/canonical-revision: 1-27-4
      sidecar.istio.io/inject: "true"
      team: network
      topology.istio.io/network: sandbox
      topology.kubernetes.io/zone: us-west-2b
      visibility: private
    name: istio-on-ec2-ingressgateway-7f9bdff8b6-lj8gs
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istio-on-ec2-ingressgateway-7f9bdff8b6
      uid: 130823d6-2792-48fe-8122-1911a165cfdf
  spec:
    affinity:
      nodeAffinity: {}
    containers:
    - name: istio-proxy
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7f9bdff8b6
        matchLabels:
          app: istio-on-ec2-ingressgateway
          istio: istio-on-ec2-ingressgateway
          topology.istio.io/network: sandbox
          visibility: private
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istio-on-ec2-ingressgateway
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:13:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:12:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: ssv2-syncer
      app.kubernetes.io/instance: ssv2-syncer
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istio-control-plane
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: 1.27.4
      branch: zruchi-send-traces-to-otel-collector
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier0
      deploy_group: sandbox
      deploy_id: 01KEREQGT03GSVE8ZMQ997QZM8
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 596c694dcf
      product: foundation
      project: istio-control-plane
      revision: df3fd739028f9f0c7b989014dfa2ec2916e0445a
      role: ssv2-syncer
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: istio-control-plane
      sidecar.istio.io/inject: "false"
      tag: zruchi-send-traces-to-otel-collector
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: istio-control-plane-ssv2-syncer
      tags.datadoghq.com/version: 1.27.4
      team: network
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEREQGT03GSVE8ZMQ997QZM8
      version: 1.27.4
    name: ssv2-syncer-596c694dcf-22cgl
    namespace: istio-gateways
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ssv2-syncer-596c694dcf
      uid: b1d96c8c-be37-40c7-b0e6-0f7f72732f55
  spec:
    containers:
    - name: ssv2-syncer
      resources:
        limits:
          cpu: 20m
          memory: 75Mi
        requests:
          cpu: 10m
          memory: 25Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 596c694dcf
        matchLabels:
          project: istio-control-plane
          role: ssv2-syncer
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:45:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:19Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istiod
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istiod
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.26.3
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      criticality-tier: tier0
      datadog_log_index: foundation
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0f8e0c5e9e6e06171
      helm.sh/chart: istiod-1.26.3
      install.operator.istio.io/owning-resource: unknown
      istio: istiod
      istio.io/dataplane-mode: none
      istio.io/rev: 1-26-3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      old-pod-evictor.compute.zende.sk/enabled: "true"
      operator.istio.io/component: Pilot
      operator.istio.io/version: 1.26.3
      pod-template-hash: 6cddf57d7f
      product: foundation
      project: istio-control-plane
      role: istio
      service: istio-control-plane
      sidecar.istio.io/inject: "false"
      tags.datadoghq.com/version: 1.26.3-khris-add-preflight-check
      team: network
      topology.kubernetes.io/zone: us-west-2c
      version: 1.26.3-khris-add-preflight-check
      zende.sk/dialtone: "true"
    name: istiod-1-26-3-6cddf57d7f-klnbj
    namespace: istio-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istiod-1-26-3-6cddf57d7f
      uid: 95e3d2be-230c-4d8c-a85d-8265e250e4e2
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: discovery
      resources:
        limits:
          cpu: "10"
          memory: 10Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: cni.istio.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6cddf57d7f
        matchLabels:
          app: istiod
          istio.io/rev: 1-26-3
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istiod
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istiod
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istiod
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.26.3
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      criticality-tier: tier0
      datadog_log_index: foundation
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      helm.sh/chart: istiod-1.26.3
      install.operator.istio.io/owning-resource: unknown
      istio: istiod
      istio.io/dataplane-mode: none
      istio.io/rev: 1-26-3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      old-pod-evictor.compute.zende.sk/enabled: "true"
      operator.istio.io/component: Pilot
      operator.istio.io/version: 1.26.3
      pod-template-hash: 6cddf57d7f
      product: foundation
      project: istio-control-plane
      role: istio
      service: istio-control-plane
      sidecar.istio.io/inject: "false"
      tags.datadoghq.com/version: 1.26.3-khris-add-preflight-check
      team: network
      topology.kubernetes.io/zone: us-west-2a
      version: 1.26.3-khris-add-preflight-check
      zende.sk/dialtone: "true"
    name: istiod-1-26-3-6cddf57d7f-lr7xj
    namespace: istio-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istiod-1-26-3-6cddf57d7f
      uid: 95e3d2be-230c-4d8c-a85d-8265e250e4e2
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: discovery
      resources:
        limits:
          cpu: "10"
          memory: 10Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: cni.istio.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6cddf57d7f
        matchLabels:
          app: istiod
          istio.io/rev: 1-26-3
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istiod
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:31:25Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:30:53Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istiod
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istiod
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      criticality-tier: tier0
      datadog_log_index: foundation
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0af7d5e8a274b2b88
      helm.sh/chart: istiod-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: istiod
      istio.io/dataplane-mode: none
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      old-pod-evictor.compute.zende.sk/enabled: "true"
      operator.istio.io/component: Pilot
      operator.istio.io/version: 1.27.4
      pod-template-hash: 757cf7fb55
      product: foundation
      project: istio-control-plane
      role: istio
      service: istio-control-plane
      sidecar.istio.io/inject: "false"
      tags.datadoghq.com/version: 1.27.4-zruchi-send-traces-to-otel-collector
      team: network
      topology.kubernetes.io/zone: us-west-2b
      version: 1.27.4-zruchi-send-traces-to-otel-collector
      zende.sk/dialtone: "true"
    name: istiod-1-27-4-757cf7fb55-hg6q4
    namespace: istio-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istiod-1-27-4-757cf7fb55
      uid: b0f82128-50d2-4081-94c3-31d4a3788b58
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: discovery
      resources:
        limits:
          cpu: "10"
          memory: 10Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: cni.istio.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 757cf7fb55
        matchLabels:
          app: istiod
          istio.io/rev: 1-27-4
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istiod
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:11:17Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:10:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istiod
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istiod
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      criticality-tier: tier0
      datadog_log_index: foundation
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      helm.sh/chart: istiod-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: istiod
      istio.io/dataplane-mode: none
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      old-pod-evictor.compute.zende.sk/enabled: "true"
      operator.istio.io/component: Pilot
      operator.istio.io/version: 1.27.4
      pod-template-hash: 757cf7fb55
      product: foundation
      project: istio-control-plane
      role: istio
      service: istio-control-plane
      sidecar.istio.io/inject: "false"
      tags.datadoghq.com/version: 1.27.4-zruchi-send-traces-to-otel-collector
      team: network
      topology.kubernetes.io/zone: us-west-2a
      version: 1.27.4-zruchi-send-traces-to-otel-collector
      zende.sk/dialtone: "true"
    name: istiod-1-27-4-757cf7fb55-lbm9q
    namespace: istio-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istiod-1-27-4-757cf7fb55
      uid: b0f82128-50d2-4081-94c3-31d4a3788b58
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: discovery
      resources:
        limits:
          cpu: "10"
          memory: 10Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: cni.istio.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 757cf7fb55
        matchLabels:
          app: istiod
          istio.io/rev: 1-27-4
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istiod
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:56Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: istiod
      app.kubernetes.io/instance: istio
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: istiod
      app.kubernetes.io/part-of: istio
      app.kubernetes.io/version: 1.27.4
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      criticality-tier: tier0
      datadog_log_index: foundation
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-06bfb6b4b3967a8e2
      helm.sh/chart: istiod-1.27.4
      install.operator.istio.io/owning-resource: unknown
      istio: istiod
      istio.io/dataplane-mode: none
      istio.io/rev: 1-27-4
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      old-pod-evictor.compute.zende.sk/enabled: "true"
      operator.istio.io/component: Pilot
      operator.istio.io/version: 1.27.4
      pod-template-hash: 757cf7fb55
      product: foundation
      project: istio-control-plane
      role: istio
      service: istio-control-plane
      sidecar.istio.io/inject: "false"
      tags.datadoghq.com/version: 1.27.4-zruchi-send-traces-to-otel-collector
      team: network
      topology.kubernetes.io/zone: us-west-2c
      version: 1.27.4-zruchi-send-traces-to-otel-collector
      zende.sk/dialtone: "true"
    name: istiod-1-27-4-757cf7fb55-tlwm5
    namespace: istio-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: istiod-1-27-4-757cf7fb55
      uid: b0f82128-50d2-4081-94c3-31d4a3788b58
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: discovery
      resources:
        limits:
          cpu: "10"
          memory: 10Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: cni.istio.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 757cf7fb55
        matchLabels:
          app: istiod
          istio.io/rev: 1-27-4
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchLabels:
          app: istiod
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 74c9b88d7b
      service: kubernetes
      topology.kubernetes.io/zone: us-west-2b
    name: jobset-controller-manager-74c9b88d7b-fj8bz
    namespace: jobset-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: jobset-controller-manager-74c9b88d7b
      uid: c0fb4d83-2833-4937-8ec7-0e50baba8728
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
              - arm64
              - ppc64le
              - s390x
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 512Mi
        requests:
          cpu: 11m
          memory: 128Mi
    - name: kube-rbac-proxy
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 64Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:13:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:12:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: test-app
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 57c496cbf6
      security.istio.io/tlsMode: istio
      service.istio.io/canonical-name: test-app
      service.istio.io/canonical-revision: latest
      sidecar.istio.io/inject: "true"
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
    name: test-deployment-57c496cbf6-9l7c7
    namespace: julian
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: test-deployment-57c496cbf6
      uid: 64f74ef7-3b3b-4eef-bd03-4b2f91559444
  spec:
    containers:
    - name: nginx
      resources: {}
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57c496cbf6
        matchLabels:
          app: test-app
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:11Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: mps-control-daemon-0db99154-6a52-4245-81d8-2d6a910b530f-a19eb
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-hash: 6566b667b
      topology.kubernetes.io/zone: us-west-2a
    name: mps-control-daemon-0db99154-6a52-4245-81d8-2d6a910b530f-a1wstdb
    namespace: k8s-dra-driver-gpu
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: mps-control-daemon-0db99154-6a52-4245-81d8-2d6a910b530f-a19eb-6566b667b
      uid: 464b76ce-4141-441c-9d75-097aa8d05fe7
  spec:
    containers:
    - name: mps-control-daemon
      resources: {}
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6566b667b
        matchLabels:
          app: mps-control-daemon-0db99154-6a52-4245-81d8-2d6a910b530f-a19eb
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: mps-control-daemon-e1bdfbb3-c8f8-435e-ae2d-2139616d29fd-c8b88
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-hash: 645f7fdfb5
      topology.kubernetes.io/zone: us-west-2a
    name: mps-control-daemon-e1bdfbb3-c8f8-435e-ae2d-2139616d29fd-c8bvwn8
    namespace: k8s-dra-driver-gpu
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: mps-control-daemon-e1bdfbb3-c8f8-435e-ae2d-2139616d29fd-c8b88-645f7fdfb5
      uid: 96727180-3b53-4d54-8f7c-efcee0b27de4
  spec:
    containers:
    - name: mps-control-daemon
      resources: {}
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 645f7fdfb5
        matchLabels:
          app: mps-control-daemon-e1bdfbb3-c8f8-435e-ae2d-2139616d29fd-c8b88
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:50Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:56:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: nvidia-dra-driver-gpu
      app.kubernetes.io/instance: nvidia-dra-driver-gpu
      app.kubernetes.io/name: k8s-dra-driver-gpu
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v9
      branch: main
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: b599f4dfb
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_phase: phase-1
      ec2-instance-id: i-02ef8ae4559f58e80
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      nvidia-dra-driver-gpu-component: kubelet-plugin
      pod-template-generation: "3"
      product: foundation
      project: k8s-dra-driver-gpu
      revision: 40ccad4ddc15583a227ada93cfd5acd3cc8038a6
      role: nvidia-dra-driver-gpu
      service: k8s-dra-driver-gpu
      sidecar.istio.io/inject: "false"
      tag: v9
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-dra-driver-gpu-nvidia-dra-driver-gpu
      tags.datadoghq.com/version: v9
      team: compute-infra
      topology.kubernetes.io/zone: us-west-2a
      version: v9
    name: nvidia-dra-driver-gpu-kubelet-plugin-7mf4w
    namespace: k8s-dra-driver-gpu
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dra-driver-gpu-kubelet-plugin
      uid: 39553d2e-7654-4c6f-a0ff-d02cac357312
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: gpus
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    initContainers:
    - name: init-container
      resources: {}
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:58Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: nvidia-dra-driver-gpu
      app.kubernetes.io/instance: nvidia-dra-driver-gpu
      app.kubernetes.io/name: k8s-dra-driver-gpu
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v9
      branch: main
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: b599f4dfb
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_phase: phase-1
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      nvidia-dra-driver-gpu-component: kubelet-plugin
      pod-template-generation: "3"
      product: foundation
      project: k8s-dra-driver-gpu
      revision: 40ccad4ddc15583a227ada93cfd5acd3cc8038a6
      role: nvidia-dra-driver-gpu
      service: k8s-dra-driver-gpu
      sidecar.istio.io/inject: "false"
      tag: v9
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-dra-driver-gpu-nvidia-dra-driver-gpu
      tags.datadoghq.com/version: v9
      team: compute-infra
      topology.kubernetes.io/zone: us-west-2a
      version: v9
    name: nvidia-dra-driver-gpu-kubelet-plugin-95hpq
    namespace: k8s-dra-driver-gpu
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dra-driver-gpu-kubelet-plugin
      uid: 39553d2e-7654-4c6f-a0ff-d02cac357312
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: gpus
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    initContainers:
    - name: init-container
      resources: {}
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:32Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:03:55Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: nvidia-dra-driver-gpu
      app.kubernetes.io/instance: nvidia-dra-driver-gpu
      app.kubernetes.io/name: k8s-dra-driver-gpu
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v9
      branch: main
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: b599f4dfb
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_phase: phase-1
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      nvidia-dra-driver-gpu-component: kubelet-plugin
      pod-template-generation: "3"
      product: foundation
      project: k8s-dra-driver-gpu
      revision: 40ccad4ddc15583a227ada93cfd5acd3cc8038a6
      role: nvidia-dra-driver-gpu
      service: k8s-dra-driver-gpu
      sidecar.istio.io/inject: "false"
      tag: v9
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-dra-driver-gpu-nvidia-dra-driver-gpu
      tags.datadoghq.com/version: v9
      team: compute-infra
      topology.kubernetes.io/zone: us-west-2a
      version: v9
    name: nvidia-dra-driver-gpu-kubelet-plugin-vrnkj
    namespace: k8s-dra-driver-gpu
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dra-driver-gpu-kubelet-plugin
      uid: 39553d2e-7654-4c6f-a0ff-d02cac357312
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: gpus
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    initContainers:
    - name: init-container
      resources: {}
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:25Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:59:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dcgm-exporter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-nvidia-dcgm-exporter
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v34
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 5486db75b9
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KEXM4TTP607K6MZJZ7J4JMW4
      deploy_phase: phase-1
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "50"
      product: foundation
      project: k8s-nvidia-dcgm-exporter
      revision: 7086f5105c9ffd2cdef80959f2c9dc9bf17338a1
      role: dcgm-exporter
      service: k8s-nvidia-dcgm-exporter
      sidecar.istio.io/inject: "false"
      tag: v34
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-nvidia-dcgm-exporter-dcgm-exporter
      tags.datadoghq.com/version: v34
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEXM4TTP607K6MZJZ7J4JMW4
      version: v34
    name: dcgm-exporter-nn7d9
    namespace: k8s-nvidia-dcgm-exporter
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: 9fec69e5-e402-4661-9b83-6d1e38cc5414
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: exporter
      resources:
        limits:
          cpu: 500m
          memory: 768M
        requests:
          cpu: 50m
          memory: 512M
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:08Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:59:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dcgm-exporter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-nvidia-dcgm-exporter
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v34
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 5486db75b9
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KEXM4TTP607K6MZJZ7J4JMW4
      deploy_phase: phase-1
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "50"
      product: foundation
      project: k8s-nvidia-dcgm-exporter
      revision: 7086f5105c9ffd2cdef80959f2c9dc9bf17338a1
      role: dcgm-exporter
      service: k8s-nvidia-dcgm-exporter
      sidecar.istio.io/inject: "false"
      tag: v34
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-nvidia-dcgm-exporter-dcgm-exporter
      tags.datadoghq.com/version: v34
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEXM4TTP607K6MZJZ7J4JMW4
      version: v34
    name: dcgm-exporter-wz4t9
    namespace: k8s-nvidia-dcgm-exporter
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: 9fec69e5-e402-4661-9b83-6d1e38cc5414
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: exporter
      resources:
        limits:
          cpu: 500m
          memory: 768M
        requests:
          cpu: 50m
          memory: 512M
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:05:19Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:03:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dcgm-exporter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-nvidia-dcgm-exporter
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v34
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 5486db75b9
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KEXM4TTP607K6MZJZ7J4JMW4
      deploy_phase: phase-1
      ec2-instance-id: i-02ef8ae4559f58e80
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "50"
      product: foundation
      project: k8s-nvidia-dcgm-exporter
      revision: 7086f5105c9ffd2cdef80959f2c9dc9bf17338a1
      role: dcgm-exporter
      service: k8s-nvidia-dcgm-exporter
      sidecar.istio.io/inject: "false"
      tag: v34
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-nvidia-dcgm-exporter-dcgm-exporter
      tags.datadoghq.com/version: v34
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEXM4TTP607K6MZJZ7J4JMW4
      version: v34
    name: dcgm-exporter-wz5qt
    namespace: k8s-nvidia-dcgm-exporter
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: 9fec69e5-e402-4661-9b83-6d1e38cc5414
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: exporter
      resources:
        limits:
          cpu: 500m
          memory: 768M
        requests:
          cpu: 50m
          memory: 512M
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 2000001000
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:05:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: cluster-proxy
      app.kubernetes.io/instance: cluster-proxy
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: karmada-system
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v886
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KFJ8RXVF88MQ5YGM8QZSN4XQ
      deploy_phase: phase-sandbox
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 756b68466c
      product: foundation
      project: karmada-system
      revision: 27dd05f6833e5f497328ae59c0783d92f1e81eb8
      role: cluster-proxy
      service: karmada-system
      sidecar.istio.io/inject: "false"
      tag: v886
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: karmada-system-cluster-proxy
      tags.datadoghq.com/version: v886
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFJ8RXVF88MQ5YGM8QZSN4XQ
      version: v886
    name: cluster-proxy-756b68466c-mxbb7
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cluster-proxy-756b68466c
      uid: f9aea4c9-3318-4655-befc-81e84d5dc2a5
  spec:
    containers:
    - name: apiserver
      resources:
        limits:
          cpu: "4"
          memory: 8Gi
        requests:
          cpu: 250m
          memory: 2Gi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 756b68466c
        matchLabels:
          project: karmada-system
          role: cluster-proxy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: cluster-proxy
      app.kubernetes.io/instance: cluster-proxy
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: karmada-system
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v886
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KFJ8RXVF88MQ5YGM8QZSN4XQ
      deploy_phase: phase-sandbox
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 756b68466c
      product: foundation
      project: karmada-system
      revision: 27dd05f6833e5f497328ae59c0783d92f1e81eb8
      role: cluster-proxy
      service: karmada-system
      sidecar.istio.io/inject: "false"
      tag: v886
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: karmada-system-cluster-proxy
      tags.datadoghq.com/version: v886
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFJ8RXVF88MQ5YGM8QZSN4XQ
      version: v886
    name: cluster-proxy-756b68466c-xcnk9
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cluster-proxy-756b68466c
      uid: f9aea4c9-3318-4655-befc-81e84d5dc2a5
  spec:
    containers:
    - name: apiserver
      resources:
        limits:
          cpu: "4"
          memory: 8Gi
        requests:
          cpu: 250m
          memory: 2Gi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 756b68466c
        matchLabels:
          project: karmada-system
          role: cluster-proxy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: cluster-proxy
      app.kubernetes.io/instance: cluster-proxy
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: karmada-system
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v886
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KFJ8RXVF88MQ5YGM8QZSN4XQ
      deploy_phase: phase-sandbox
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 756b68466c
      product: foundation
      project: karmada-system
      revision: 27dd05f6833e5f497328ae59c0783d92f1e81eb8
      role: cluster-proxy
      service: karmada-system
      sidecar.istio.io/inject: "false"
      tag: v886
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: karmada-system-cluster-proxy
      tags.datadoghq.com/version: v886
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFJ8RXVF88MQ5YGM8QZSN4XQ
      version: v886
    name: cluster-proxy-756b68466c-zhz8q
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cluster-proxy-756b68466c
      uid: f9aea4c9-3318-4655-befc-81e84d5dc2a5
  spec:
    containers:
    - name: apiserver
      resources:
        limits:
          cpu: "4"
          memory: 8Gi
        requests:
          cpu: 250m
          memory: 2Gi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier2
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 756b68466c
        matchLabels:
          project: karmada-system
          role: cluster-proxy
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: b496b477
      product: foundation
      project: karmada-system
      role: karmada-aggregated-apiserver
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    name: karmada-aggregated-apiserver-b496b477-ml5td
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-aggregated-apiserver-b496b477
      uid: d960a97d-3687-4522-b21f-094e302243db
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - karmada-aggregated-apiserver
          topologyKey: kubernetes.io/hostname
    containers:
    - name: karmada-aggregated-apiserver
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 20m
          memory: 256Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/control-plane: control-plane
    priority: 100
    priorityClassName: tier3
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - b496b477
        matchLabels:
          project: karmada-system
          role: karmada-aggregated-apiserver
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: b496b477
      product: foundation
      project: karmada-system
      role: karmada-aggregated-apiserver
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    name: karmada-aggregated-apiserver-b496b477-t7phh
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-aggregated-apiserver-b496b477
      uid: d960a97d-3687-4522-b21f-094e302243db
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - karmada-aggregated-apiserver
          topologyKey: kubernetes.io/hostname
    containers:
    - name: karmada-aggregated-apiserver
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 20m
          memory: 256Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/control-plane: control-plane
    priority: 100
    priorityClassName: tier3
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - b496b477
        matchLabels:
          project: karmada-system
          role: karmada-aggregated-apiserver
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:32Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 687d7f97c4
      product: foundation
      project: karmada-system
      role: karmada-controller-manager
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    name: karmada-controller-manager-687d7f97c4-pl7cj
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-controller-manager-687d7f97c4
      uid: 8f2b5121-5378-468a-a984-323eb7b75173
  spec:
    containers:
    - name: karmada-controller-manager
      resources:
        limits:
          cpu: "8"
          memory: 4Gi
        requests:
          cpu: 35m
          memory: 1Gi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 687d7f97c4
        matchLabels:
          project: karmada-system
          role: karmada-controller-manager
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 687d7f97c4
      product: foundation
      project: karmada-system
      role: karmada-controller-manager
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    name: karmada-controller-manager-687d7f97c4-rhzff
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-controller-manager-687d7f97c4
      uid: 8f2b5121-5378-468a-a984-323eb7b75173
  spec:
    containers:
    - name: karmada-controller-manager
      resources:
        limits:
          cpu: "8"
          memory: 4Gi
        requests:
          cpu: 35m
          memory: 1Gi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 687d7f97c4
        matchLabels:
          project: karmada-system
          role: karmada-controller-manager
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T08:03:26Z"
      observedGeneration: 46
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T08:03:22Z"
      observedGeneration: 46
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 858b68f5b6
      product: foundation
      project: karmada-system
      role: karmada-scheduler
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2a
    name: karmada-scheduler-858b68f5b6-jjpfk
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-scheduler-858b68f5b6
      uid: 82826a3b-1ab6-47d5-ab11-285429364811
  spec:
    containers:
    - name: karmada-scheduler
      resources:
        limits:
          cpu: "1"
          memory: 250Mi
        requests:
          cpu: 11m
          memory: 64Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 858b68f5b6
        matchLabels:
          project: karmada-system
          role: karmada-scheduler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:23Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 858b68f5b6
      product: foundation
      project: karmada-system
      role: karmada-scheduler
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    name: karmada-scheduler-858b68f5b6-ql5sj
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-scheduler-858b68f5b6
      uid: 82826a3b-1ab6-47d5-ab11-285429364811
  spec:
    containers:
    - name: karmada-scheduler
      resources:
        limits:
          cpu: "1"
          memory: 250Mi
        requests:
          cpu: 11m
          memory: 64Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 858b68f5b6
        matchLabels:
          project: karmada-system
          role: karmada-scheduler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:12:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T18:12:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 86f7c86b5f
      product: foundation
      project: karmada-system
      role: karmada-webhook
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2b
    name: karmada-webhook-86f7c86b5f-9tmkk
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-webhook-86f7c86b5f
      uid: 49132c7b-9846-4638-8c0d-50a604efc30d
  spec:
    containers:
    - name: karmada-webhook
      resources:
        limits:
          cpu: "2"
          memory: 256Mi
        requests:
          cpu: 49m
          memory: 64Mi
    - name: cert-controller
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 128Mi
    initContainers:
    - name: cert-controller-initialize
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 86f7c86b5f
        matchLabels:
          project: karmada-system
          role: karmada-webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:57:09Z"
      observedGeneration: 144
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:56:58Z"
      observedGeneration: 144
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 86f7c86b5f
      product: foundation
      project: karmada-system
      role: karmada-webhook
      service: karmada-system
      team: compute
      topology.kubernetes.io/zone: us-west-2c
    name: karmada-webhook-86f7c86b5f-c9dh8
    namespace: karmada-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: karmada-webhook-86f7c86b5f
      uid: 49132c7b-9846-4638-8c0d-50a604efc30d
  spec:
    containers:
    - name: karmada-webhook
      resources:
        limits:
          cpu: "2"
          memory: 256Mi
        requests:
          cpu: 49m
          memory: 64Mi
    - name: cert-controller
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 128Mi
    initContainers:
    - name: cert-controller-initialize
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 86f7c86b5f
        matchLabels:
          project: karmada-system
          role: karmada-webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: webhook
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-admission
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v532
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      deploy_group: sandbox
      deploy_id: 01KG06D5M5W5691C9WESN7QWQG
      deploy_phase: phase-1
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: 56df87c4f4
      product: foundation
      project: kube-admission
      revision: 5987b411c1f41f09c108631cddde91d73ccbc5cd
      role: webhook
      service: kube-admission
      sidecar.istio.io/inject: "false"
      tag: v532
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-admission
      tags.datadoghq.com/version: v532
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG06D5M5W5691C9WESN7QWQG
      version: v532
    name: kube-admission-56df87c4f4-4xvjq
    namespace: kube-admission
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-admission-56df87c4f4
      uid: d870a848-a221-477c-88c8-a9838eec0183
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
          weight: 100
        - preference:
            matchExpressions:
            - key: node-type
              operator: In
              values:
              - node-arm
          weight: 50
    containers:
    - name: webhook
      resources:
        limits:
          cpu: "2"
          memory: 256Mi
        requests:
          cpu: 20m
          memory: 50Mi
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    - name: cert-controller
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 128Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T17:01:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T17:01:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: webhook
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-admission
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v532
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      deploy_group: sandbox
      deploy_id: 01KG06D5M5W5691C9WESN7QWQG
      deploy_phase: phase-1
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: 56df87c4f4
      product: foundation
      project: kube-admission
      revision: 5987b411c1f41f09c108631cddde91d73ccbc5cd
      role: webhook
      service: kube-admission
      sidecar.istio.io/inject: "false"
      tag: v532
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-admission
      tags.datadoghq.com/version: v532
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG06D5M5W5691C9WESN7QWQG
      version: v532
    name: kube-admission-56df87c4f4-rn62p
    namespace: kube-admission
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-admission-56df87c4f4
      uid: d870a848-a221-477c-88c8-a9838eec0183
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
          weight: 100
        - preference:
            matchExpressions:
            - key: node-type
              operator: In
              values:
              - node-arm
          weight: 50
    containers:
    - name: webhook
      resources:
        limits:
          cpu: "2"
          memory: 256Mi
        requests:
          cpu: 20m
          memory: 50Mi
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    - name: cert-controller
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 128Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T17:01:16Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T17:01:03Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: kube-custom-metrics.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: 00774a3a0cd8ddfaf4293f67b4e5d99b06cc3c25e577f077b2c5ae4a3fd9413
      apps.kubernetes.io/pod-index: "0"
      branch: main
      cicd-toolkit.zende.sk/config-revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      cicd-toolkit.zende.sk/config-version: v39
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: otel-collector-collector-699795cb4c
      criticality-tier: tier4
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/pre-stop-required: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      product: foundation
      project: kube-custom-metrics
      role: otel-collector
      service: kube-custom-metrics
      statefulset.kubernetes.io/pod-name: otel-collector-collector-0
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-custom-metrics-otel-collector
      tags.datadoghq.com/version: v39
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: otel-collector-collector-0
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: otel-collector-collector
      uid: 647d5241-d16c-45ab-88cd-1cfb090d4166
  spec:
    containers:
    - name: otc-container
      resources:
        limits:
          cpu: "1"
          memory: 2048M
        requests:
          cpu: 200m
          memory: 1024M
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          project: kube-custom-metrics
          role: otel-collector
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T18:56:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T18:55:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: kube-custom-metrics.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: 00774a3a0cd8ddfaf4293f67b4e5d99b06cc3c25e577f077b2c5ae4a3fd9413
      apps.kubernetes.io/pod-index: "1"
      branch: main
      cicd-toolkit.zende.sk/config-revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      cicd-toolkit.zende.sk/config-version: v39
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: otel-collector-collector-699795cb4c
      criticality-tier: tier4
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/pre-stop-required: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      product: foundation
      project: kube-custom-metrics
      role: otel-collector
      service: kube-custom-metrics
      statefulset.kubernetes.io/pod-name: otel-collector-collector-1
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-custom-metrics-otel-collector
      tags.datadoghq.com/version: v39
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: otel-collector-collector-1
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: otel-collector-collector
      uid: 647d5241-d16c-45ab-88cd-1cfb090d4166
  spec:
    containers:
    - name: otc-container
      resources:
        limits:
          cpu: "1"
          memory: 2048M
        requests:
          cpu: 200m
          memory: 1024M
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          project: kube-custom-metrics
          role: otel-collector
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opentelemetry-collector
      app.kubernetes.io/instance: kube-custom-metrics.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: 00774a3a0cd8ddfaf4293f67b4e5d99b06cc3c25e577f077b2c5ae4a3fd9413
      apps.kubernetes.io/pod-index: "2"
      branch: main
      cicd-toolkit.zende.sk/config-revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      cicd-toolkit.zende.sk/config-version: v39
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: otel-collector-collector-699795cb4c
      criticality-tier: tier4
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/pre-stop-required: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      product: foundation
      project: kube-custom-metrics
      role: otel-collector
      service: kube-custom-metrics
      statefulset.kubernetes.io/pod-name: otel-collector-collector-2
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-custom-metrics-otel-collector
      tags.datadoghq.com/version: v39
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: otel-collector-collector-2
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: otel-collector-collector
      uid: 647d5241-d16c-45ab-88cd-1cfb090d4166
  spec:
    containers:
    - name: otc-container
      resources:
        limits:
          cpu: "1"
          memory: 2048M
        requests:
          cpu: 200m
          memory: 1024M
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          project: kube-custom-metrics
          role: otel-collector
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:43Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opentelemetry-targetallocator
      app.kubernetes.io/instance: kube-custom-metrics.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: 5435661c1eb52e46c6810b8bc225b555ed96cb989a60096616addfd1c197957
      branch: main
      cicd-toolkit.zende.sk/config-revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      cicd-toolkit.zende.sk/config-version: v39
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/pre-stop-required: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      pod-template-hash: 5dc884f7c4
      product: foundation
      project: kube-custom-metrics
      role: otel-collector
      service: kube-custom-metrics
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-custom-metrics-otel-collector
      tags.datadoghq.com/version: v39
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: otel-collector-targetallocator-5dc884f7c4-2qzrh
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-collector-targetallocator-5dc884f7c4
      uid: fc4c845b-f38c-4f5e-8384-617ec0d72dbd
  spec:
    containers:
    - name: ta-container
      resources:
        limits:
          cpu: "1"
          memory: 2048M
        requests:
          cpu: 23m
          memory: 1024M
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5dc884f7c4
        matchLabels:
          app.kubernetes.io/component: opentelemetry-targetallocator
          app.kubernetes.io/instance: kube-custom-metrics.otel-collector
          app.kubernetes.io/managed-by: opentelemetry-operator
          app.kubernetes.io/name: kube-custom-metrics
          app.kubernetes.io/part-of: opentelemetry
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:08Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opentelemetry-targetallocator
      app.kubernetes.io/instance: kube-custom-metrics.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: 5435661c1eb52e46c6810b8bc225b555ed96cb989a60096616addfd1c197957
      branch: main
      cicd-toolkit.zende.sk/config-revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      cicd-toolkit.zende.sk/config-version: v39
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/pre-stop-required: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      pod-template-hash: 5dc884f7c4
      product: foundation
      project: kube-custom-metrics
      role: otel-collector
      service: kube-custom-metrics
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-custom-metrics-otel-collector
      tags.datadoghq.com/version: v39
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: otel-collector-targetallocator-5dc884f7c4-dz97l
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-collector-targetallocator-5dc884f7c4
      uid: fc4c845b-f38c-4f5e-8384-617ec0d72dbd
  spec:
    containers:
    - name: ta-container
      resources:
        limits:
          cpu: "1"
          memory: 2048M
        requests:
          cpu: 23m
          memory: 1024M
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5dc884f7c4
        matchLabels:
          app.kubernetes.io/component: opentelemetry-targetallocator
          app.kubernetes.io/instance: kube-custom-metrics.otel-collector
          app.kubernetes.io/managed-by: opentelemetry-operator
          app.kubernetes.io/name: kube-custom-metrics
          app.kubernetes.io/part-of: opentelemetry
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opentelemetry-targetallocator
      app.kubernetes.io/instance: kube-custom-metrics.otel-collector
      app.kubernetes.io/managed-by: opentelemetry-operator
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: opentelemetry
      app.kubernetes.io/version: 5435661c1eb52e46c6810b8bc225b555ed96cb989a60096616addfd1c197957
      branch: main
      cicd-toolkit.zende.sk/config-revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      cicd-toolkit.zende.sk/config-version: v39
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/pre-stop-required: "false"
      opa-gatekeeper.zendesk.com/valid-istio-service-port-name: "false"
      pod-template-hash: 5dc884f7c4
      product: foundation
      project: kube-custom-metrics
      role: otel-collector
      service: kube-custom-metrics
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-custom-metrics-otel-collector
      tags.datadoghq.com/version: v39
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: otel-collector-targetallocator-5dc884f7c4-vgkm8
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-collector-targetallocator-5dc884f7c4
      uid: fc4c845b-f38c-4f5e-8384-617ec0d72dbd
  spec:
    containers:
    - name: ta-container
      resources:
        limits:
          cpu: "1"
          memory: 2048M
        requests:
          cpu: 23m
          memory: 1024M
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5dc884f7c4
        matchLabels:
          app.kubernetes.io/component: opentelemetry-targetallocator
          app.kubernetes.io/instance: kube-custom-metrics.otel-collector
          app.kubernetes.io/managed-by: opentelemetry-operator
          app.kubernetes.io/name: kube-custom-metrics
          app.kubernetes.io/part-of: opentelemetry
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T18:55:59Z"
      observedGeneration: 2
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T18:55:56Z"
      observedGeneration: 2
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: prometheus-adapter
      app.kubernetes.io/instance: prometheus-adapter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-custom-metrics
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v39
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KG0CZMHGR5973QCV2V2795HT
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 8df5647d9
      product: foundation
      project: kube-custom-metrics
      revision: c6d35000023d65b02f7f82f99a5e634f9e97fc4a
      role: prometheus-adapter
      security.istio.io/tlsMode: istio
      service: kube-custom-metrics
      service.istio.io/canonical-name: kube-custom-metrics-prometheus-adapter
      service.istio.io/canonical-revision: v39
      sidecar.istio.io/inject: "true"
      tag: v39
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: kube-custom-metrics-prometheus-adapter
      tags.datadoghq.com/istio-proxy.version: v39
      tags.datadoghq.com/service: kube-custom-metrics-prometheus-adapter
      tags.datadoghq.com/version: v39
      team: compute
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0CZMHGR5973QCV2V2795HT
      version: v39
    name: prometheus-adapter-8df5647d9-rk7sk
    namespace: kube-custom-metrics
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-adapter-8df5647d9
      uid: 899351ea-b497-4922-ab54-afc9bddf2057
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-type
              operator: In
              values:
              - node-arm
          weight: 50
    containers:
    - name: aws-sigv4-proxy
      resources:
        limits:
          cpu: 200m
          memory: 300Mi
        requests:
          cpu: 11m
          memory: 150Mi
    - name: prometheus-adapter
      resources:
        limits:
          cpu: 500m
          memory: 300Mi
        requests:
          cpu: 11m
          memory: 200Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8df5647d9
        matchLabels:
          project: kube-custom-metrics
          role: prometheus-adapter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T18:56:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T18:55:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: compute-scheduler
      app.kubernetes.io/instance: compute-scheduler
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-scheduler
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v49
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/vpa-opt-in: "true"
      criticality-tier: tier0
      deploy_group: sandbox
      deploy_id: 01KF04V7MFD2NSHQB48X1S6R1K
      deploy_phase: phase-1
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: 77fb85cdfd
      product: foundation
      project: kube-scheduler
      revision: 905b60c3b01e46a22298111c2c76b56f9ba3b69f
      role: compute-scheduler
      service: kube-scheduler
      sidecar.istio.io/inject: "false"
      tag: v49
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-scheduler
      tags.datadoghq.com/version: v49
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KF04V7MFD2NSHQB48X1S6R1K
      version: v49
    name: kube-scheduler-77fb85cdfd-8dsms
    namespace: kube-scheduler
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-scheduler-77fb85cdfd
      uid: 0d7c6a0c-9409-4084-8c5b-929d4b312a9b
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: project
              operator: In
              values:
              - kube-scheduler
          topologyKey: kubernetes.io/hostname
    containers:
    - name: kube-scheduler
      resources:
        limits:
          cpu: "4"
          memory: 8Gi
        requests:
          cpu: 20m
          memory: 192Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/control-plane: control-plane
    priority: 2000000000
    priorityClassName: system-cluster-critical
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: node-role.kubernetes.compute.zende.sk/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 77fb85cdfd
        matchLabels:
          project: kube-scheduler
          role: compute-scheduler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:01Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: compute-scheduler
      app.kubernetes.io/instance: compute-scheduler
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-scheduler
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v49
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/vpa-opt-in: "true"
      criticality-tier: tier0
      deploy_group: sandbox
      deploy_id: 01KF04V7MFD2NSHQB48X1S6R1K
      deploy_phase: phase-1
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: 77fb85cdfd
      product: foundation
      project: kube-scheduler
      revision: 905b60c3b01e46a22298111c2c76b56f9ba3b69f
      role: compute-scheduler
      service: kube-scheduler
      sidecar.istio.io/inject: "false"
      tag: v49
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-scheduler
      tags.datadoghq.com/version: v49
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KF04V7MFD2NSHQB48X1S6R1K
      version: v49
    name: kube-scheduler-77fb85cdfd-zk564
    namespace: kube-scheduler
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-scheduler-77fb85cdfd
      uid: 0d7c6a0c-9409-4084-8c5b-929d4b312a9b
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: project
              operator: In
              values:
              - kube-scheduler
          topologyKey: kubernetes.io/hostname
    containers:
    - name: kube-scheduler
      resources:
        limits:
          cpu: "4"
          memory: 8Gi
        requests:
          cpu: 20m
          memory: 192Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    nodeSelector:
      node-role.kubernetes.io/control-plane: control-plane
    priority: 2000000000
    priorityClassName: system-cluster-critical
    tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: node-role.kubernetes.compute.zende.sk/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 77fb85cdfd
        matchLabels:
          project: kube-scheduler
          role: compute-scheduler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:57Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-smoke-test
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v339
      apps.kubernetes.io/pod-index: "0"
      branch: v339
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: kube-smoke-test-7f787c499
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KFXZK3FCR46E26TGTG0ZZSVT
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: foundation
      project: kube-smoke-test
      revision: e9f139254ecfc2c7def936342cb4d1a88348984e
      role: worker
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: kube-smoke-test
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: kube-smoke-test-0
      tag: v339
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-smoke-test-worker
      tags.datadoghq.com/version: v339
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFXZK3FCR46E26TGTG0ZZSVT
      version: v339
    name: kube-smoke-test-0
    namespace: kube-smoke-test-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kube-smoke-test
      uid: 55f25940-d46e-42be-9266-defad07f7e41
  spec:
    containers:
    - name: default
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 23m
          memory: 256M
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:17:29Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:17:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/termination-sensitive: "true"
      criticality-tier: tier4
      datadog_log_index: foundation
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      product: foundation
      project: kube-smoke-test
      role: test
      service: kube-smoke-test
      team: compute
      test: audit-logs-test-container-audit-logs
      topology.kubernetes.io/zone: us-west-2b
      worker: AuditLogs
    name: audit-logs-test-container-audit-logs
    namespace: kube-smoke-test
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: compute.zende.sk/termination-sensitive
              operator: In
              values:
              - "true"
          weight: 10
        - preference:
            matchExpressions:
            - key: compute.zendesk.com/ec2_image_sha
              operator: In
              values:
              - b7f462ec
          weight: 100
    containers:
    - name: test
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 50m
          memory: 50Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:32Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      apps.kubernetes.io/pod-index: "0"
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: backup-test-backup-54d49fb567
      criticality-tier: tier4
      datadog_log_index: foundation
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      product: foundation
      project: kube-smoke-test
      role: test
      service: kube-smoke-test
      statefulset.kubernetes.io/pod-name: backup-test-backup-0
      team: compute
      test: backup-test-backup
      topology.kubernetes.io/zone: us-west-2a
      worker: Backup
    name: backup-test-backup-0
    namespace: kube-smoke-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: backup-test-backup
      uid: 91102b0d-664b-419d-b4b2-e74e3c2cd462
  spec:
    containers:
    - name: test
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 50Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:34Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      apps.kubernetes.io/pod-index: "0"
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: horizontal-pod-autoscaling-test-internal-metrics-5fd77b98fd
      criticality-tier: tier4
      datadog_log_index: foundation
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      product: foundation
      project: kube-smoke-test
      role: test
      service: kube-smoke-test
      statefulset.kubernetes.io/pod-name: horizontal-pod-autoscaling-test-internal-metrics-0
      team: compute
      test: horizontal-pod-autoscaling-test-internal-metrics
      topology.kubernetes.io/zone: us-west-2b
      worker: HorizontalPodAutoscaling
    name: horizontal-pod-autoscaling-test-internal-metrics-0
    namespace: kube-smoke-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: horizontal-pod-autoscaling-test-internal-metrics
      uid: 78a6b01b-8f95-4509-8f20-55a6e77a9ae2
  spec:
    containers:
    - name: test
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 50Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:30Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      apps.kubernetes.io/pod-index: "1"
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: horizontal-pod-autoscaling-test-internal-metrics-5fd77b98fd
      criticality-tier: tier4
      datadog_log_index: foundation
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      product: foundation
      project: kube-smoke-test
      role: test
      service: kube-smoke-test
      statefulset.kubernetes.io/pod-name: horizontal-pod-autoscaling-test-internal-metrics-1
      team: compute
      test: horizontal-pod-autoscaling-test-internal-metrics
      topology.kubernetes.io/zone: us-west-2a
      worker: HorizontalPodAutoscaling
    name: horizontal-pod-autoscaling-test-internal-metrics-1
    namespace: kube-smoke-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: horizontal-pod-autoscaling-test-internal-metrics
      uid: 78a6b01b-8f95-4509-8f20-55a6e77a9ae2
  spec:
    containers:
    - name: test
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 50Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:32Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      datadog_log_index: foundation
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 64648fb55f
      product: foundation
      project: kube-smoke-test
      role: test
      service: kube-smoke-test
      team: compute
      test: scheduled-scaling-test-scheduled-scaling
      topology.kubernetes.io/zone: us-west-2b
      worker: ScheduledScaling
    name: scheduled-scaling-test-scheduled-scaling-64648fb55f-s4jkx
    namespace: kube-smoke-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: scheduled-scaling-test-scheduled-scaling-64648fb55f
      uid: 2405f458-93e2-40af-9943-2dbb8f2a27e4
  spec:
    containers:
    - name: test
      resources:
        limits:
          cpu: "1"
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 50Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 64648fb55f
        matchLabels:
          test: scheduled-scaling-test-scheduled-scaling
          worker: ScheduledScaling
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:43Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      apps.kubernetes.io/pod-index: "0"
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/vpa-opt-in: "true"
      controller-revision-hash: vpa-test-vpa-c5f95446d
      criticality-tier: tier4
      datadog_log_index: foundation
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      old-pod-evictor.compute.zende.sk/enabled: "true"
      product: foundation
      project: kube-smoke-test
      role: test
      service: kube-smoke-test
      statefulset.kubernetes.io/pod-name: vpa-test-vpa-0
      team: compute
      test: vpa-test-vpa
      topology.kubernetes.io/zone: us-west-2b
      worker: Vpa
    name: vpa-test-vpa-0
    namespace: kube-smoke-test
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: vpa-test-vpa
      uid: dc514598-b1fc-4f4a-a8ff-75c796a7a475
  spec:
    containers:
    - name: test
      resources:
        limits:
          cpu: 11m
          memory: 11500k
        requests:
          cpu: 11m
          memory: 11500k
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:46Z"
      message: 'containers with unready status: [test]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-snapscheduler
      app.kubernetes.io/part-of: dev-platform
      app.kubernetes.io/version: v19
      backube/snapscheduler-affinity: manager
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01KF0BQZB2B047X57WB6A2R553
      deploy_phase: phase-1
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 769cdbd6ff
      product: foundation
      project: kube-snapscheduler
      revision: 97fb483bed786898fbb254800d2215e079a1c6b2
      role: operator
      service: kube-snapscheduler
      sidecar.istio.io/inject: "false"
      tag: v19
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-snapscheduler-operator
      tags.datadoghq.com/version: v19
      team: compute
      topology.kubernetes.io/zone: us-west-2b
      track: 01KF0BQZB2B047X57WB6A2R553
      version: v19
    name: snapscheduler-769cdbd6ff-89kpp
    namespace: kube-snapscheduler
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: snapscheduler-769cdbd6ff
      uid: 515e059a-8c54-4640-a0fd-a33f91fe0356
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-type
              operator: In
              values:
              - node-arm
          weight: 50
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: backube/snapscheduler-affinity
                operator: In
                values:
                - manager
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: manager
      resources:
        limits:
          cpu: 100m
          memory: 160M
        requests:
          cpu: 11m
          memory: 80M
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 2000000000
    priorityClassName: system-cluster-critical
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 769cdbd6ff
        matchLabels:
          project: kube-snapscheduler
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:58:23Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:58:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: kube-snapscheduler
      app.kubernetes.io/part-of: dev-platform
      app.kubernetes.io/version: v19
      backube/snapscheduler-affinity: manager
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01KF0BQZB2B047X57WB6A2R553
      deploy_phase: phase-1
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 769cdbd6ff
      product: foundation
      project: kube-snapscheduler
      revision: 97fb483bed786898fbb254800d2215e079a1c6b2
      role: operator
      service: kube-snapscheduler
      sidecar.istio.io/inject: "false"
      tag: v19
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: kube-snapscheduler-operator
      tags.datadoghq.com/version: v19
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KF0BQZB2B047X57WB6A2R553
      version: v19
    name: snapscheduler-769cdbd6ff-tq4g8
    namespace: kube-snapscheduler
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: snapscheduler-769cdbd6ff
      uid: 515e059a-8c54-4640-a0fd-a33f91fe0356
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-type
              operator: In
              values:
              - node-arm
          weight: 50
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: backube/snapscheduler-affinity
                operator: In
                values:
                - manager
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: manager
      resources:
        limits:
          cpu: 100m
          memory: 160M
        requests:
          cpu: 11m
          memory: 80M
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    priority: 2000000000
    priorityClassName: system-cluster-critical
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 769cdbd6ff
        matchLabels:
          project: kube-snapscheduler
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:56:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:56:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/instance: lws
      app.kubernetes.io/name: lws
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 79c74bc64f
      topology.kubernetes.io/zone: us-west-2a
    name: lws-controller-manager-79c74bc64f-5sd5h
    namespace: lws-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: lws-controller-manager-79c74bc64f
      uid: 3720f625-eef4-4d51-be65-800ff5f2e26b
  spec:
    containers:
    - name: manager
      resources:
        requests:
          cpu: 11m
          memory: 1Gi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79c74bc64f
        matchLabels:
          app.kubernetes.io/instance: lws
          app.kubernetes.io/name: lws
          control-plane: controller-manager
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: init-checker
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      istio.io/dataplane-mode: ambient
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 6578fbb48c
      topology.kubernetes.io/zone: us-west-2b
    name: init-checker-6578fbb48c-4vzpz
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: init-checker-6578fbb48c
      uid: 1d1db7b8-75d0-44e4-9a01-ada084fd7e41
  spec:
    containers:
    - name: main-app
      resources: {}
    initContainers:
    - name: init-check
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6578fbb48c
        matchLabels:
          app: init-checker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:20Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: meshmon
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v3-11-55
      azIndex: "0"
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KE9XBS8PZ5P5ZMB3RX3X5YWA
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-hash: 6bf9c68949
      product: foundation
      project: meshmon
      revision: 188423f512fd2d52b1ea52e0ecef75e58d43a675
      role: app-server
      security.istio.io/tlsMode: istio
      service: meshmon
      service.istio.io/canonical-name: meshmon
      service.istio.io/canonical-revision: v3-11-55
      sidecar.istio.io/inject: "true"
      tag: v3-11-55
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: meshmon
      tags.datadoghq.com/istio-proxy.version: v3-11-55
      tags.datadoghq.com/service: meshmon
      tags.datadoghq.com/version: v3-11-55
      team: network
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2a
      track: 01KE9XBS8PZ5P5ZMB3RX3X5YWA
      version: v3-11-55
      zende.sk/dialtone: "true"
    name: meshmon-az-0-6bf9c68949-6jbbt
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshmon-az-0-6bf9c68949
      uid: e695fd08-92e8-4028-8b21-28bf997978c1
  spec:
    containers:
    - name: meshmon
      resources:
        limits:
          cpu: 50m
          memory: 1Gi
        requests:
          cpu: 15m
          memory: 150Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 20m
          memory: 150Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 20m
          memory: 150Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    nodeSelector:
      topology.kubernetes.io/zone: us-west-2a
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6bf9c68949
        matchLabels:
          azIndex: "0"
          project: meshmon
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: meshmon
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v3-11-55
      azIndex: "1"
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KE9XBS8PZ5P5ZMB3RX3X5YWA
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-hash: 55fdf95b99
      product: foundation
      project: meshmon
      revision: 188423f512fd2d52b1ea52e0ecef75e58d43a675
      role: app-server
      security.istio.io/tlsMode: istio
      service: meshmon
      service.istio.io/canonical-name: meshmon
      service.istio.io/canonical-revision: v3-11-55
      sidecar.istio.io/inject: "true"
      tag: v3-11-55
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: meshmon
      tags.datadoghq.com/istio-proxy.version: v3-11-55
      tags.datadoghq.com/service: meshmon
      tags.datadoghq.com/version: v3-11-55
      team: network
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01KE9XBS8PZ5P5ZMB3RX3X5YWA
      version: v3-11-55
      zende.sk/dialtone: "true"
    name: meshmon-az-1-55fdf95b99-rj9bk
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshmon-az-1-55fdf95b99
      uid: 9d7c13ac-d515-4d81-909f-c63d762c26a9
  spec:
    containers:
    - name: meshmon
      resources:
        limits:
          cpu: 50m
          memory: 1Gi
        requests:
          cpu: 15m
          memory: 150Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 20m
          memory: 150Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 20m
          memory: 150Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    nodeSelector:
      topology.kubernetes.io/zone: us-west-2b
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 55fdf95b99
        matchLabels:
          azIndex: "1"
          project: meshmon
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-24T13:31:23Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-24T13:31:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: meshmon
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v3-11-55
      azIndex: "2"
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KE9XBS8PZ5P5ZMB3RX3X5YWA
      deploy_phase: ""
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-hash: 594749fd7c
      product: foundation
      project: meshmon
      revision: 188423f512fd2d52b1ea52e0ecef75e58d43a675
      role: app-server
      security.istio.io/tlsMode: istio
      service: meshmon
      service.istio.io/canonical-name: meshmon
      service.istio.io/canonical-revision: v3-11-55
      sidecar.istio.io/inject: "true"
      tag: v3-11-55
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: meshmon
      tags.datadoghq.com/istio-proxy.version: v3-11-55
      tags.datadoghq.com/service: meshmon
      tags.datadoghq.com/version: v3-11-55
      team: network
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01KE9XBS8PZ5P5ZMB3RX3X5YWA
      version: v3-11-55
      zende.sk/dialtone: "true"
    name: meshmon-az-2-594749fd7c-8ft6b
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: meshmon-az-2-594749fd7c
      uid: 8f3a6904-2f64-4ad5-b784-a584cca5863b
  spec:
    containers:
    - name: meshmon
      resources:
        limits:
          cpu: 50m
          memory: 1Gi
        requests:
          cpu: 15m
          memory: 150Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 20m
          memory: 150Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 20m
          memory: 150Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    nodeSelector:
      topology.kubernetes.io/zone: us-west-2c
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 594749fd7c
        matchLabels:
          azIndex: "2"
          project: meshmon
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: persistent-server
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 75895c7db6
      topology.kubernetes.io/zone: us-west-2b
      version: v1
    name: persistent-server-75895c7db6-q5d77
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: persistent-server-75895c7db6
      uid: a0a47f1f-a861-44bb-a7e0-8034b8ed2b5c
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 64Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 75895c7db6
        matchLabels:
          app: persistent-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: sleep
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-0af7d5e8a274b2b88
      istio.io/dataplane-mode: ambient
      istio.io/use-waypoint: waypoint
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7dd5bdcc45
      product: foundation
      service: istio-control-plane
      team: network
      topology.kubernetes.io/zone: us-west-2b
    name: sleep-7dd5bdcc45-9jvdq
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sleep-7dd5bdcc45
      uid: 12de4496-de88-40b0-8c33-0f9406ae3c72
  spec:
    containers:
    - name: sleep
      resources: {}
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7dd5bdcc45
        matchLabels:
          app: sleep
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:19Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: sleep
      compute.zende.sk/ec2-lifecycle: on-demand
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 59497747fc
      security.istio.io/tlsMode: istio
      service.istio.io/canonical-name: sleep
      service.istio.io/canonical-revision: latest
      sidecar.istio.io/inject: "true"
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
    name: sleep-canary-59497747fc-z8smg
    namespace: meshmon
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sleep-canary-59497747fc
      uid: a1391ee7-1274-4e89-ade5-b5b00c97e99a
  spec:
    containers:
    - name: sleep
      resources: {}
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 59497747fc
        matchLabels:
          app: sleep
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: nodegroup-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v740-8f5b4d7104b597c8bd3d398e23a3229be879a0bb
      branch: v740
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      criticality-tier: tier4
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KF02300K9N3N3H70X6FRNHPG
      deploy_phase: phase-1
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: 6479465d8b
      product: foundation
      project: nodegroup-operator
      revision: 8f5b4d7104b597c8bd3d398e23a3229be879a0bb
      role: operator
      service: nodegroup-operator
      sidecar.istio.io/inject: "false"
      tag: v740
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: nodegroup-operator
      tags.datadoghq.com/version: v740-8f5b4d7104b597c8bd3d398e23a3229be879a0bb
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KF02300K9N3N3H70X6FRNHPG
      version: v740-8f5b4d7104b597c8bd3d398e23a3229be879a0bb
    name: nodegroup-operator-controller-manager-6479465d8b-8vbz7
    namespace: nodegroup-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nodegroup-operator-controller-manager-6479465d8b
      uid: 9dc67527-21be-4f8c-a56a-ba8695812e23
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.compute.zende.sk/control-plane
              operator: Exists
          - matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
          - matchExpressions:
            - key: node.kubernetes.io/role
              operator: In
              values:
              - master
              - control-plane
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: project
              operator: In
              values:
              - nodegroup-operator
          topologyKey: kubernetes.io/hostname
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 1024M
        requests:
          cpu: 35m
          memory: 512M
    - name: cert-controller
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 128Mi
    initContainers:
    - name: cert-controller-initialize
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: node-role.kubernetes.compute.zende.sk/control-plane
      operator: Exists
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/role
      operator: Equal
      value: master
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:29Z"
      observedGeneration: 50
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:05Z"
      observedGeneration: 50
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: nodegroup-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v740-8f5b4d7104b597c8bd3d398e23a3229be879a0bb
      branch: v740
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      criticality-tier: tier4
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KF02300K9N3N3H70X6FRNHPG
      deploy_phase: phase-1
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-hash: 6479465d8b
      product: foundation
      project: nodegroup-operator
      revision: 8f5b4d7104b597c8bd3d398e23a3229be879a0bb
      role: operator
      service: nodegroup-operator
      sidecar.istio.io/inject: "false"
      tag: v740
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: nodegroup-operator
      tags.datadoghq.com/version: v740-8f5b4d7104b597c8bd3d398e23a3229be879a0bb
      team: compute
      topology.kubernetes.io/zone: us-west-2a
      track: 01KF02300K9N3N3H70X6FRNHPG
      version: v740-8f5b4d7104b597c8bd3d398e23a3229be879a0bb
    name: nodegroup-operator-controller-manager-6479465d8b-sg28l
    namespace: nodegroup-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nodegroup-operator-controller-manager-6479465d8b
      uid: 9dc67527-21be-4f8c-a56a-ba8695812e23
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.compute.zende.sk/control-plane
              operator: Exists
          - matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
          - matchExpressions:
            - key: node.kubernetes.io/role
              operator: In
              values:
              - master
              - control-plane
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: project
              operator: In
              values:
              - nodegroup-operator
          topologyKey: kubernetes.io/hostname
    containers:
    - name: manager
      resources:
        limits:
          cpu: "2"
          memory: 1024M
        requests:
          cpu: 35m
          memory: 512M
    - name: cert-controller
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 128Mi
    initContainers:
    - name: cert-controller-initialize
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - key: node-role.kubernetes.compute.zende.sk/control-plane
      operator: Exists
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/role
      operator: Equal
      value: master
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:03Z"
      observedGeneration: 50
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:34Z"
      observedGeneration: 50
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: opencost
      app.kubernetes.io/instance: opencost
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: opencost
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v163
      branch: v163
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      datadog_log_index: foundation
      deploy_group: sandbox
      deploy_id: 01KF3N5JXJAYE93B6HVPWYNCFC
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 79bffd8b6f
      product: foundation
      project: opencost
      revision: b372522ad8478ca889b7a03c1c59c427a6f4c202
      role: opencost
      service: opencost
      sidecar.istio.io/inject: "false"
      tag: v163
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: opencost
      tags.datadoghq.com/version: v163
      team: optimize
      topology.kubernetes.io/zone: us-west-2b
      track: 01KF3N5JXJAYE93B6HVPWYNCFC
      version: v163
    name: opencost-79bffd8b6f-s7m9q
    namespace: opencost
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: opencost-79bffd8b6f
      uid: a02637a6-03fa-44bc-b790-a339a5b15129
  spec:
    containers:
    - name: opencost
      resources:
        limits:
          cpu: "16"
          memory: 8Gi
        requests:
          cpu: 23m
          memory: 500Mi
    - name: sigv4proxy
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 32Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79bffd8b6f
        matchLabels:
          project: opencost
          role: opencost
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:05:57Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:03:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: prometheus
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: opencost
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v163
      branch: v163
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KF3N5JXJAYE93B6HVPWYNCFC
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 6688db774b
      product: foundation
      project: opencost
      revision: b372522ad8478ca889b7a03c1c59c427a6f4c202
      role: prometheus
      service: opencost
      sidecar.istio.io/inject: "false"
      tag: v163
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: opencost-prometheus
      tags.datadoghq.com/version: v163
      team: optimize
      topology.kubernetes.io/zone: us-west-2b
      track: 01KF3N5JXJAYE93B6HVPWYNCFC
      version: v163
    name: prometheus-server-6688db774b-wj4rz
    namespace: opencost
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-server-6688db774b
      uid: 146fe7f3-8a2d-4201-a2c0-dc5fa9a76f51
  spec:
    containers:
    - name: prometheus-server
      resources:
        limits:
          cpu: "12"
          memory: 12Gi
        requests:
          cpu: 30m
          memory: 1Gi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T08:55:48Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T08:54:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-aggregation-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYQ8EESDCKZD7JF33T9NRE0
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 57b76fff46
      product: reliability
      project: otel-aggregation-service
      revision: 290806922d2a05f1427945586bdb88aa463aa255
      role: aggregator
      service: otel-aggregation-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-aggregation-service-aggregator
      tags.datadoghq.com/version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYQ8EESDCKZD7JF33T9NRE0
      version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
    name: aggregator-57b76fff46-7bz8t
    namespace: otel-aggregation-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: aggregator-57b76fff46
      uid: 901e5537-5bf1-483f-bd4f-b2cedb46ea5f
  spec:
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 250m
          memory: 256Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57b76fff46
        matchLabels:
          project: otel-aggregation-service
          role: aggregator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57b76fff46
        matchLabels:
          project: otel-aggregation-service
          role: aggregator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:05Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-aggregation-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYQ8EESDCKZD7JF33T9NRE0
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 57b76fff46
      product: reliability
      project: otel-aggregation-service
      revision: 290806922d2a05f1427945586bdb88aa463aa255
      role: aggregator
      service: otel-aggregation-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-aggregation-service-aggregator
      tags.datadoghq.com/version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYQ8EESDCKZD7JF33T9NRE0
      version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
    name: aggregator-57b76fff46-cb2sd
    namespace: otel-aggregation-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: aggregator-57b76fff46
      uid: 901e5537-5bf1-483f-bd4f-b2cedb46ea5f
  spec:
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 250m
          memory: 256Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57b76fff46
        matchLabels:
          project: otel-aggregation-service
          role: aggregator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57b76fff46
        matchLabels:
          project: otel-aggregation-service
          role: aggregator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-aggregation-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYQ8EESDCKZD7JF33T9NRE0
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 57b76fff46
      product: reliability
      project: otel-aggregation-service
      revision: 290806922d2a05f1427945586bdb88aa463aa255
      role: aggregator
      service: otel-aggregation-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-aggregation-service-aggregator
      tags.datadoghq.com/version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYQ8EESDCKZD7JF33T9NRE0
      version: sandbox-onboarding-290806922d2a05f1427945586bdb88aa463aa255
    name: aggregator-57b76fff46-fx8vd
    namespace: otel-aggregation-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: aggregator-57b76fff46
      uid: 901e5537-5bf1-483f-bd4f-b2cedb46ea5f
  spec:
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 250m
          memory: 256Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57b76fff46
        matchLabels:
          project: otel-aggregation-service
          role: aggregator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 57b76fff46
        matchLabels:
          project: otel-aggregation-service
          role: aggregator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:21Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:05Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-2mhfw
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-125.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:22:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-2x54z
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-157.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:30Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-4285t
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:59:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-022ad1b23bc585a34
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-47jxc
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-252.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-4f8wd
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-222.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-4vjvf
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-94.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-6pnk2
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-221.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-02ef8ae4559f58e80
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-7974z
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0f04554f198c692f3
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-cbj82
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-249.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-227-249.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:43Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-092f11a650f55f796
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-cfhqz
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-54.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:28Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-cn9qh
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-86.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:25Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-083870684c4116622
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-dfpfq
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-59.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-59.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-dnv58
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-44.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-03416309429e0355e
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-fr86r
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-180.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-180.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0e39ec8dde59fddd9
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-hftjm
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-71.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-226-71.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0b960eeb84dec48cf
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-jlkld
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-237.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-228-237.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:08:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-089f56965bd0d4ef1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-jzppv
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-207.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-227-207.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:56Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0cc30e7b1b8984d2d
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-jzvtd
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-153.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-227-153.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:00:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:00:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-08c30b7d86243ffe6
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-k5gdv
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-160.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-160.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:50:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-031d33af9d554291f
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-ksbnv
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-125.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-125.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:31Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:12Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-lcs8j
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-184.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:42Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:25Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-nrxxb
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-75.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:11:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:10:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-p9fgl
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-233.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:00Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:43Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-qswbl
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-5.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-rzh8r
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-219.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:45Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-s59b2
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-88.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:17Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:27:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-02eb8fb6af2c9b25e
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-shqgf
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-66.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-227-66.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-trbcz
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-48.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:16Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-v6thx
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:18Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:03:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-vt2rs
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-136.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:23Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-09203d47c303c0b85
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-wltnh
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-12.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:44:52Z"
      observedGeneration: 1
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T00:26:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-06745aa58d1ddcc0c
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-x8cvj
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-7.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:16:18Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:15:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-09604efed6dbf1fba
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-zr7lp
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-47.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-225-47.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:36Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: dbf97769
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRHTFX7XBH558N1QCZM9JB
      deploy_phase: ""
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "1"
      product: reliability
      project: otel-daemonset
      revision: 04ee723e016b3e9f72368676f176a7cc5ad487c1
      role: otel-collector
      service: otel-daemonset
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-daemonset-otel-collector
      tags.datadoghq.com/version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRHTFX7XBH558N1QCZM9JB
      version: sandbox-onboarding-04ee723e016b3e9f72368676f176a7cc5ad487c1
    name: otel-collector-zwlk2
    namespace: otel-daemonset
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: otel-collector
      uid: 0520c295-555c-4ba2-9a31-7752f0f61982
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-182.us-west-2.compute.internal
    containers:
    - name: collector
      resources:
        limits:
          cpu: 600m
          memory: 800Mi
        requests:
          cpu: 10m
          memory: 100Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-operator
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: fix-manifest-29e9d10e47ceb0a80d7e4832de0dec8a4afed598
      branch: fix-manifest
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KFWTQ8DMPK99HSFEV2VXP7ME
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-hash: 7496bf7bc8
      product: reliability
      project: otel-operator
      revision: 29e9d10e47ceb0a80d7e4832de0dec8a4afed598
      role: operator
      service: otel-operator
      sidecar.istio.io/inject: "false"
      tag: fix-manifest
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-operator-operator
      tags.datadoghq.com/version: fix-manifest-29e9d10e47ceb0a80d7e4832de0dec8a4afed598
      team: sre-observability
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFWTQ8DMPK99HSFEV2VXP7ME
      version: fix-manifest-29e9d10e47ceb0a80d7e4832de0dec8a4afed598
    name: opentelemetry-operator-controller-manager-7496bf7bc8-596c6
    namespace: otel-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: opentelemetry-operator-controller-manager-7496bf7bc8
      uid: 907dde39-0e81-4754-b8d1-b1a72d240fb4
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 256Mi
    - name: cert-controller
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 128Mi
    - name: kube-rbac-proxy
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 64Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7496bf7bc8
        matchLabels:
          app.kubernetes.io/name: otel-operator
          control-plane: controller-manager
          project: otel-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T09:39:50Z"
      observedGeneration: 2
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T09:39:19Z"
      observedGeneration: 2
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-operator
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: fix-manifest-29e9d10e47ceb0a80d7e4832de0dec8a4afed598
      branch: fix-manifest
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01KFWTQ8DMPK99HSFEV2VXP7ME
      deploy_phase: ""
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-hash: 7496bf7bc8
      product: reliability
      project: otel-operator
      revision: 29e9d10e47ceb0a80d7e4832de0dec8a4afed598
      role: operator
      service: otel-operator
      sidecar.istio.io/inject: "false"
      tag: fix-manifest
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-operator-operator
      tags.datadoghq.com/version: fix-manifest-29e9d10e47ceb0a80d7e4832de0dec8a4afed598
      team: sre-observability
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFWTQ8DMPK99HSFEV2VXP7ME
      version: fix-manifest-29e9d10e47ceb0a80d7e4832de0dec8a4afed598
    name: opentelemetry-operator-controller-manager-7496bf7bc8-r5lvr
    namespace: otel-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: opentelemetry-operator-controller-manager-7496bf7bc8
      uid: 907dde39-0e81-4754-b8d1-b1a72d240fb4
  spec:
    containers:
    - name: manager
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 256Mi
    - name: cert-controller
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 11m
          memory: 128Mi
    - name: kube-rbac-proxy
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 64Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7496bf7bc8
        matchLabels:
          app.kubernetes.io/name: otel-operator
          control-plane: controller-manager
          project: otel-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:23:17Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:23:01Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: tail-sampler
      app.kubernetes.io/instance: tail-sampler
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-tailsampling-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRAXG8ET3XCFK9CNYFGBKB
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 8f7757b9f
      product: reliability
      project: otel-tailsampling-service
      revision: bb0a8ae280cec6dffa59ded3581b549a5a280b21
      role: tail-sampler
      service: otel-tailsampling-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-tailsampling-service-tail-sampler
      tags.datadoghq.com/version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYRAXG8ET3XCFK9CNYFGBKB
      version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
    name: tail-sampler-8f7757b9f-bdzrk
    namespace: otel-tailsampling-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tail-sampler-8f7757b9f
      uid: f61fc357-93ef-436b-be89-59ba3b5ee46d
  spec:
    containers:
    - name: tail-sampler
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8f7757b9f
        matchLabels:
          project: otel-tailsampling-service
          role: tail-sampler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8f7757b9f
        matchLabels:
          project: otel-tailsampling-service
          role: tail-sampler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: tail-sampler
      app.kubernetes.io/instance: tail-sampler
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-tailsampling-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRAXG8ET3XCFK9CNYFGBKB
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 8f7757b9f
      product: reliability
      project: otel-tailsampling-service
      revision: bb0a8ae280cec6dffa59ded3581b549a5a280b21
      role: tail-sampler
      service: otel-tailsampling-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-tailsampling-service-tail-sampler
      tags.datadoghq.com/version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYRAXG8ET3XCFK9CNYFGBKB
      version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
    name: tail-sampler-8f7757b9f-nsc9r
    namespace: otel-tailsampling-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tail-sampler-8f7757b9f
      uid: f61fc357-93ef-436b-be89-59ba3b5ee46d
  spec:
    containers:
    - name: tail-sampler
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8f7757b9f
        matchLabels:
          project: otel-tailsampling-service
          role: tail-sampler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8f7757b9f
        matchLabels:
          project: otel-tailsampling-service
          role: tail-sampler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:26Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: tail-sampler
      app.kubernetes.io/instance: tail-sampler
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-tailsampling-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYRAXG8ET3XCFK9CNYFGBKB
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 8f7757b9f
      product: reliability
      project: otel-tailsampling-service
      revision: bb0a8ae280cec6dffa59ded3581b549a5a280b21
      role: tail-sampler
      service: otel-tailsampling-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-tailsampling-service-tail-sampler
      tags.datadoghq.com/version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYRAXG8ET3XCFK9CNYFGBKB
      version: sandbox-onboarding-bb0a8ae280cec6dffa59ded3581b549a5a280b21
    name: tail-sampler-8f7757b9f-tpcj6
    namespace: otel-tailsampling-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tail-sampler-8f7757b9f
      uid: f61fc357-93ef-436b-be89-59ba3b5ee46d
  spec:
    containers:
    - name: tail-sampler
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8f7757b9f
        matchLabels:
          project: otel-tailsampling-service
          role: tail-sampler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8f7757b9f
        matchLabels:
          project: otel-tailsampling-service
          role: tail-sampler
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-transporter
      app.kubernetes.io/instance: otel-transporter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-transport-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYR438T70CSTR5WBAC9H33V
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 54d6ff8c97
      product: reliability
      project: otel-transport-service
      revision: 6a4511c1833f2462b614da5b4903549cb31ccb65
      role: otel-transporter
      service: otel-transport-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-transport-service-otel-transporter
      tags.datadoghq.com/version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2b
      track: 01KDYR438T70CSTR5WBAC9H33V
      version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
    name: otel-transporter-54d6ff8c97-b7kcc
    namespace: otel-transport-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-transporter-54d6ff8c97
      uid: 798f5d32-bf90-4980-b080-bf150bf789b0
  spec:
    containers:
    - name: otel-transporter
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54d6ff8c97
        matchLabels:
          project: otel-transport-service
          role: otel-transporter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54d6ff8c97
        matchLabels:
          project: otel-transport-service
          role: otel-transporter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-transporter
      app.kubernetes.io/instance: otel-transporter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-transport-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYR438T70CSTR5WBAC9H33V
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 54d6ff8c97
      product: reliability
      project: otel-transport-service
      revision: 6a4511c1833f2462b614da5b4903549cb31ccb65
      role: otel-transporter
      service: otel-transport-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-transport-service-otel-transporter
      tags.datadoghq.com/version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2c
      track: 01KDYR438T70CSTR5WBAC9H33V
      version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
    name: otel-transporter-54d6ff8c97-cgxnx
    namespace: otel-transport-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-transporter-54d6ff8c97
      uid: 798f5d32-bf90-4980-b080-bf150bf789b0
  spec:
    containers:
    - name: otel-transporter
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54d6ff8c97
        matchLabels:
          project: otel-transport-service
          role: otel-transporter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54d6ff8c97
        matchLabels:
          project: otel-transport-service
          role: otel-transporter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: otel-transporter
      app.kubernetes.io/instance: otel-transporter
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: otel-transport-service
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
      branch: sandbox-onboarding
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KDYR438T70CSTR5WBAC9H33V
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 54d6ff8c97
      product: reliability
      project: otel-transport-service
      revision: 6a4511c1833f2462b614da5b4903549cb31ccb65
      role: otel-transporter
      service: otel-transport-service
      sidecar.istio.io/inject: "false"
      tag: sandbox-onboarding
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: otel-transport-service-otel-transporter
      tags.datadoghq.com/version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
      team: sre-core-apac
      topology.kubernetes.io/zone: us-west-2a
      track: 01KDYR438T70CSTR5WBAC9H33V
      version: sandbox-onboarding-6a4511c1833f2462b614da5b4903549cb31ccb65
    name: otel-transporter-54d6ff8c97-gdns9
    namespace: otel-transport-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-transporter-54d6ff8c97
      uid: 798f5d32-bf90-4980-b080-bf150bf789b0
  spec:
    containers:
    - name: otel-transporter
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    nodeSelector:
      node-type: node-arm
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54d6ff8c97
        matchLabels:
          project: otel-transport-service
          role: otel-transporter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeTaintsPolicy: Honor
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 54d6ff8c97
        matchLabels:
          project: otel-transport-service
          role: otel-transporter
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:26Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: metrics-collector
      app.kubernetes.io/instance: metrics-collector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 748b9565f6
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: metrics-collector
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-metrics-collector
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
    name: metrics-collector-748b9565f6-6nblh
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-collector-748b9565f6
      uid: e71034b9-63d5-4557-b35c-c64597b5d10e
  spec:
    containers:
    - name: secret-service-api
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 20m
          memory: 256Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 748b9565f6
        matchLabels:
          project: secret-service-api
          role: metrics-collector
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T20:55:50Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T20:55:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: replication-worker
      app.kubernetes.io/instance: replication-worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 5fb5746cbf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: replication-worker
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-replication-worker
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: replication-worker-5fb5746cbf-fg7t7
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: replication-worker-5fb5746cbf
      uid: 80475da8-ba3f-4c68-b2ef-c9d3f63ab65b
  spec:
    containers:
    - name: replication-worker
      resources:
        limits:
          cpu: "5"
          memory: 1536Mi
        requests:
          cpu: 11m
          memory: 800Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5fb5746cbf
        matchLabels:
          project: secret-service-api
          role: replication-worker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: replication-worker
      app.kubernetes.io/instance: replication-worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 5fb5746cbf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: replication-worker
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-replication-worker
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: replication-worker-5fb5746cbf-vt6xd
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: replication-worker-5fb5746cbf
      uid: 80475da8-ba3f-4c68-b2ef-c9d3f63ab65b
  spec:
    containers:
    - name: replication-worker
      resources:
        limits:
          cpu: "5"
          memory: 1536Mi
        requests:
          cpu: 11m
          memory: 800Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5fb5746cbf
        matchLabels:
          project: secret-service-api
          role: replication-worker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:25:10Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: replication-worker
      app.kubernetes.io/instance: replication-worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 5fb5746cbf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: replication-worker
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-replication-worker
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: replication-worker-5fb5746cbf-zk7wb
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: replication-worker-5fb5746cbf
      uid: 80475da8-ba3f-4c68-b2ef-c9d3f63ab65b
  spec:
    containers:
    - name: replication-worker
      resources:
        limits:
          cpu: "5"
          memory: 1536Mi
        requests:
          cpu: 11m
          memory: 800Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5fb5746cbf
        matchLabels:
          project: secret-service-api
          role: replication-worker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 79f58cf6cf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server
      security.istio.io/tlsMode: istio
      service: secret-service-api
      service.istio.io/canonical-name: secret-service-api-server
      service.istio.io/canonical-revision: v1-0-555
      sidecar.istio.io/inject: "true"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-api-server
      tags.datadoghq.com/istio-proxy.version: v1-0-555
      tags.datadoghq.com/service: secret-service-api-server
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-79f58cf6cf-6b79k
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-79f58cf6cf
      uid: 4314d38b-1144-4fd8-956c-289e352366d3
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: 7500m
          memory: 3Gi
        requests:
          cpu: 25m
          memory: 1536Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79f58cf6cf
        matchLabels:
          project: secret-service-api
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:42Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 79f58cf6cf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server
      security.istio.io/tlsMode: istio
      service: secret-service-api
      service.istio.io/canonical-name: secret-service-api-server
      service.istio.io/canonical-revision: v1-0-555
      sidecar.istio.io/inject: "true"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-api-server
      tags.datadoghq.com/istio-proxy.version: v1-0-555
      tags.datadoghq.com/service: secret-service-api-server
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-79f58cf6cf-6kz4n
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-79f58cf6cf
      uid: 4314d38b-1144-4fd8-956c-289e352366d3
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: 7500m
          memory: 3Gi
        requests:
          cpu: 25m
          memory: 1536Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79f58cf6cf
        matchLabels:
          project: secret-service-api
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:26:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:25:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 79f58cf6cf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server
      security.istio.io/tlsMode: istio
      service: secret-service-api
      service.istio.io/canonical-name: secret-service-api-server
      service.istio.io/canonical-revision: v1-0-555
      sidecar.istio.io/inject: "true"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-api-server
      tags.datadoghq.com/istio-proxy.version: v1-0-555
      tags.datadoghq.com/service: secret-service-api-server
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-79f58cf6cf-j5sd2
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-79f58cf6cf
      uid: 4314d38b-1144-4fd8-956c-289e352366d3
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: 7500m
          memory: 3Gi
        requests:
          cpu: 25m
          memory: 1536Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79f58cf6cf
        matchLabels:
          project: secret-service-api
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 79f58cf6cf
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server
      security.istio.io/tlsMode: istio
      service: secret-service-api
      service.istio.io/canonical-name: secret-service-api-server
      service.istio.io/canonical-revision: v1-0-555
      sidecar.istio.io/inject: "true"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-api-server
      tags.datadoghq.com/istio-proxy.version: v1-0-555
      tags.datadoghq.com/service: secret-service-api-server
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-79f58cf6cf-wh4qc
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-79f58cf6cf
      uid: 4314d38b-1144-4fd8-956c-289e352366d3
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: 7500m
          memory: 3Gi
        requests:
          cpu: 25m
          memory: 1536Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 79f58cf6cf
        matchLabels:
          project: secret-service-api
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:04:56Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:04:36Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server-main
      app.kubernetes.io/instance: server-main
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7b7848849
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server-main
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-server-main
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-main-7b7848849-6f7kz
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-main-7b7848849
      uid: 81fd8216-4516-4405-9ace-59bca7c289dc
  spec:
    containers:
    - name: server-main
      resources:
        limits:
          cpu: 7500m
          memory: 2Gi
        requests:
          cpu: 25m
          memory: 512Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7b7848849
        matchLabels:
          project: secret-service-api
          role: server-main
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:20Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:32:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server-main
      app.kubernetes.io/instance: server-main
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7b7848849
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server-main
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-server-main
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-main-7b7848849-cvj8d
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-main-7b7848849
      uid: 81fd8216-4516-4405-9ace-59bca7c289dc
  spec:
    containers:
    - name: server-main
      resources:
        limits:
          cpu: 7500m
          memory: 2Gi
        requests:
          cpu: 25m
          memory: 512Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7b7848849
        matchLabels:
          project: secret-service-api
          role: server-main
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:08:32Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:08:21Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server-main
      app.kubernetes.io/instance: server-main
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7b7848849
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server-main
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-server-main
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-main-7b7848849-sg69f
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-main-7b7848849
      uid: 81fd8216-4516-4405-9ace-59bca7c289dc
  spec:
    containers:
    - name: server-main
      resources:
        limits:
          cpu: 7500m
          memory: 2Gi
        requests:
          cpu: 25m
          memory: 512Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7b7848849
        matchLabels:
          project: secret-service-api
          role: server-main
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:41:04Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:40:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server-main
      app.kubernetes.io/instance: server-main
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 7b7848849
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server-main
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-server-main
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-main-7b7848849-t2qpv
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-main-7b7848849
      uid: 81fd8216-4516-4405-9ace-59bca7c289dc
  spec:
    containers:
    - name: server-main
      resources:
        limits:
          cpu: 7500m
          memory: 2Gi
        requests:
          cpu: 25m
          memory: 512Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7b7848849
        matchLabels:
          project: secret-service-api
          role: server-main
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:22:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:22:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server-main
      app.kubernetes.io/instance: server-main
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7b7848849
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: server-main
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-server-main
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: server-main-7b7848849-vglhp
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-main-7b7848849
      uid: 81fd8216-4516-4405-9ace-59bca7c289dc
  spec:
    containers:
    - name: server-main
      resources:
        limits:
          cpu: 7500m
          memory: 2Gi
        requests:
          cpu: 25m
          memory: 512Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7b7848849
        matchLabels:
          project: secret-service-api
          role: server-main
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:24Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:37:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vault-main-writer-worker
      app.kubernetes.io/instance: vault-main-writer-worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 869d95cbd4
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: vault-main-writer-worker
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-vault-main-writer-worker
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: vault-main-writer-worker-869d95cbd4-56kdz
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: vault-main-writer-worker-869d95cbd4
      uid: f4ed3c73-77c9-4fa7-a3e8-2fd5261dba61
  spec:
    containers:
    - name: vault-main-writer-worker
      resources:
        limits:
          cpu: "5"
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 896Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 869d95cbd4
        matchLabels:
          project: secret-service-api
          role: vault-main-writer-worker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:25:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vault-main-writer-worker
      app.kubernetes.io/instance: vault-main-writer-worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 869d95cbd4
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: vault-main-writer-worker
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-vault-main-writer-worker
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: vault-main-writer-worker-869d95cbd4-756hl
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: vault-main-writer-worker-869d95cbd4
      uid: f4ed3c73-77c9-4fa7-a3e8-2fd5261dba61
  spec:
    containers:
    - name: vault-main-writer-worker
      resources:
        limits:
          cpu: "5"
          memory: 2Gi
        requests:
          cpu: 23m
          memory: 896Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 869d95cbd4
        matchLabels:
          project: secret-service-api
          role: vault-main-writer-worker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:13:22Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vault-main-writer-worker
      app.kubernetes.io/instance: vault-main-writer-worker
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-api
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-555
      branch: v1-0-555
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier1
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KG0KTB8DR254F6CZFS1CZPDS
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 869d95cbd4
      product: foundation
      project: secret-service-api
      revision: 1978e259145e5205d03c52910819f20534b00342
      role: vault-main-writer-worker
      service: secret-service-api
      sidecar.istio.io/inject: "false"
      tag: v1-0-555
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-api-vault-main-writer-worker
      tags.datadoghq.com/version: v1-0-555
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG0KTB8DR254F6CZFS1CZPDS
      version: v1-0-555
      zende.sk/dialtone: "true"
    name: vault-main-writer-worker-869d95cbd4-rzwfm
    namespace: secret-service-api
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: vault-main-writer-worker-869d95cbd4
      uid: f4ed3c73-77c9-4fa7-a3e8-2fd5261dba61
  spec:
    containers:
    - name: vault-main-writer-worker
      resources:
        limits:
          cpu: "5"
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 896Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 869d95cbd4
        matchLabels:
          project: secret-service-api
          role: vault-main-writer-worker
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: ui
      app.kubernetes.io/instance: ui
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-ui
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v345-0-407
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KFWQM14BBSTE7FWA1RE7FKWP
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7c8dccdccb
      product: foundation
      project: secret-service-ui
      revision: 6a13c814a84facf5172a327e20644f26cd90a6e5
      role: ui
      security.istio.io/tlsMode: istio
      service: secret-service-ui
      service.istio.io/canonical-name: secret-service-ui-ui
      service.istio.io/canonical-revision: v345-0-407
      sidecar.istio.io/inject: "true"
      tag: v345-0-407
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-ui-ui
      tags.datadoghq.com/istio-proxy.version: v345-0-407
      tags.datadoghq.com/service: secret-service-ui-ui
      tags.datadoghq.com/version: v345-0-407
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFWQM14BBSTE7FWA1RE7FKWP
      version: v345-0-407
    name: ui-7c8dccdccb-k428d
    namespace: secret-service-ui
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ui-7c8dccdccb
      uid: 0d813340-e160-4157-8cfa-36d2fee01a73
  spec:
    containers:
    - name: ui
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7c8dccdccb
        matchLabels:
          project: secret-service-ui
          role: ui
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:50Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: ui
      app.kubernetes.io/instance: ui
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-ui
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v345-0-407
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KFWQM14BBSTE7FWA1RE7FKWP
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 7c8dccdccb
      product: foundation
      project: secret-service-ui
      revision: 6a13c814a84facf5172a327e20644f26cd90a6e5
      role: ui
      security.istio.io/tlsMode: istio
      service: secret-service-ui
      service.istio.io/canonical-name: secret-service-ui-ui
      service.istio.io/canonical-revision: v345-0-407
      sidecar.istio.io/inject: "true"
      tag: v345-0-407
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-ui-ui
      tags.datadoghq.com/istio-proxy.version: v345-0-407
      tags.datadoghq.com/service: secret-service-ui-ui
      tags.datadoghq.com/version: v345-0-407
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFWQM14BBSTE7FWA1RE7FKWP
      version: v345-0-407
    name: ui-7c8dccdccb-pn7vg
    namespace: secret-service-ui
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ui-7c8dccdccb
      uid: 0d813340-e160-4157-8cfa-36d2fee01a73
  spec:
    containers:
    - name: ui
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7c8dccdccb
        matchLabels:
          project: secret-service-ui
          role: ui
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:17:10Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:17:06Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: ui
      app.kubernetes.io/instance: ui
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-ui
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v345-0-407
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KFWQM14BBSTE7FWA1RE7FKWP
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7c8dccdccb
      product: foundation
      project: secret-service-ui
      revision: 6a13c814a84facf5172a327e20644f26cd90a6e5
      role: ui
      security.istio.io/tlsMode: istio
      service: secret-service-ui
      service.istio.io/canonical-name: secret-service-ui-ui
      service.istio.io/canonical-revision: v345-0-407
      sidecar.istio.io/inject: "true"
      tag: v345-0-407
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: secret-service-ui-ui
      tags.datadoghq.com/istio-proxy.version: v345-0-407
      tags.datadoghq.com/service: secret-service-ui-ui
      tags.datadoghq.com/version: v345-0-407
      team: lockbox
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFWQM14BBSTE7FWA1RE7FKWP
      version: v345-0-407
    name: ui-7c8dccdccb-xjt8n
    namespace: secret-service-ui
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: ui-7c8dccdccb
      uid: 0d813340-e160-4157-8cfa-36d2fee01a73
  spec:
    containers:
    - name: ui
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7c8dccdccb
        matchLabels:
          project: secret-service-ui
          role: ui
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:13Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: webhook
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-admission-controller
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-209
      branch: v1-0-209
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KE9NXK4M8S4WPGHKRFNKP8Z8
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7ffdf4f644
      product: foundation
      project: secret-service-v2-admission-controller
      revision: d657e1106ed31c6f5b39ff62f1dfd65f1b30c0f6
      role: webhook
      service: secret-service-v2-admission-controller
      sidecar.istio.io/inject: "false"
      tag: v1-0-209
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-admission-controller-webhook
      tags.datadoghq.com/version: v1-0-209
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01KE9NXK4M8S4WPGHKRFNKP8Z8
      version: v1-0-209
      zende.sk/dialtone: "true"
    name: webhook-7ffdf4f644-btgkj
    namespace: secret-service-v2-admission-controller
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webhook-7ffdf4f644
      uid: 888147de-8491-45ee-a3cf-46e528a404e8
  spec:
    containers:
    - name: webhook
      resources:
        limits:
          cpu: 300m
          memory: 256Mi
        requests:
          cpu: 25m
          memory: 64Mi
    initContainers:
    - name: cert-controller
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7ffdf4f644
        matchLabels:
          project: secret-service-v2-admission-controller
          role: webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: webhook
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-admission-controller
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-209
      branch: v1-0-209
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KE9NXK4M8S4WPGHKRFNKP8Z8
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7ffdf4f644
      product: foundation
      project: secret-service-v2-admission-controller
      revision: d657e1106ed31c6f5b39ff62f1dfd65f1b30c0f6
      role: webhook
      service: secret-service-v2-admission-controller
      sidecar.istio.io/inject: "false"
      tag: v1-0-209
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-admission-controller-webhook
      tags.datadoghq.com/version: v1-0-209
      team: lockbox
      topology.kubernetes.io/zone: us-west-2b
      track: 01KE9NXK4M8S4WPGHKRFNKP8Z8
      version: v1-0-209
      zende.sk/dialtone: "true"
    name: webhook-7ffdf4f644-j7j76
    namespace: secret-service-v2-admission-controller
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webhook-7ffdf4f644
      uid: 888147de-8491-45ee-a3cf-46e528a404e8
  spec:
    containers:
    - name: webhook
      resources:
        limits:
          cpu: 300m
          memory: 256Mi
        requests:
          cpu: 25m
          memory: 64Mi
    initContainers:
    - name: cert-controller
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7ffdf4f644
        matchLabels:
          project: secret-service-v2-admission-controller
          role: webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T23:54:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T23:54:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: webhook
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-admission-controller
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-209
      branch: v1-0-209
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01KE9NXK4M8S4WPGHKRFNKP8Z8
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 7ffdf4f644
      product: foundation
      project: secret-service-v2-admission-controller
      revision: d657e1106ed31c6f5b39ff62f1dfd65f1b30c0f6
      role: webhook
      service: secret-service-v2-admission-controller
      sidecar.istio.io/inject: "false"
      tag: v1-0-209
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-admission-controller-webhook
      tags.datadoghq.com/version: v1-0-209
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KE9NXK4M8S4WPGHKRFNKP8Z8
      version: v1-0-209
      zende.sk/dialtone: "true"
    name: webhook-7ffdf4f644-kcpsv
    namespace: secret-service-v2-admission-controller
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webhook-7ffdf4f644
      uid: 888147de-8491-45ee-a3cf-46e528a404e8
  spec:
    containers:
    - name: webhook
      resources:
        limits:
          cpu: 300m
          memory: 256Mi
        requests:
          cpu: 25m
          memory: 64Mi
    initContainers:
    - name: cert-controller
      resources:
        limits:
          cpu: 200m
          memory: 500Mi
        requests:
          cpu: 25m
          memory: 128Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7ffdf4f644
        matchLabels:
          project: secret-service-v2-admission-controller
          role: webhook
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-49
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-secure
      deploy_group: sandbox
      deploy_id: 01K9SJEE0J1CNQ4WN4TN66MAKF
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      name: secret-service-v2-operator
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 64f76fb998
      product: foundation
      project: secret-service-v2-operator
      revision: 3ce45144a17325d0cbf89280e1d3d5fabb37a104
      role: operator
      service: secret-service-v2-operator
      sidecar.istio.io/inject: "false"
      tag: v1-0-49
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-operator-operator
      tags.datadoghq.com/version: v1-0-49
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01K9SJEE0J1CNQ4WN4TN66MAKF
      usespot.compute.zende.sk/opt-out: "true"
      version: v1-0-49
      zende.sk/dialtone: "true"
    name: secret-service-v2-operator-64f76fb998-4smlh
    namespace: secret-service-v2-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: secret-service-v2-operator-64f76fb998
      uid: 1f3e164d-3b38-4746-83de-eafb5a81814a
  spec:
    containers:
    - name: secret-service-v2-operator
      resources:
        limits:
          cpu: "4"
          memory: 768Mi
        requests:
          cpu: 11m
          memory: 256Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 64f76fb998
        matchLabels:
          name: secret-service-v2-operator
          project: secret-service-v2-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T21:08:22Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T21:08:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: redis
      app.kubernetes.io/instance: redis
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-redis
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-50
      branch: v1-0-50
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KE9NVSTM593KXMR4C2M8DR89
      deploy_phase: ""
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 55696976b8
      product: foundation
      project: secret-service-v2-redis
      revision: 4eaa1700790fdf75223e6ae9712c6897cee312ff
      role: redis
      service: secret-service-v2-redis
      sidecar.istio.io/inject: "false"
      tag: v1-0-50
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-redis-redis
      tags.datadoghq.com/version: v1-0-50
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KE9NVSTM593KXMR4C2M8DR89
      version: v1-0-50
      zende.sk/dialtone: "true"
    name: redis-55696976b8-5kf9d
    namespace: secret-service-v2-redis
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-55696976b8
      uid: 602ee154-4079-4269-bccd-285595521df4
  spec:
    containers:
    - name: redis
      resources:
        limits:
          cpu: 2048m
          memory: 1Gi
        requests:
          cpu: 23m
          memory: 768Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 55696976b8
        matchLabels:
          project: secret-service-v2-redis
          role: redis
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:34:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: redis-main
      app.kubernetes.io/instance: redis-main
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-redis
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v1-0-50
      branch: v1-0-50
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KE9NVSTM593KXMR4C2M8DR89
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 684855bc46
      product: foundation
      project: secret-service-v2-redis
      revision: 4eaa1700790fdf75223e6ae9712c6897cee312ff
      role: redis-main
      service: secret-service-v2-redis
      sidecar.istio.io/inject: "false"
      tag: v1-0-50
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-redis-redis-main
      tags.datadoghq.com/version: v1-0-50
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01KE9NVSTM593KXMR4C2M8DR89
      version: v1-0-50
      zende.sk/dialtone: "true"
    name: redis-main-684855bc46-brlht
    namespace: secret-service-v2-redis
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-main-684855bc46
      uid: f6d8acbb-ad13-412f-8eb5-36b2a8803468
  spec:
    containers:
    - name: redis-main
      resources:
        limits:
          cpu: 2048m
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 768Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 684855bc46
        matchLabels:
          project: secret-service-v2-redis
          role: redis-main
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T23:54:53Z"
      observedGeneration: 4
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T23:54:41Z"
      observedGeneration: 4
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      branch: rbayerl-k8s
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K6N1VWMM7YPB7HDY0MQRDPYH
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: f99df5645
      product: foundation
      project: secret-service-v2-truth-service
      revision: 911c7fcba6941cc71c791647a80c90f6bbcd4117
      role: server
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api: "true"
      secret-service.zende.sk/sidecar-api-port: "9999"
      secret-service.zende.sk/versioning_flag: "true"
      service: secret-service-v2-truth-service
      sidecar.istio.io/inject: "false"
      tag: rbayerl-k8s
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-truth-service-server
      tags.datadoghq.com/version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      team: lockbox
      topology.kubernetes.io/zone: us-west-2b
      track: 01K6N1VWMM7YPB7HDY0MQRDPYH
      version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      zende.sk/dialtone: "true"
    name: server-f99df5645-57mq5
    namespace: secret-service-v2-truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-f99df5645
      uid: 27d79394-4fb9-433b-8d83-7495237d8bc4
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: "1"
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 50Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - f99df5645
        matchLabels:
          project: secret-service-v2-truth-service
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:48:44Z"
      message: 'containers with unready status: [secret-sidecar server]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:48:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      branch: rbayerl-k8s
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K6N1VWMM7YPB7HDY0MQRDPYH
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: f99df5645
      product: foundation
      project: secret-service-v2-truth-service
      revision: 911c7fcba6941cc71c791647a80c90f6bbcd4117
      role: server
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api: "true"
      secret-service.zende.sk/sidecar-api-port: "9999"
      secret-service.zende.sk/versioning_flag: "true"
      service: secret-service-v2-truth-service
      sidecar.istio.io/inject: "false"
      tag: rbayerl-k8s
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-truth-service-server
      tags.datadoghq.com/version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      team: lockbox
      topology.kubernetes.io/zone: us-west-2a
      track: 01K6N1VWMM7YPB7HDY0MQRDPYH
      version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      zende.sk/dialtone: "true"
    name: server-f99df5645-h4f7c
    namespace: secret-service-v2-truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-f99df5645
      uid: 27d79394-4fb9-433b-8d83-7495237d8bc4
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: "1"
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 50Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - f99df5645
        matchLabels:
          project: secret-service-v2-truth-service
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:40:44Z"
      message: 'containers with unready status: [secret-sidecar server]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:40:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: server
      app.kubernetes.io/instance: server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: secret-service-v2-truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      branch: rbayerl-k8s
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: unknown
      deploy_group: sandbox
      deploy_id: 01K6N1VWMM7YPB7HDY0MQRDPYH
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: f99df5645
      product: foundation
      project: secret-service-v2-truth-service
      revision: 911c7fcba6941cc71c791647a80c90f6bbcd4117
      role: server
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api: "true"
      secret-service.zende.sk/sidecar-api-port: "9999"
      secret-service.zende.sk/versioning_flag: "true"
      service: secret-service-v2-truth-service
      sidecar.istio.io/inject: "false"
      tag: rbayerl-k8s
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: secret-service-v2-truth-service-server
      tags.datadoghq.com/version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      team: lockbox
      topology.kubernetes.io/zone: us-west-2c
      track: 01K6N1VWMM7YPB7HDY0MQRDPYH
      version: rbayerl-k8s-911c7fcba6941cc71c791647a80c90f6bbcd4117
      zende.sk/dialtone: "true"
    name: server-f99df5645-z2xkl
    namespace: secret-service-v2-truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: server-f99df5645
      uid: 27d79394-4fb9-433b-8d83-7495237d8bc4
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: "1"
          memory: 128Mi
        requests:
          cpu: 11m
          memory: 50Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: normal
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - f99df5645
        matchLabels:
          project: secret-service-v2-truth-service
          role: server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:50:44Z"
      message: 'containers with unready status: [secret-sidecar server]'
      observedGeneration: 1
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T04:50:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: storage-request-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: yelshall-dialtone-2a7312c797b424ac132ca9fd2a230fd48d9ee17c
      branch: yelshall-dialtone
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-storage
      deploy_group: sandbox
      deploy_id: 01J4J3NKF1DRJ5DRNJHFCJMJXQ
      deploy_phase: phase-sandbox
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 7799ff5449
      product: foundation
      project: storage-request-operator
      revision: 2a7312c797b424ac132ca9fd2a230fd48d9ee17c
      role: operator
      service: storage-request-operator
      sidecar.istio.io/inject: "false"
      tag: yelshall-dialtone
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: storage-request-operator
      tags.datadoghq.com/version: yelshall-dialtone-2a7312c797b424ac132ca9fd2a230fd48d9ee17c
      team: emporium
      topology.kubernetes.io/zone: us-west-2b
      track: 01J4J3NKF1DRJ5DRNJHFCJMJXQ
      version: yelshall-dialtone-2a7312c797b424ac132ca9fd2a230fd48d9ee17c
      zende.sk/dialtone: "true"
    name: storage-request-operator-7799ff5449-g7l4s
    namespace: storage-request-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: storage-request-operator-7799ff5449
      uid: c459ce1c-976f-45ab-b622-14129886adfc
  spec:
    containers:
    - name: operator
      resources:
        limits:
          cpu: 500m
          memory: 400Mi
        requests:
          cpu: 11m
          memory: 300Mi
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 7799ff5449
        matchLabels:
          project: storage-request-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:45:11Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:45:06Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-03416309429e0355e
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-2gd8q
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-180.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-180.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-2rtbz
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-125.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:22:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-2sgfr
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 195Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:03:55Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-6ctl7
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-233.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 256m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-02eb8fb6af2c9b25e
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-6lnrv
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-66.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-227-66.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:20Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-7zhg6
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-75.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:11:16Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:10:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-8fdkt
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-94.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 205m
          memory: 675Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:34:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0e39ec8dde59fddd9
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-8wmhj
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-71.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-226-71.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:21Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-9cl8c
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-157.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:34:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-09203d47c303c0b85
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-bbkk2
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-12.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 263m
          memory: 900Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:44:53Z"
      observedGeneration: 1
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T00:26:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-btvls
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-86.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:42Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent-clusterchecks
      app.kubernetes.io/instance: agent-clusterchecks
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 787f8cc644
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent-clusterchecks
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent-clusterchecks
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-clusterchecks-787f8cc644-bfm89
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: datadog-clusterchecks-787f8cc644
      uid: 493260f7-e83c-474e-b355-0c68b08b41d4
  spec:
    containers:
    - name: datadog-clusterchecks
      resources:
        limits:
          cpu: 1500m
          memory: 4000Mi
        requests:
          cpu: 750m
          memory: 2000Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 787f8cc644
        matchLabels:
          project: datadog
          role: agent-clusterchecks
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:51:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:51:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0cc30e7b1b8984d2d
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-ctj2s
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-153.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-227-153.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-czphl
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-222.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 256m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:34:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-02ef8ae4559f58e80
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-dnk8k
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 195Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-09604efed6dbf1fba
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-dp7j8
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-47.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-47.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0f04554f198c692f3
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-h85ft
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-249.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 205m
          memory: 675Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-227-249.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-092f11a650f55f796
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-jfkvl
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-54.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 281m
          memory: 900Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-089f56965bd0d4ef1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-jm4l8
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-207.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-227-207.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-jt9vv
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-48.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:25Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-mt6ld
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-44.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:41Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:21Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-mxd5n
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-88.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:24Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-083870684c4116622
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-nwd8c
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-59.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-59.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:34:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:40Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-nwhzx
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-5.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-p9tpl
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 195Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:59:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-pphfd
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-184.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-022ad1b23bc585a34
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-qf62q
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-252.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 299m
          memory: 900Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:46Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-qr4zw
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-136.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:23Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-qxzxh
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-182.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 235m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:28:01Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-rqzxj
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-221.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 205m
          memory: 675Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:10Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:49Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-06745aa58d1ddcc0c
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-thv49
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-7.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 281m
          memory: 900Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:16:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:15:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-031d33af9d554291f
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-v67zg
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-125.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-229-125.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0b960eeb84dec48cf
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-v784j
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-237.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-228-237.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:16Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-08c30b7d86243ffe6
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-vdbnx
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-160.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 30m
          memory: 225Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-160.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:10Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:32:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: agent
      app.kubernetes.io/instance: agent
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: datadog-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v603
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 757f59595c
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KG01AQSHQYKWYQ83NQTRS5M3
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "793"
      product: reliability
      project: datadog
      revision: aaa48e8157bd9099bef448caf82cdc9d60f79d48
      role: agent
      service: datadog-daemonset
      sidecar.istio.io/inject: "false"
      tag: v603
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: datadog-daemonset-agent
      tags.datadoghq.com/version: v603
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KG01AQSHQYKWYQ83NQTRS5M3
      version: v603
    name: datadog-zwpbw
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: datadog
      uid: 9c3b6be0-3430-468f-a647-a7f7c042be6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-219.us-west-2.compute.internal
    containers:
    - name: dd-agent
      resources:
        limits:
          cpu: 1500m
          memory: 2000Mi
        requests:
          cpu: 256m
          memory: 1500Mi
    - name: system-probe
      resources:
        limits:
          cpu: "1"
          memory: 1200Mi
        requests:
          cpu: 200m
          memory: 200Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:37Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:33:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-001d80fdf5fa88e94
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-2d67h
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-221.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 10m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-031d33af9d554291f
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-2hwtd
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-125.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-125.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:12Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0cc30e7b1b8984d2d
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-2qcx4
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-153.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-153.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:00:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-6bvtt
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-86.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:16Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:07Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-6fgbg
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-44.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0e39ec8dde59fddd9
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-9p6sr
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-71.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-71.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector-arm
      app.kubernetes.io/instance: vector-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 7666bb8dbf
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "61"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent-arm
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector-arm
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-arm-22n52
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector-arm
      uid: 25b2d63f-81ee-4c6c-aa8a-bcaf3fa6cc7b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-233.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 500m
          memory: 1000Mi
        requests:
          cpu: 50m
          memory: 300Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:52Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector-arm
      app.kubernetes.io/instance: vector-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 7666bb8dbf
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "61"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent-arm
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector-arm
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-arm-jvxs4
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector-arm
      uid: 25b2d63f-81ee-4c6c-aa8a-bcaf3fa6cc7b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-219.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 500m
          memory: 1000Mi
        requests:
          cpu: 50m
          memory: 300Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:52Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector-arm
      app.kubernetes.io/instance: vector-arm
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 7666bb8dbf
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "61"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent-arm
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector-arm
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-arm-vw4kw
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector-arm
      uid: 25b2d63f-81ee-4c6c-aa8a-bcaf3fa6cc7b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-222.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 500m
          memory: 1000Mi
        requests:
          cpu: 50m
          memory: 300Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:58Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-bxjg7
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-157.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:23Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:14Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-022ad1b23bc585a34
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-c6v67
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-252.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 5m
          memory: 100Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:26Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:17Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-08c30b7d86243ffe6
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-cf9vl
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-160.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-160.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:50:21Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-07582b05c1d7c23a6
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-clzkr
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:03:55Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-cznqs
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-75.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:10:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:10:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-03416309429e0355e
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-dtb6n
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-180.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-180.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:42:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-h558z
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-88.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:27:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-092f11a650f55f796
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-hv7kk
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-54.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 5m
          memory: 100Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:23Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0b14e96587341e0d7
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-hw6t5
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:59:48Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-00dfd5a02430664e8
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-lwkds
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-5.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:24Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-m4fzv
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-182.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:28:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-06bfb6b4b3967a8e2
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-mmktc
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-136.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:33Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:23Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-04e3afe8284718809
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-nf9m5
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-48.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:09Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:35:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-09203d47c303c0b85
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-nx2dl
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-12.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 5m
          memory: 100Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:44:53Z"
      observedGeneration: 1
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T00:26:47Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-09604efed6dbf1fba
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-q47bj
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-47.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-47.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:50:14Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:36Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-02ef8ae4559f58e80
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-qs7ff
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-086ec19ffcf526103
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-rh4ff
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-94.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 10m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:16Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0f8e0c5e9e6e06171
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-rhlpr
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-184.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-089f56965bd0d4ef1
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-rkdr7
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-207.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-207.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:38Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-06745aa58d1ddcc0c
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-vj77g
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-7.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 5m
          memory: 100Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:16:11Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:15:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-083870684c4116622
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-wtg7f
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-59.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-59.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:15Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0b960eeb84dec48cf
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-z842h
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-237.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-237.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:08:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:58Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-zjjl8
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-125.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 20m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-0f04554f198c692f3
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-zl6ps
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-249.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 10m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-249.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: vector
      app.kubernetes.io/instance: vector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-daemonset
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v412
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      controller-revision-hash: 694b5db68
      criticality-tier: tier0
      daemonsetresourcesbynodesize.compute.zende.sk/opt-in: "true"
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFK6F08A3Z25WA0RHAT65AP9
      deploy_phase: ""
      ec2-instance-id: i-02eb8fb6af2c9b25e
      kubernetes.io/arch: arm64
      kubernetes.io/cluster-service: "true"
      log_via: telemetry-pipeline
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-generation: "305"
      product: reliability
      project: vector
      revision: db8fadb0f4cab41435a504ec848fa3c0ef92c804
      role: agent
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "1111"
      service: vector-daemonset
      sidecar.istio.io/inject: "false"
      tag: v412
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-daemonset-vector
      tags.datadoghq.com/version: v412
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFK6F08A3Z25WA0RHAT65AP9
      version: v412
      zende.sk/dialtone: "true"
    name: vector-zrwfc
    namespace: telemetry-pipeline
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: vector
      uid: b8138830-64c1-4633-a49c-dedb7d05ced9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-66.us-west-2.compute.internal
    containers:
    - name: vector-agent
      resources:
        limits:
          cpu: 150m
          memory: 300Mi
        requests:
          cpu: 2m
          memory: 20Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-66.us-west-2.compute.internal
    priority: 21000000
    priorityClassName: node-critical
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:48Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v271
      branch: v271
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/custom-metric: istio
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01K6HK46B3N211VN1NF9M1AD41
      deploy_phase: phase-1
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-092f11a650f55f796
      evict.compute.zende.sk/now: "true"
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-hash: 699cf7fc8c
      product: foundation
      project: truth-service
      rescheduler.compute.zende.sk/rescheduled: "true"
      revision: 3bab1c685bdb2884147451b7a74d1170136b2a71
      role: app-server
      security.istio.io/tlsMode: istio
      service: truth-service
      service.istio.io/canonical-name: truth-service
      service.istio.io/canonical-revision: v271
      sidecar.istio.io/inject: "true"
      tag: v271
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: truth-service
      tags.datadoghq.com/istio-proxy.version: v271
      tags.datadoghq.com/service: truth-service
      tags.datadoghq.com/version: v271
      team: compute
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01K6HK46B3N211VN1NF9M1AD41
      version: v271
    name: truth-service-app-server-699cf7fc8c-2b8w6
    namespace: truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: truth-service-app-server-699cf7fc8c
      uid: edb91ed5-7f9d-4ac3-8d5e-297650fc418a
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - app-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: truth-service-app-server
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 1Gi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    nodeSelector:
      node-type: node-spot
    priority: 100
    priorityClassName: tier4
    tolerations:
    - operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 699cf7fc8c
        matchLabels:
          project: truth-service
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:01:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:01:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v271
      branch: v271
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/custom-metric: istio
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01K6HK46B3N211VN1NF9M1AD41
      deploy_phase: phase-1
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-06745aa58d1ddcc0c
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-hash: 699cf7fc8c
      product: foundation
      project: truth-service
      revision: 3bab1c685bdb2884147451b7a74d1170136b2a71
      role: app-server
      security.istio.io/tlsMode: istio
      service: truth-service
      service.istio.io/canonical-name: truth-service
      service.istio.io/canonical-revision: v271
      sidecar.istio.io/inject: "true"
      tag: v271
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: truth-service
      tags.datadoghq.com/istio-proxy.version: v271
      tags.datadoghq.com/service: truth-service
      tags.datadoghq.com/version: v271
      team: compute
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2c
      track: 01K6HK46B3N211VN1NF9M1AD41
      version: v271
    name: truth-service-app-server-699cf7fc8c-2mbp5
    namespace: truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: truth-service-app-server-699cf7fc8c
      uid: edb91ed5-7f9d-4ac3-8d5e-297650fc418a
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - app-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: truth-service-app-server
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 1Gi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    nodeSelector:
      node-type: node-spot
    priority: 100
    priorityClassName: tier4
    tolerations:
    - operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 699cf7fc8c
        matchLabels:
          project: truth-service
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:45:54Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:45:39Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v271
      branch: v271
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/custom-metric: istio
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01K6HK46B3N211VN1NF9M1AD41
      deploy_phase: phase-1
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-022ad1b23bc585a34
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      pod-template-hash: 699cf7fc8c
      product: foundation
      project: truth-service
      revision: 3bab1c685bdb2884147451b7a74d1170136b2a71
      role: app-server
      security.istio.io/tlsMode: istio
      service: truth-service
      service.istio.io/canonical-name: truth-service
      service.istio.io/canonical-revision: v271
      sidecar.istio.io/inject: "true"
      tag: v271
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: truth-service
      tags.datadoghq.com/istio-proxy.version: v271
      tags.datadoghq.com/service: truth-service
      tags.datadoghq.com/version: v271
      team: compute
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01K6HK46B3N211VN1NF9M1AD41
      version: v271
    name: truth-service-app-server-699cf7fc8c-6v76c
    namespace: truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: truth-service-app-server-699cf7fc8c
      uid: edb91ed5-7f9d-4ac3-8d5e-297650fc418a
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - app-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: truth-service-app-server
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 1Gi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    nodeSelector:
      node-type: node-spot
    priority: 100
    priorityClassName: tier4
    tolerations:
    - operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 699cf7fc8c
        matchLabels:
          project: truth-service
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:15:21Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:14:51Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v271
      branch: v271
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/custom-metric: istio
      compute.zende.sk/duplicate: "true"
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01K6HK46B3N211VN1NF9M1AD41
      deploy_phase: phase-1
      descheduler.compute.zende.sk/opt-in: "true"
      pod-template-hash: 699cf7fc8c
      product: foundation
      project: truth-service
      revision: 3bab1c685bdb2884147451b7a74d1170136b2a71
      role: app-server
      security.istio.io/tlsMode: istio
      service: truth-service
      service.istio.io/canonical-name: truth-service
      service.istio.io/canonical-revision: v271
      sidecar.istio.io/inject: "true"
      tag: v271
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: truth-service
      tags.datadoghq.com/istio-proxy.version: v271
      tags.datadoghq.com/service: truth-service
      tags.datadoghq.com/version: v271
      team: compute
      topology.istio.io/network: pod998
      track: 01K6HK46B3N211VN1NF9M1AD41
      version: v271
    name: truth-service-app-server-699cf7fc8c-j6qj5
    namespace: truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: truth-service-app-server-699cf7fc8c
      uid: edb91ed5-7f9d-4ac3-8d5e-297650fc418a
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - app-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: truth-service-app-server
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 1Gi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeSelector:
      node-type: node-spot
    priority: 100
    priorityClassName: tier4
    tolerations:
    - operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 699cf7fc8c
        matchLabels:
          project: truth-service
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:00:06Z"
      message: '0/34 nodes are available: 1 node(s) didn''t match pod anti-affinity
        rules, 3 node(s) didn''t match pod topology spread constraints, 30 node(s)
        didn''t match Pod''s node affinity/selector. preemption: not eligible due
        to preemptionPolicy=Never.'
      reason: Unschedulable
      status: "False"
      type: PodScheduled
    phase: Pending
- metadata:
    labels:
      app.kubernetes.io/component: app-server
      app.kubernetes.io/instance: app-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: truth-service
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v271
      branch: v271
      cicd-toolkit.zende.sk/pipeline: sandbox
      cicd-toolkit.zende.sk/pipeline-deploy-sequence-index: "0"
      compute.zende.sk/custom-metric: istio
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: spot
      criticality-tier: tier4
      datadog_log_index: compute
      deploy_group: sandbox
      deploy_id: 01K6HK46B3N211VN1NF9M1AD41
      deploy_phase: phase-1
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-09203d47c303c0b85
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      pod-template-hash: 699cf7fc8c
      product: foundation
      project: truth-service
      revision: 3bab1c685bdb2884147451b7a74d1170136b2a71
      role: app-server
      security.istio.io/tlsMode: istio
      service: truth-service
      service.istio.io/canonical-name: truth-service
      service.istio.io/canonical-revision: v271
      sidecar.istio.io/inject: "true"
      tag: v271
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: truth-service
      tags.datadoghq.com/istio-proxy.version: v271
      tags.datadoghq.com/service: truth-service
      tags.datadoghq.com/version: v271
      team: compute
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2a
      track: 01K6HK46B3N211VN1NF9M1AD41
      version: v271
    name: truth-service-app-server-699cf7fc8c-w5ppf
    namespace: truth-service
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: truth-service-app-server-699cf7fc8c
      uid: edb91ed5-7f9d-4ac3-8d5e-297650fc418a
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: role
              operator: In
              values:
              - app-server
          topologyKey: kubernetes.io/hostname
    containers:
    - name: truth-service-app-server
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
        requests:
          cpu: 11m
          memory: 1Gi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 10m
          memory: 50Mi
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    nodeSelector:
      node-type: node-spot
    priority: 100
    priorityClassName: tier4
    tolerations:
    - operator: Exists
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 699cf7fc8c
        matchLabels:
          project: truth-service
          role: app-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:44:52Z"
      observedGeneration: 1
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:42:36Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vault-access-operator
      canary: "false"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      name: vault-access-operator
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/run-as-non-root: "false"
      pod-template-hash: 65fdbf4875
      product: foundation
      project: vault-access-operator
      role: operator
      service: vault-access-operator
      team: lockbox
      topology.kubernetes.io/zone: us-west-2b
      zende.sk/dialtone: "true"
    name: vault-access-operator-65fdbf4875-t7x5c
    namespace: vault-access-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: vault-access-operator-65fdbf4875
      uid: 54986fa3-66d3-48fd-baf5-3996dae62cf4
  spec:
    containers:
    - name: vault-access-operator
      resources:
        limits:
          cpu: "1"
          memory: 512Mi
        requests:
          cpu: 49m
          memory: 100Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 65fdbf4875
        matchLabels:
          name: vault-access-operator
          project: vault-access-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T05:02:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "0"
      availability_zone: us-west-2a
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2a-77f4b6b748
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2a-0
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2a-0
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2a
      uid: 8255655d-b362-4d88-bd57-f43a29843568
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2a
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:25:07Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:24:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "1"
      availability_zone: us-west-2a
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2a-77f4b6b748
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2a-1
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2a-1
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2a
      uid: 8255655d-b362-4d88-bd57-f43a29843568
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2a
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:25:13Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:24:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "2"
      availability_zone: us-west-2a
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2a-77f4b6b748
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2a-2
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2a-2
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2a
      uid: 8255655d-b362-4d88-bd57-f43a29843568
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2a
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:14Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:20Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "0"
      availability_zone: us-west-2b
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2b-7b7576dd7c
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2b-0
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2b-0
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2b
      uid: 27ba901c-e40f-4155-aafb-0428384bc897
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2b
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:45:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:34Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "1"
      availability_zone: us-west-2b
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2b-7b7576dd7c
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2b-1
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2b-1
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2b
      uid: 27ba901c-e40f-4155-aafb-0428384bc897
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2b
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:45:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:34Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "2"
      availability_zone: us-west-2b
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2b-7b7576dd7c
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-09517c0dfc55bfd1e
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2b-2
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2b-2
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2b
      uid: 27ba901c-e40f-4155-aafb-0428384bc897
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2b
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:38:53Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:37:44Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "0"
      availability_zone: us-west-2c
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2c-6f8db548cd
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2c-0
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2c-0
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2c
      uid: 91f8506a-a4a1-4d8a-9b35-3c62a8cf732e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2c
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:42:05Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:41:06Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "1"
      availability_zone: us-west-2c
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2c-6f8db548cd
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2c-1
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2c-1
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2c
      uid: 91f8506a-a4a1-4d8a-9b35-3c62a8cf732e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2c
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:42:05Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:41:02Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: aggregator
      app.kubernetes.io/instance: aggregator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: vector-log-collector
      app.kubernetes.io/part-of: reliability
      app.kubernetes.io/version: v361
      apps.kubernetes.io/pod-index: "2"
      availability_zone: us-west-2c
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      compute.zende.sk/network-usage: high
      configuration-delivery: "true"
      controller-revision-hash: aggregator-us-west-2c-6f8db548cd
      criticality-tier: tier0
      datadog_log_index: observability
      deploy_group: sandbox
      deploy_id: 01KFKBE9TGZNTAG5A9G9KKXV9F
      deploy_phase: ""
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      opa-gatekeeper.zendesk.com/required-probes: "false"
      product: reliability
      project: vector-log-collector
      revision: 225cd2dab936dfa0b8057f90e45cc87dc0139f08
      role: aggregator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: vector-log-collector
      sidecar.istio.io/inject: "false"
      statefulset.kubernetes.io/pod-name: aggregator-us-west-2c-2
      tag: v361
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: vector-log-collector-aggregator
      tags.datadoghq.com/version: v361
      team: sre-observability
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFKBE9TGZNTAG5A9G9KKXV9F
      version: v361
      zende.sk/dialtone: "true"
    name: aggregator-us-west-2c-2
    namespace: vector-log-collector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: aggregator-us-west-2c
      uid: 91f8506a-a4a1-4d8a-9b35-3c62a8cf732e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: topology.kubernetes.io/zone
              operator: In
              values:
              - us-west-2c
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: compute.zende.sk/network-usage
                operator: In
                values:
                - high
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - name: aggregator
      resources:
        limits:
          cpu: "3"
          memory: 4Gi
        requests:
          cpu: "2"
          memory: 3Gi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:30:34Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:29:31Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: velero
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: compute-velero
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v67
      branch: v67
      cicd-toolkit.zende.sk/pipeline: sandbox
      component: velero
      compute.zende.sk/duplicate: "true"
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy: velero
      deploy_group: sandbox
      deploy_id: 01KFF6QWK2A2DJTGNNGY9ATFPD
      deploy_phase: phase-1
      ec2-instance-id: i-00d9ece9adca2e6a7
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 5cbd8c68f5
      product: foundation
      project: compute-velero
      revision: 714ce7c70fe34c0533d6d1e4fae03a32b0cec4b1
      role: velero
      service: compute-velero
      sidecar.istio.io/inject: "false"
      tag: v67
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: compute-velero-velero
      tags.datadoghq.com/version: v67
      team: compute
      topology.kubernetes.io/zone: us-west-2c
      track: 01KFF6QWK2A2DJTGNNGY9ATFPD
      version: v67
    name: velero-5cbd8c68f5-mgh7j
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: velero-5cbd8c68f5
      uid: dc98c2e0-406e-4c79-9a36-fd27f43d38ab
  spec:
    containers:
    - name: velero
      resources:
        limits:
          cpu: "4"
          memory: 4Gi
        requests:
          cpu: 23m
          memory: 512Mi
    initContainers:
    - name: velero-plugin-for-aws-init
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 100m
          memory: 256M
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    nodeSelector:
      kubernetes.io/os: linux
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5cbd8c68f5
        matchLabels:
          deploy: velero
          project: compute-velero
          role: velero
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:17:14Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:17:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-kubernetes-connector
      app.kubernetes.io/instance: wiz-kubernetes-connector
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-kubernetes-connector
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v31
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier4
      deploy_group: sandbox
      deploy_id: 01KEXCZN8RNWXWG2PR829JJX7J
      deploy_phase: phase-1
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      helm.sh/chart: wiz-broker-3.0.3
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-hash: 5c9cc8cd74
      product: foundation
      project: wiz-kubernetes-connector
      revision: c39a09ca832d0d8a2a7b9fe4cd52a626a8a7a47f
      role: wiz-kubernetes-connector
      service: wiz-kubernetes-connector
      sidecar.istio.io/inject: "false"
      tag: v31
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-kubernetes-connector
      tags.datadoghq.com/version: v31
      team: compute-infra
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEXCZN8RNWXWG2PR829JJX7J
      version: v31
      wiz.io/component: broker
    name: wiz-kubernetes-connector-broker-5c9cc8cd74-2qdp5
    namespace: wiz-kubernetes-connector
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: wiz-kubernetes-connector-broker-5c9cc8cd74
      uid: d8377345-9a68-482e-b420-44e12131a897
  spec:
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - preference:
            matchExpressions:
            - key: node-type
              operator: In
              values:
              - node-arm
          weight: 50
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
            - key: kubernetes.io/arch
              operator: In
              values:
              - arm64
              - amd64
    containers:
    - name: wiz-broker
      resources:
        limits:
          cpu: "1"
          memory: 512M
        requests:
          cpu: 11m
          memory: 256M
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier4
    tolerations:
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node-arm
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T17:29:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T17:29:43Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-064c2e3e5ae6fa9ad
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-27x9m
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-233.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-233.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:41:06Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0b960eeb84dec48cf
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-29xlw
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-237.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-237.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:10:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:08:11Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0f8e0c5e9e6e06171
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-2k6sb
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-184.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-184.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:31:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-02ef8ae4559f58e80
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-2q56p
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-140.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-140.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:38Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:33Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0a58c862ae9a8aadd
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-2rd7t
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-222.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-222.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:31:03Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:46Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-083870684c4116622
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-475sw
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-59.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-59.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:58:24Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:28Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0f04554f198c692f3
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-69kl6
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-249.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-249.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:20:52Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:40:59Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-09203d47c303c0b85
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: r6i.8xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-6l6jv
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-12.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-224-12.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:44:52Z"
      observedGeneration: 1
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T00:27:01Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-06745aa58d1ddcc0c
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-6v8m4
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-7.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-7.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:16:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:16:08Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-04e3afe8284718809
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-6xm76
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-228-48.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-228-48.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:18Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:36:13Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-7zflc
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-44.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:51:57Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:44:27Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-001d80fdf5fa88e94
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-8hxcp
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-221.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-221.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:37:44Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:02Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-8vmr6
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-157.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:29Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T19:31:24Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-09604efed6dbf1fba
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-bxcv7
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-47.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-47.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:39:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-02eb8fb6af2c9b25e
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-dqh42
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-66.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-66.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:14:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:54:21Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0cc30e7b1b8984d2d
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-dskjp
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-153.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-153.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:39:21Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:00:42Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0b5fecdbd22bdd3e3
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-g74bz
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-75.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-224-75.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:11:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:10:57Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-09517c0dfc55bfd1e
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-h2pb5
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-86.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-86.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:25Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:33:20Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0b14e96587341e0d7
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-jrjr9
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-151.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-151.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:14Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:00:01Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-08c30b7d86243ffe6
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-k4mxj
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-160.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-160.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:58:35Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:49:56Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-086ec19ffcf526103
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: api
      node.kubernetes.io/instance-type: m7g.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-l26jl
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-94.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-94.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:07:50Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:37Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0be8cb19d8c2b2a0f
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: node-arm
      node.kubernetes.io/instance-type: m6g.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-pwjrx
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-219.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-219.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:13:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:41:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-092f11a650f55f796
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c5.12xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-qvbmb
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-54.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-54.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:29Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:01:23Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-031d33af9d554291f
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-rljtl
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-125.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-125.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:59:51Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:07:26Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-06bfb6b4b3967a8e2
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-sd87s
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-136.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:40Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:58:35Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: spot
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-022ad1b23bc585a34
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-spot
      node.kubernetes.io/instance-type: c7i.16xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-sxprr
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-252.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-252.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:12:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-089f56965bd0d4ef1
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd-events
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-t2llg
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-227-207.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-227-207.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:26:49Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:01:52Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-07582b05c1d7c23a6
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node-nvidia-gpu
      node.kubernetes.io/instance-type: g4dn.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-ts422
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-224-171.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-224-171.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:26Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T03:04:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-00dfd5a02430664e8
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-vbbm4
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-5.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:39Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:30Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-00d9ece9adca2e6a7
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2c
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-w4z4v
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-229-182.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-182.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:48Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T21:27:41Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-03416309429e0355e
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-xbdn9
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-180.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-180.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T15:10:02Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:43:10Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-010877a189d01d91a
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-xl5n9
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-88.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:18Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:28:13Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0e39ec8dde59fddd9
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: arm64
      node-type: etcd
      node.kubernetes.io/instance-type: m6g.xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2b
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-zcrt5
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-226-71.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-71.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T14:37:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T01:55:29Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: wiz-sensor
      app.kubernetes.io/instance: wiz-sensor
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: wiz-runtime-sensor
      app.kubernetes.io/part-of: security
      app.kubernetes.io/version: v35
      branch: v35
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      controller-revision-hash: 6c765b56b6
      criticality-tier: tier2
      deploy_group: sandbox
      deploy_id: 01KEA4YB69MA3DEX33VTHN6031
      deploy_phase: ""
      ec2-instance-id: i-0c394e6fb29b43132
      helm.sh/chart: wiz-sensor-1.0.4365
      image/tag: v1
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-generation: "30"
      product: security
      project: wiz-runtime-sensor
      revision: a341b2f0d2193f0f3b8226090910e33d04fa9b4a
      role: wiz-sensor
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: wiz-runtime-sensor
      sidecar.istio.io/inject: "false"
      tag: v35
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: wiz-runtime-sensor-wiz-sensor
      tags.datadoghq.com/version: v35
      team: product-security
      topology.kubernetes.io/zone: us-west-2a
      track: 01KEA4YB69MA3DEX33VTHN6031
      version: v35
      zende.sk/dialtone: "true"
    name: wiz-sensor-zr2tg
    namespace: wiz-runtime-sensor
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: wiz-sensor
      uid: dd406067-2776-4124-a3e8-a0c370292fd7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - ip-172-30-225-125.us-west-2.compute.internal
    containers:
    - name: wiz-sensor
      resources:
        limits:
          cpu: "1"
          memory: 1536Mi
        requests:
          cpu: 30m
          memory: 250Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 20000000
    priorityClassName: cluster-critical
    tolerations:
    - effect: NoSchedule
      key: kubernetes.io/arch
      operator: Equal
      value: arm64
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
    - key: node.kubernetes.io/role
      operator: Exists
    - key: node.kubernetes.io/unschedulable
      operator: Exists
    - key: ownedby
      operator: Exists
    - key: kube-node-monitor
      operator: Exists
    - key: compute.zende.sk/nodegroup
      operator: Exists
    - key: compute.zendesk.com/eni_max_ips_reached
      operator: Exists
    - key: compute.zendesk.com/persistent_volumes_limit_reached
      operator: Exists
    - key: compute.zendesk.com/root_volume_limit_reached
      operator: Exists
    - key: DeletionCandidateOfClusterAutoscaler
      operator: Exists
    - key: ToBeDeletedByClusterAutoscaler
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:59Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:21:54Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-access-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v210
      branch: master
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      deploy_group: sandbox
      deploy_id: 01KG10Y48QWYK5GC3RGFSNV53Z
      deploy_phase: ""
      ec2-instance-id: i-00c5b7259f4923f4f
      kubernetes.io/arch: amd64
      name: zendesk-access-operator
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 6845b7b74
      product: foundation
      project: zendesk-access-operator
      revision: bc39db86f798a535cf1e7ade4c17618835a3ea00
      role: operator
      secret-service.zende.sk/sidecar: sidecar
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: zendesk-access-operator
      sidecar.istio.io/inject: "false"
      tag: v210
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: zendesk-access-operator-operator
      tags.datadoghq.com/version: v210
      team: guardians
      topology.kubernetes.io/zone: us-west-2c
      track: 01KG10Y48QWYK5GC3RGFSNV53Z
      version: v210
      zende.sk/dialtone: "true"
    name: zendesk-access-operator-6845b7b74-mw7w5
    namespace: zendesk-access-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-access-operator-6845b7b74
      uid: f590a794-a9b4-49c6-b443-bb0e86ce38cc
  spec:
    containers:
    - name: zendesk-access-operator
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 11m
          memory: 200Mi
    initContainers:
    - name: secret-sidecar
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-157.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 6845b7b74
        matchLabels:
          name: zendesk-access-operator
          project: zendesk-access-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:24:27Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:24:06Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-auth-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: amir-sandbox-640685a3c95a85bea980536e71afe93b157a7903
      branch: amir-sandbox
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-storage
      deploy_group: sandbox
      deploy_id: 01J4Q62ZVV4T6507H768FCE42Z
      deploy_phase: ""
      ec2-instance-id: i-0af7d5e8a274b2b88
      from-operator: zendesk-auth-operator
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 5bb4dd857d
      product: foundation
      project: zendesk-auth-operator
      revision: 640685a3c95a85bea980536e71afe93b157a7903
      role: operator
      security.istio.io/tlsMode: istio
      service: zendesk-auth-operator
      service.istio.io/canonical-name: zendesk-auth-operator-operator
      service.istio.io/canonical-revision: amir-sandbox-640685a3c95a85bea980536e71afe93b157a7903
      sidecar.istio.io/inject: "true"
      tag: amir-sandbox
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/istio-proxy.env: staging
      tags.datadoghq.com/istio-proxy.service: zendesk-auth-operator-operator
      tags.datadoghq.com/istio-proxy.version: amir-sandbox-640685a3c95a85bea980536e71afe93b157a7903
      tags.datadoghq.com/service: zendesk-auth-operator-operator
      tags.datadoghq.com/version: amir-sandbox-640685a3c95a85bea980536e71afe93b157a7903
      team: emporium
      topology.istio.io/network: pod998
      topology.kubernetes.io/zone: us-west-2b
      track: 01J4Q62ZVV4T6507H768FCE42Z
      version: amir-sandbox-640685a3c95a85bea980536e71afe93b157a7903
      zende.sk/dialtone: "true"
    name: zendesk-auth-operator-5bb4dd857d-w7zr9
    namespace: zendesk-auth-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-auth-operator-5bb4dd857d
      uid: b94b3939-0e32-49fa-a89d-14459a957ff3
  spec:
    containers:
    - name: zendesk-auth-operator
      resources:
        limits:
          cpu: "1"
          memory: 2500Mi
        requests:
          cpu: 11m
          memory: 256Mi
    initContainers:
    - name: istio-init
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    - name: istio-proxy
      resources:
        limits:
          cpu: "4"
          memory: 2Gi
        requests:
          cpu: 150m
          memory: 256Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 5bb4dd857d
        matchLabels:
          project: zendesk-auth-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:06:55Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-23T02:06:50Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: zendesk-auth-webhook
      app.kubernetes.io/component: zendesk-auth-webhook-server
      app.kubernetes.io/instance: zendesk-auth-webhook-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-auth-webhook
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v380
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-storage
      deploy_group: sandbox
      deploy_id: 01KC5XS3ZB757J0YE633XE48EE
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-00dfd5a02430664e8
      from-operator: zendesk-auth-operator
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 774b8bc65c
      product: foundation
      project: zendesk-auth-webhook
      revision: ad2504af66687596472b8556263eb29658ccf75d
      role: zendesk-auth-webhook-server
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: zendesk-auth-webhook
      sidecar.istio.io/inject: "false"
      tag: v380
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: zendesk-auth-webhook-server
      tags.datadoghq.com/version: v380
      team: emporium-b-team
      topology.kubernetes.io/zone: us-west-2b
      track: 01KC5XS3ZB757J0YE633XE48EE
      usespot.compute.zende.sk/opt-out: "true"
      version: v380
      zende.sk/dialtone: "true"
    name: zendesk-auth-webhook-server-774b8bc65c-scxtz
    namespace: zendesk-auth-webhook
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-auth-webhook-server-774b8bc65c
      uid: d37a9d10-3254-4568-98db-6f883c46897e
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: "1"
          memory: 488Mi
        requests:
          cpu: 11m
          memory: 488Mi
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-226-5.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 774b8bc65c
        matchLabels:
          project: zendesk-auth-webhook
          role: zendesk-auth-webhook-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:47Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:24:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: zendesk-auth-webhook
      app.kubernetes.io/component: zendesk-auth-webhook-server
      app.kubernetes.io/instance: zendesk-auth-webhook-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-auth-webhook
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v380
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-storage
      deploy_group: sandbox
      deploy_id: 01KC5XS3ZB757J0YE633XE48EE
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-010877a189d01d91a
      from-operator: zendesk-auth-operator
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 774b8bc65c
      product: foundation
      project: zendesk-auth-webhook
      revision: ad2504af66687596472b8556263eb29658ccf75d
      role: zendesk-auth-webhook-server
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: zendesk-auth-webhook
      sidecar.istio.io/inject: "false"
      tag: v380
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: zendesk-auth-webhook-server
      tags.datadoghq.com/version: v380
      team: emporium-b-team
      topology.kubernetes.io/zone: us-west-2a
      track: 01KC5XS3ZB757J0YE633XE48EE
      usespot.compute.zende.sk/opt-out: "true"
      version: v380
      zende.sk/dialtone: "true"
    name: zendesk-auth-webhook-server-774b8bc65c-x2qqp
    namespace: zendesk-auth-webhook
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-auth-webhook-server-774b8bc65c
      uid: d37a9d10-3254-4568-98db-6f883c46897e
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: "1"
          memory: 488Mi
        requests:
          cpu: 11m
          memory: 488Mi
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-225-88.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 774b8bc65c
        matchLabels:
          project: zendesk-auth-webhook
          role: zendesk-auth-webhook-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:45Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T14:29:32Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app: zendesk-auth-webhook
      app.kubernetes.io/component: zendesk-auth-webhook-server
      app.kubernetes.io/instance: zendesk-auth-webhook-server
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-auth-webhook
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v380
      branch: main
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      criticality-tier: tier3
      datadog_log_index: foundation-storage
      deploy_group: sandbox
      deploy_id: 01KC5XS3ZB757J0YE633XE48EE
      deploy_phase: ""
      descheduler.compute.zende.sk/opt-in: "true"
      ec2-instance-id: i-06bfb6b4b3967a8e2
      from-operator: zendesk-auth-operator
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c6i.2xlarge
      pod-template-hash: 774b8bc65c
      product: foundation
      project: zendesk-auth-webhook
      revision: ad2504af66687596472b8556263eb29658ccf75d
      role: zendesk-auth-webhook-server
      secret-service.zende.sk/sidecar: init
      secret-service.zende.sk/sidecar-api-port: "9999"
      service: zendesk-auth-webhook
      sidecar.istio.io/inject: "false"
      tag: v380
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: zendesk-auth-webhook-server
      tags.datadoghq.com/version: v380
      team: emporium-b-team
      topology.kubernetes.io/zone: us-west-2c
      track: 01KC5XS3ZB757J0YE633XE48EE
      usespot.compute.zende.sk/opt-out: "true"
      version: v380
      zende.sk/dialtone: "true"
    name: zendesk-auth-webhook-server-774b8bc65c-xmdtd
    namespace: zendesk-auth-webhook
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-auth-webhook-server-774b8bc65c
      uid: d37a9d10-3254-4568-98db-6f883c46897e
  spec:
    containers:
    - name: server
      resources:
        limits:
          cpu: "1"
          memory: 488Mi
        requests:
          cpu: 11m
          memory: 488Mi
    initContainers:
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 3m
          memory: 35Mi
    nodeName: ip-172-30-229-136.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 774b8bc65c
        matchLabels:
          project: zendesk-auth-webhook
          role: zendesk-auth-webhook-server
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:15Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T02:02:04Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-redis-operator
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: abdulhusain-ssd-3302-add-dialtone-label-f6d36af11266bca884d3588
      branch: abdulhusain-ssd-3302-add-dialtone-label
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      control-plane: controller-manager
      criticality-tier: tier3
      datadog_log_index: foundation-storage
      deploy_group: sandbox
      deploy_id: 01JD6NMR1W7QNYSKK6C5TX473W
      deploy_phase: phase-sandbox
      ec2-instance-id: i-0c394e6fb29b43132
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: c5.2xlarge
      pod-template-hash: 749d5fcdb
      product: foundation
      project: zendesk-redis-operator
      revision: f6d36af11266bca884d35880a4049207dbbe7a4a
      role: operator
      service: zendesk-redis-operator
      sidecar.istio.io/inject: "false"
      tag: abdulhusain-ssd-3302-add-dialtone-la
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: zendesk-redis-operator
      tags.datadoghq.com/version: abdulhusain-ssd-3302-add-dialtone-label-f6d36af11266bca884d3588
      team: emporium
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2a
      track: 01JD6NMR1W7QNYSKK6C5TX473W
      version: abdulhusain-ssd-3302-add-dialtone-label-f6d36af11266bca884d3588
      zende.sk/dialtone: "true"
    name: zendesk-redis-operator-1-749d5fcdb-lvvm2
    namespace: zendesk-redis-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zendesk-redis-operator-1-749d5fcdb
      uid: 396625d0-29cc-40e6-a44f-afdcc7a200fa
  spec:
    containers:
    - name: operator-1
      resources:
        limits:
          cpu: 200m
          memory: 1Gi
        requests:
          cpu: 11m
          memory: 512Mi
    - name: zendesk-auth-sidecar-container
      resources:
        limits:
          cpu: 150m
          memory: 128M
        requests:
          cpu: 11m
          memory: 128M
    initContainers:
    - name: zendesk-auth-init-container
      resources:
        limits:
          cpu: 100m
          memory: 128M
        requests:
          cpu: 100m
          memory: 128M
    - name: secret-init
      resources:
        limits:
          cpu: 250m
          memory: 100Mi
        requests:
          cpu: 2m
          memory: 20Mi
    nodeName: ip-172-30-225-125.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier3
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 749d5fcdb
        matchLabels:
          control-plane: controller-manager
          project: zendesk-redis-operator
          role: operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:33:42Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-28T01:33:34Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
- metadata:
    labels:
      app.kubernetes.io/component: spark-operator
      app.kubernetes.io/instance: spark-operator
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: zendesk-spark-operator
      app.kubernetes.io/part-of: explore
      app.kubernetes.io/version: amir-operator-v2-89742b5541c7b6f7a4ed6356edf3b79466130553
      branch: amir-operator-v2
      cicd-toolkit.zende.sk/pipeline: sandbox
      compute.zende.sk/ec2-lifecycle: on-demand
      configuration-delivery: "true"
      criticality-tier: tier1
      deploy_group: sandbox
      deploy_id: 01KF49D3Y5BHVJZC6TK23PE5A7
      deploy_phase: phase-1
      ec2-instance-id: i-0af7d5e8a274b2b88
      kubernetes.io/arch: amd64
      node-type: node
      node.kubernetes.io/instance-type: m7i-flex.2xlarge
      pod-template-hash: 8585867bdf
      product: explore
      project: zendesk-spark-operator
      revision: 89742b5541c7b6f7a4ed6356edf3b79466130553
      role: spark-operator
      service: zendesk-spark-operator
      sidecar.istio.io/inject: "false"
      tag: amir-operator-v2
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: zendesk-spark-operator-spark-operator
      tags.datadoghq.com/version: amir-operator-v2-89742b5541c7b6f7a4ed6356edf3b79466130553
      team: compute
      temp-auth: enabled
      topology.kubernetes.io/zone: us-west-2b
      track: 01KF49D3Y5BHVJZC6TK23PE5A7
      version: amir-operator-v2-89742b5541c7b6f7a4ed6356edf3b79466130553
    name: spark-operator-8585867bdf-9js6g
    namespace: zendesk-spark-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: spark-operator-8585867bdf
      uid: 94ed820c-ad98-41de-a623-019e6ce56539
  spec:
    containers:
    - name: spark-operator
      resources:
        limits:
          cpu: "22"
          memory: 4Gi
        requests:
          cpu: 10m
          memory: 50Mi
    nodeName: ip-172-30-227-44.us-west-2.compute.internal
    priority: 100
    priorityClassName: tier1
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: compute.zende.sk/nodegroup
      value: node
    topologySpreadConstraints:
    - labelSelector:
        matchExpressions:
        - key: pod-template-hash
          operator: In
          values:
          - 8585867bdf
        matchLabels:
          project: zendesk-spark-operator
          role: spark-operator
      matchLabelKeys:
      - pod-template-hash
      maxSkew: 1
      nodeAffinityPolicy: Ignore
      nodeTaintsPolicy: Honor
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: DoNotSchedule
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:36Z"
      observedGeneration: 1
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-25T20:53:18Z"
      observedGeneration: 1
      status: "True"
      type: PodScheduled
    phase: Running
priorityClasses:
- description: Used for cluster critical components.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: cluster-critical
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:42Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:42Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:22Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:58Z"
    name: cluster-critical
    resourceVersion: "3399493664"
    uid: 91f79693-0b8f-47d8-9353-cc985457deed
  preemptionPolicy: PreemptLowerPriority
  value: 20000000
- description: Used for tier 0 services, deprecated.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: critical
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:43Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:43Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:22Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:56Z"
    name: critical
    resourceVersion: "3399492958"
    uid: 22ced30f-56d3-43ae-843d-a1fd8d790bdb
  preemptionPolicy: PreemptLowerPriority
  value: 100
- description: Used for GPU workloads to pre-empt CPU-only workloads.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: gpu
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"scheduling.k8s.io/v1","description":"Used for GPU workloads to pre-empt CPU-only workloads.","globalDefault":false,"kind":"PriorityClass","metadata":{"annotations":{"ad.datadoghq.com/tags":"{\"git.commit.sha\":\"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98\",\"git.repository_url\":\"github.com/zendesk/k8s-static-resources\",\"team\":\"compute\"}","artifact.spinnaker.io/location":"","artifact.spinnaker.io/name":"gpu","artifact.spinnaker.io/type":"kubernetes/PriorityClass","artifact.spinnaker.io/version":"","cicd-toolkit.zende.sk/environment":"staging","cicd-toolkit.zende.sk/partition":"sandbox","cicd-toolkit.zende.sk/phase":"phase-1","cicd-toolkit.zende.sk/role":"k8s-static-resources","cicd-toolkit.zende.sk/version":"v8.680.0","moniker.spinnaker.io/application":"k8s-static-resources","moniker.spinnaker.io/cluster":"sandbox","moniker.spinnaker.io/detail":"v48","moniker.spinnaker.io/stack":"k8s-static-resources-k8s-static-resources","strategy.spinnaker.io/recreate":"true","strategy.spinnaker.io/replace":"true"},"labels":{"app.kubernetes.io/component":"k8s-static-resources","app.kubernetes.io/instance":"k8s-static-resources","app.kubernetes.io/managed-by":"spinnaker","app.kubernetes.io/name":"k8s-static-resources","app.kubernetes.io/part-of":"foundation","app.kubernetes.io/version":"v48","branch":"v48","cicd-toolkit.zende.sk/config-revision":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","cicd-toolkit.zende.sk/config-version":"v48","cicd-toolkit.zende.sk/pipeline":"sandbox","compute.zende.sk/duplicate":"true","deploy_id":"01KEXCVHFRQCEE56Z0Q13DMSWT","product":"foundation","project":"zendesk_kubernetes","role":"priority-classes","service":"k8s-static-resources","tags.datadoghq.com/env":"staging","tags.datadoghq.com/service":"k8s-static-resources","tags.datadoghq.com/version":"v48","team":"compute","track":"01KEXCVHFRQCEE56Z0Q13DMSWT","version":"v48"},"name":"gpu"},"preemptionPolicy":"PreemptLowerPriority","value":101}
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/recreate: "true"
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2026-01-14T04:55:24Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/cluster: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/recreate: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:compute.zende.sk/duplicate: {}
            f:deploy_id: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:team: {}
            f:track: {}
            f:version: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2026-01-14T04:55:24Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:54Z"
    name: gpu
    resourceVersion: "3399492442"
    uid: 6b6d6e2d-6aae-4777-aa04-8c63ab58923c
  preemptionPolicy: PreemptLowerPriority
  value: 101
- description: Used for tier 1 services, deprecated.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: high
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:44Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:44Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:24Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:56Z"
    name: high
    resourceVersion: "3399492836"
    uid: fa8fc451-339a-4caf-b05a-7616a648b262
  preemptionPolicy: Never
  value: 100
- description: Used for critical components running on node, like CNI.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: node-critical
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:45Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:45Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:25Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:56Z"
    name: node-critical
    resourceVersion: "3399492986"
    uid: d6efe99b-911e-4db3-98b8-4fb422c6a377
  preemptionPolicy: PreemptLowerPriority
  value: 21000000
- description: Used for tier 2/3/4 services, deprecated.
  globalDefault: true
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: normal
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:46Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:globalDefault: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:46Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:26Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:54Z"
    name: normal
    resourceVersion: "3399492493"
    uid: 7826f4d8-5e52-4d74-8970-95f1c9677479
  preemptionPolicy: Never
  value: 100
- description: Used for pause pods used to over-provision clusters.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: overprovisioning
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:47Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:47Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:26Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:51Z"
    name: overprovisioning
    resourceVersion: "3399491647"
    uid: cae63c70-1605-49fe-b0fd-976cf950a19b
  preemptionPolicy: Never
  value: 1
- description: Used for tier 0 services.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: tier0
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T22:57:48Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-client-side-apply
      operation: Update
      time: "2025-02-24T22:57:48Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:27Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:50Z"
    name: tier0
    resourceVersion: "3399491191"
    uid: 5e3e958a-b7a5-4285-89ce-29cc18faa8e3
  preemptionPolicy: PreemptLowerPriority
  value: 100
- description: Used for tier 1 services.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: tier1
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T23:11:56Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-create
      operation: Update
      time: "2025-02-24T23:11:56Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:28Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:56Z"
    name: tier1
    resourceVersion: "3399492885"
    uid: 2a05e44a-55f5-4d14-b07b-2073e28983d9
  preemptionPolicy: Never
  value: 100
- description: Used for tier 2.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: tier2
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T23:11:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-create
      operation: Update
      time: "2025-02-24T23:11:57Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:29Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:57Z"
    name: tier2
    resourceVersion: "3399493161"
    uid: 5dba1f59-228b-4c86-81f4-6b7e1fee009f
  preemptionPolicy: Never
  value: 100
- description: Used for tier 3.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: tier3
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T23:11:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-create
      operation: Update
      time: "2025-02-24T23:11:57Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:29Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:54Z"
    name: tier3
    resourceVersion: "3399492418"
    uid: 7c777c62-fffd-443d-a423-6821ca0254d5
  preemptionPolicy: Never
  value: 100
- description: Used for tier 4.
  metadata:
    annotations:
      ad.datadoghq.com/tags: '{"git.commit.sha":"a09ee21bbf9e1a56e076b769cf06351f6e4b6f98","git.repository_url":"github.com/zendesk/k8s-static-resources","team":"compute"}'
      artifact.spinnaker.io/location: ""
      artifact.spinnaker.io/name: tier4
      artifact.spinnaker.io/type: kubernetes/PriorityClass
      artifact.spinnaker.io/version: ""
      cicd-toolkit.zende.sk/environment: staging
      cicd-toolkit.zende.sk/partition: sandbox
      cicd-toolkit.zende.sk/phase: phase-1
      cicd-toolkit.zende.sk/role: k8s-static-resources
      cicd-toolkit.zende.sk/version: v8.680.0
      clusterpropagationpolicy.karmada.io/name: duplicate
      moniker.spinnaker.io/application: k8s-static-resources
      moniker.spinnaker.io/cluster: sandbox
      moniker.spinnaker.io/detail: v48
      moniker.spinnaker.io/stack: k8s-static-resources-k8s-static-resources
      strategy.spinnaker.io/replace: "true"
    creationTimestamp: "2025-02-24T23:11:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: k8s-static-resources
      app.kubernetes.io/instance: k8s-static-resources
      app.kubernetes.io/managed-by: spinnaker
      app.kubernetes.io/name: k8s-static-resources
      app.kubernetes.io/part-of: foundation
      app.kubernetes.io/version: v48
      branch: v48
      cicd-toolkit.zende.sk/config-revision: a09ee21bbf9e1a56e076b769cf06351f6e4b6f98
      cicd-toolkit.zende.sk/config-version: v48
      cicd-toolkit.zende.sk/pipeline: sandbox
      clusterpropagationpolicy.karmada.io/permanent-id: 4a93d802-cfcc-4b30-98eb-77dd2eee641d
      compute.zende.sk/duplicate: "true"
      deploy_id: 01KEXCVHFRQCEE56Z0Q13DMSWT
      product: foundation
      project: zendesk_kubernetes
      role: priority-classes
      service: k8s-static-resources
      tags.datadoghq.com/env: staging
      tags.datadoghq.com/service: k8s-static-resources
      tags.datadoghq.com/version: v48
      team: compute
      track: 01KEXCVHFRQCEE56Z0Q13DMSWT
      version: v48
    managedFields:
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:description: {}
        f:metadata:
          f:annotations:
            .: {}
            f:cicd-toolkit.zende.sk/environment: {}
            f:cicd-toolkit.zende.sk/partition: {}
            f:cicd-toolkit.zende.sk/phase: {}
            f:moniker.spinnaker.io/cluster: {}
            f:strategy.spinnaker.io/replace: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:compute.zende.sk/duplicate: {}
            f:product: {}
            f:project: {}
            f:role: {}
            f:service: {}
            f:tags.datadoghq.com/env: {}
            f:team: {}
        f:preemptionPolicy: {}
        f:value: {}
      manager: kubectl-create
      operation: Update
      time: "2025-02-24T23:11:57Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:ad.datadoghq.com/tags: {}
            f:artifact.spinnaker.io/location: {}
            f:artifact.spinnaker.io/name: {}
            f:artifact.spinnaker.io/type: {}
            f:artifact.spinnaker.io/version: {}
            f:cicd-toolkit.zende.sk/role: {}
            f:cicd-toolkit.zende.sk/version: {}
            f:moniker.spinnaker.io/application: {}
            f:moniker.spinnaker.io/detail: {}
            f:moniker.spinnaker.io/stack: {}
          f:labels:
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/managed-by: {}
            f:app.kubernetes.io/version: {}
            f:branch: {}
            f:cicd-toolkit.zende.sk/config-revision: {}
            f:cicd-toolkit.zende.sk/config-version: {}
            f:cicd-toolkit.zende.sk/pipeline: {}
            f:deploy_id: {}
            f:tags.datadoghq.com/service: {}
            f:tags.datadoghq.com/version: {}
            f:track: {}
            f:version: {}
      manager: kubectl-replace
      operation: Update
      time: "2026-01-14T04:55:30Z"
    - apiVersion: scheduling.k8s.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:clusterpropagationpolicy.karmada.io/name: {}
          f:labels:
            f:clusterpropagationpolicy.karmada.io/permanent-id: {}
      manager: karmada-controller-manager
      operation: Update
      time: "2026-01-14T11:18:51Z"
    name: tier4
    resourceVersion: "3399491642"
    uid: 817de33c-26fe-41c8-80a9-193c2ab0cf2a
  preemptionPolicy: Never
  value: 100
